{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a563374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "if not os.path.exists('./modules') and not os.path.exists('modules.zip'):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "if not os.path.exists('./modules') and os.path.exists('modules.zip'):\n",
    "    os.system('unzip modules.zip -d .')\n",
    "\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "from modules import EEGDataset\n",
    "from modules.utils import split_and_get_loaders, evaluate_model, manual_write_study_params\n",
    "\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! need to modify those for the competition itself\n",
    "TRIAL_LENGTH = 640  # frequency of changing.. frequency\n",
    "# Download dataset\n",
    "# path_1 = kagglehub.dataset_download(\"xuannguyenuet2004/12-class-ssvep-eeg-data\") proofed to be bad\n",
    "path_1 = kagglehub.dataset_download(\"girgismicheal/steadystate-visual-evoked-potential-signals\")\n",
    "path_1 += \"/SSVEP (BrainWheel)\"\n",
    "print(\"Download datasetaset files:\", \"\\n\", path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSVEPClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, out_size: int, hidden_size: int, num_layers: int, dropout: float, bidirectional: bool, device=None):\n",
    "        super().__init__()\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dir_mult = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional, device=self.device, batch_first=True)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.dir_mult, out_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h0 = torch.zeros([self.num_layers * self.dir_mult, x.shape[0], self.hidden_size], device=self.device)\n",
    "        c0 = torch.zeros([self.num_layers * self.dir_mult, x.shape[0], self.hidden_size], device=self.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))  # out shape [B x window_length x out_shape]\n",
    "        return self.fc_out(out[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dec2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.train_epochs = 1000\n",
    "        self.tune_epochs = 25\n",
    "        self.optuna_n_trials = 120\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = None\n",
    "        self.trial = None\n",
    "\n",
    "        self.train_loader = None\n",
    "        self.eval_loader = None\n",
    "        self.test_loader = None\n",
    "        self.dataset = None\n",
    "\n",
    "        self.storage = \"sqlite:///optuna_studies.db\"\n",
    "        self.study_name = \"ssvep_classifier_optimization\"\n",
    "        \n",
    "        self.checkpoint_path = \"./checkpoints/ssvep\"\n",
    "        os.makedirs(os.path.join(self.checkpoint_path, \"models\"), exist_ok=True)\n",
    "        self.checkpoint_model_path = os.path.join(self.checkpoint_path, \"models\")\n",
    "\n",
    "    def _train_loop(self, n_epochs: int, should_save=False, should_print=False):\n",
    "        assert isinstance(self.optimizer, torch.optim.Optimizer), \"optimizer is not a valid optimizer\"\n",
    "        assert isinstance(self.train_loader, DataLoader), \"train_laoder is not valid Datloader\"\n",
    "        if self.trial is None:\n",
    "            print(\"Warning: self.trial is none, we are probably in acutal training phase\")\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            self.model.to(self.device)\n",
    "            self.model.train()\n",
    "\n",
    "            avg_loss = 0\n",
    "            for x, y in self.train_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(x)  # B x out_size\n",
    "                loss = self.criterion(y_pred, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "            avg_loss = avg_loss / len(self.train_loader)\n",
    "            evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "            \n",
    "            if self.trial is not None:\n",
    "                self.trial.report(evaluation, epoch)\n",
    "                if self.trial.should_prune():\n",
    "                    optuna.exceptions.TrialPruned()\n",
    "\n",
    "            if should_print:\n",
    "                print(f\"epoch {epoch}, evaluation {evaluation}, avg_loss {avg_loss}\")\n",
    "                \n",
    "            if should_save:\n",
    "                self.model.cpu()\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.checkpoint_model_path, f\"ssvep.pth\"))\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "    \n",
    "    def _prepare_training(self, is_trial):\n",
    "        if is_trial:\n",
    "            assert isinstance(self.trial, optuna.Trial), \"trial is none, cant' suggest params\"\n",
    "            \n",
    "            window_length = self.trial.suggest_categorical(\"window_length\", [128, 160])\n",
    "            stride_factor = self.trial.suggest_int(\"stride\", 2, 3)\n",
    "\n",
    "            hidden_size = self.trial.suggest_int(\"hidden_size\", 64, 192, step=32)\n",
    "            num_layers = self.trial.suggest_int(\"num_layers\", 1, 3)\n",
    "            dropout = self.trial.suggest_float(\"dropout\", 0, 0.4)\n",
    "            lr = self.trial.suggest_float(\"lr\", 3e-4, 3e-2, log=True)\n",
    "            batch_size = self.trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "            \n",
    "        else:\n",
    "            best_params = self._get_study().best_params\n",
    "            \n",
    "            window_length = best_params['window_length']\n",
    "            stride_factor = best_params['stride']\n",
    "            num_layers = best_params[\"num_layers\"]\n",
    "            dropout = best_params[\"dropout\"]\n",
    "            lr = best_params[\"lr\"]\n",
    "            batch_size = best_params[\"batch_size\"]\n",
    "            hidden_size = best_params[\"hidden_size\"]\n",
    "                \n",
    "        stride = int(window_length // stride_factor)\n",
    "        self.dataset = EEGDataset(path_1, TRIAL_LENGTH, window_length, stride=stride)\n",
    "        unique_freqs = torch.unique(self.dataset.labels)\n",
    "\n",
    "        input_size = self.dataset.data[0].shape[1]  # data[0] shape: CxT\n",
    "        out_size = len(unique_freqs)\n",
    "\n",
    "        self.model = SSVEPClassifier(input_size, out_size, hidden_size, num_layers, dropout, bidirectional=True)\n",
    "        self.train_loader, self.val_loader, self.test_loader = split_and_get_loaders(self.dataset, batch_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "    \n",
    "    def _objective(self, trial: optuna.Trial):\n",
    "        self.trial = trial\n",
    "        self._prepare_training(True)\n",
    "        \n",
    "        self._train_loop(self.tune_epochs, should_save=False, should_print=False)\n",
    "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "        return evaluation\n",
    "\n",
    "    def _get_study(self):\n",
    "        return optuna.create_study(study_name=self.study_name, storage=self.storage, direction=\"maximize\", load_if_exists=True)\n",
    "        \n",
    "    def optimize(self, delete_existing=False):\n",
    "        if delete_existing:\n",
    "            try:\n",
    "                optuna.delete_study(study_name=self.study_name, storage=self.storage)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        study = self._get_study()\n",
    "        study.optimize(self._objective, n_trials=self.optuna_n_trials, timeout=60 * 10)\n",
    "\n",
    "        # Print optimization results\n",
    "        print(\"\\nStudy statistics:\")\n",
    "        print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "        print(f\"  Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "        print(f\"  Number of complete trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
    "\n",
    "        print(\"\\nBest trial:\")\n",
    "        trial = study.best_trial\n",
    "        print(f\"  Value: {trial.value}\")\n",
    "        print(\"\\nBest hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        return study.best_params\n",
    "        \n",
    "    def train(self):\n",
    "        self.trial = None\n",
    "        self._prepare_training(False)\n",
    "\n",
    "        self._train_loop(self.train_epochs, should_save=True, should_print=True)\n",
    "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "        print(\"done training\")\n",
    "        return evaluation\n",
    "\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_existing = True\n",
    "trainer.optimize(delete_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a93294ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 16:16:08,926] A new study created in RDB with name: ssvep_classifier_optimization\n",
      "<string>:8: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=160, low=128, step=1) is deprecated and internally converted to IntDistribution(high=160, log=False, low=128, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=192, low=64, step=32) is deprecated and internally converted to IntDistribution(high=192, log=False, low=64, step=32). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=3, low=1, step=1) is deprecated and internally converted to IntDistribution(high=3, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[I 2025-06-16 16:16:08,985] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: self.trial is none, we are probably in acutal training phase\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m manual_write_study_params(trainer.study_name, trainer.storage)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28mself\u001b[39m.trial = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_training(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_save\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_print\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m evaluation = evaluate_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone training\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mTrainer._train_loop\u001b[39m\u001b[34m(self, n_epochs, should_save, should_print)\u001b[39m\n\u001b[32m     46\u001b[39m     avg_loss += loss.item()\n\u001b[32m     48\u001b[39m avg_loss = avg_loss / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m evaluation = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.trial.report(evaluation, epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/ai_projects/eeg_detection/modules/utils.py:18\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[32m     17\u001b[39m     x = x.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     y = y.to(device)\n\u001b[32m     20\u001b[39m     outputs = model(x)\n\u001b[32m     21\u001b[39m     _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mSSVEPClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m h0 = torch.zeros([\u001b[38;5;28mself\u001b[39m.num_layers * \u001b[38;5;28mself\u001b[39m.dir_mult, x.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.hidden_size], device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     19\u001b[39m c0 = torch.zeros([\u001b[38;5;28mself\u001b[39m.num_layers * \u001b[38;5;28mself\u001b[39m.dir_mult, x.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.hidden_size], device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m out, (hn, cn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# out shape [B x window_length x out_shape]\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc_out(out[:, -\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "manual_write_study_params(trainer.study_name, trainer.storage)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icmtc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
