{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a563374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "if not os.path.exists('./modules') and not os.path.exists('modules.zip'):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "if not os.path.exists('./modules') and os.path.exists('modules.zip'):\n",
    "    os.system('unzip modules.zip -d .')\n",
    "\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import optuna\n",
    "from modules import EEGDataset\n",
    "from modules.utils import split_and_get_loaders, evaluate_model, manual_write_study_params\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb6b091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download datasetaset files: \n",
      " /home/zeyadcode/.cache/kagglehub/datasets/girgismicheal/steadystate-visual-evoked-potential-signals/versions/1/SSVEP (BrainWheel)\n"
     ]
    }
   ],
   "source": [
    "#! need to modify those for the competition itself\n",
    "TRIAL_LENGTH = 640  # frequency of changing.. frequency\n",
    "# Download dataset\n",
    "path_1 = kagglehub.dataset_download(\"girgismicheal/steadystate-visual-evoked-potential-signals\")\n",
    "path_1 += \"/SSVEP (BrainWheel)\"\n",
    "print(\"Download datasetaset files:\", \"\\n\", path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b83a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)\n",
      "  return F.conv2d(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0132,  0.5764, -0.1946,  0.1811],\n",
       "        [ 0.2503, -0.1922, -0.1633,  0.2259],\n",
       "        [ 0.4580, -0.2843,  0.5024,  0.5419],\n",
       "        [-0.2598,  0.0032,  0.3093, -0.4052],\n",
       "        [ 0.0108,  0.2246,  0.0293,  0.1898]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DepthWiseConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, dim_mult=1, padding=0, bias=False):\n",
    "        super(DepthWiseConv2D, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels * dim_mult, padding=padding, kernel_size=kernel_size, groups=in_channels, bias=bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.depthwise(x)\n",
    "\n",
    "\n",
    "class SeperableConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, bias=False):\n",
    "        super(SeperableConv2D, self).__init__()\n",
    "        self.depthwise = DepthWiseConv2D(in_channels, kernel_size, padding=padding)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class SSVEPClassifier(nn.Module):\n",
    "    # EEG Net Based\n",
    "    # todo look at this https://paperswithcode.com/paper/a-transformer-based-deep-neural-network-model\n",
    "    def __init__(self, n_electrodes=16, n_samples=128, out_dim=4, timesteps=120, dropout=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        nb_classes = 4\n",
    "        Chans = n_electrodes\n",
    "        dropoutRate = 0.5\n",
    "        kernLength = 256\n",
    "        F1 = 96\n",
    "        D = 1\n",
    "        F2 = 96\n",
    "        \n",
    "        # B x C x T\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            #\n",
    "            DepthWiseConv2D(F1, (Chans, 1), dim_mult=D, bias=False),\n",
    "            nn.BatchNorm2d(F1*D),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4)), # todo try making this max pool\n",
    "            nn.Dropout(dropoutRate),\n",
    "            #\n",
    "            SeperableConv2D(F1 * D, F2, kernel_size=(1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8)),\n",
    "            nn.Dropout(dropoutRate),\n",
    "        )\n",
    "\n",
    "        self.classif_head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(F1 * n_samples // 4 // 8, nb_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"expected input shape: BxCxT\"\"\"\n",
    "        x = x.unsqueeze(1)\n",
    "        y = self.block_1(x)\n",
    "        y = self.classif_head(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "dummy_x = torch.randn(5, 14, 320)\n",
    "model = SSVEPClassifier(n_electrodes=dummy_x.shape[1], n_samples=dummy_x.shape[2])\n",
    "model(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42089fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 320\n",
    "stride = window_length\n",
    "batch_size = 32\n",
    "\n",
    "dataset = EEGDataset(path_1, TRIAL_LENGTH, window_length, stride=stride)\n",
    "train_loader, val_loader, test_loader = split_and_get_loaders(dataset, batch_size)\n",
    "model = SSVEPClassifier(n_electrodes=dummy_x.shape[1], n_samples=dummy_x.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b164b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, avg_loss: 1.042821516351002, val_evaluation: 0.421875\n",
      "epoch: 1, avg_loss: 0.9996108514506642, val_evaluation: 0.41875\n",
      "epoch: 2, avg_loss: 0.9743724392681588, val_evaluation: 0.415625\n",
      "epoch: 3, avg_loss: 0.9410026945718905, val_evaluation: 0.43125\n",
      "epoch: 4, avg_loss: 0.9254281375466323, val_evaluation: 0.41875\n",
      "epoch: 5, avg_loss: 0.8972443458510608, val_evaluation: 0.40625\n",
      "epoch: 6, avg_loss: 0.8815615569672933, val_evaluation: 0.41875\n",
      "epoch: 7, avg_loss: 0.8472689477408805, val_evaluation: 0.409375\n",
      "epoch: 8, avg_loss: 0.8108456033032115, val_evaluation: 0.409375\n",
      "epoch: 9, avg_loss: 0.8204494467595729, val_evaluation: 0.425\n",
      "epoch: 10, avg_loss: 0.8109133490702001, val_evaluation: 0.396875\n",
      "epoch: 11, avg_loss: 0.7882963491649162, val_evaluation: 0.44375\n",
      "epoch: 12, avg_loss: 0.724695548778627, val_evaluation: 0.4125\n",
      "epoch: 13, avg_loss: 0.6763580245215718, val_evaluation: 0.428125\n",
      "epoch: 14, avg_loss: 0.6387860128065435, val_evaluation: 0.440625\n",
      "epoch: 15, avg_loss: 0.6180984777648274, val_evaluation: 0.43125\n",
      "epoch: 16, avg_loss: 0.5924003727552367, val_evaluation: 0.421875\n",
      "epoch: 17, avg_loss: 0.5868945986759372, val_evaluation: 0.421875\n",
      "epoch: 18, avg_loss: 0.5598544035016036, val_evaluation: 0.440625\n",
      "epoch: 19, avg_loss: 0.5697486291571361, val_evaluation: 0.45625\n",
      "epoch: 20, avg_loss: 0.5307268442177191, val_evaluation: 0.428125\n",
      "epoch: 21, avg_loss: 0.5284172151146865, val_evaluation: 0.446875\n",
      "epoch: 22, avg_loss: 0.47884062296006735, val_evaluation: 0.45\n",
      "epoch: 23, avg_loss: 0.4378802634593917, val_evaluation: 0.471875\n",
      "epoch: 24, avg_loss: 0.42342050082799865, val_evaluation: 0.46875\n",
      "epoch: 25, avg_loss: 0.41748016999989024, val_evaluation: 0.5\n",
      "epoch: 26, avg_loss: 0.3564906894433789, val_evaluation: 0.4625\n",
      "epoch: 27, avg_loss: 0.34816774071716683, val_evaluation: 0.465625\n",
      "epoch: 28, avg_loss: 0.363046935418757, val_evaluation: 0.471875\n",
      "epoch: 29, avg_loss: 0.36689308721844743, val_evaluation: 0.4875\n",
      "epoch: 30, avg_loss: 0.38541538322844154, val_evaluation: 0.496875\n",
      "epoch: 31, avg_loss: 0.5512842063496752, val_evaluation: 0.446875\n",
      "epoch: 32, avg_loss: 0.3408389505816669, val_evaluation: 0.459375\n",
      "epoch: 33, avg_loss: 0.2608298904648641, val_evaluation: 0.484375\n",
      "epoch: 34, avg_loss: 0.23778720836086972, val_evaluation: 0.490625\n",
      "epoch: 35, avg_loss: 0.21560422849000954, val_evaluation: 0.48125\n",
      "epoch: 36, avg_loss: 0.19254617929095175, val_evaluation: 0.475\n",
      "epoch: 37, avg_loss: 0.18227394679334105, val_evaluation: 0.478125\n",
      "epoch: 38, avg_loss: 0.1661709074567004, val_evaluation: 0.46875\n",
      "epoch: 39, avg_loss: 0.13752286236097172, val_evaluation: 0.490625\n",
      "epoch: 40, avg_loss: 0.13497913238115428, val_evaluation: 0.465625\n",
      "epoch: 41, avg_loss: 0.11933949598815383, val_evaluation: 0.48125\n",
      "epoch: 42, avg_loss: 0.1077214644812956, val_evaluation: 0.471875\n",
      "epoch: 43, avg_loss: 0.10960219573320412, val_evaluation: 0.46875\n",
      "epoch: 44, avg_loss: 0.39147556236967807, val_evaluation: 0.43125\n",
      "epoch: 45, avg_loss: 0.34664749735739175, val_evaluation: 0.440625\n",
      "epoch: 46, avg_loss: 0.3716825717469541, val_evaluation: 0.459375\n",
      "epoch: 47, avg_loss: 0.34100902153224477, val_evaluation: 0.45625\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "avg_losses = []\n",
    "evaluations = []\n",
    "\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        y_pred = model(x).to(device)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "    avg_loss /= len(train_loader)\n",
    "    avg_losses.append(avg_loss)\n",
    "    \n",
    "    evaluation = evaluate_model(model, val_loader, device)\n",
    "    evaluations.append(evaluation)\n",
    "    print(f'epoch: {epoch}, avg_loss: {avg_loss}, val_evaluation: {evaluation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3308c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x751732d5eba0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(range(len(avg_losses)), avg_losses)\n",
    "plt.title(\"train loss graph\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(len(evaluations)), evaluations)\n",
    "plt.title(\"train loss graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2dec2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 10:14:39,657] A new study created in RDB with name: ssvep_classifier_optimization\n",
      "<string>:8: FutureWarning: IntUniformDistribution has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :class:`~optuna.distributions.IntDistribution` instead.\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=160, low=128, step=1) is deprecated and internally converted to IntDistribution(high=160, log=False, low=128, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=192, low=64, step=32) is deprecated and internally converted to IntDistribution(high=192, log=False, low=64, step=32). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/distributions.py:783: FutureWarning: IntUniformDistribution(high=3, low=1, step=1) is deprecated and internally converted to IntDistribution(high=3, log=False, low=1, step=1). See https://github.com/optuna/optuna/issues/2941.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[I 2025-06-17 10:14:39,712] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: self.trial is none, we are probably in acutal training phase\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4, 1, 2, 32], expected input[1, 16, 33, 98] to have 1 channels, but got 16 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m trainer = Trainer()\n\u001b[32m    143\u001b[39m manual_write_study_params(trainer.study_name, trainer.storage)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.trial = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_training(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_save\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_print\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m evaluation = evaluate_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone training\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mTrainer._train_loop\u001b[39m\u001b[34m(self, n_epochs, should_save, should_print)\u001b[39m\n\u001b[32m     37\u001b[39m x = x.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     38\u001b[39m y = y.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B x out_size\u001b[39;00m\n\u001b[32m     41\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(y_pred, y)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mSSVEPClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Layer 2\u001b[39;00m\n\u001b[32m     38\u001b[39m x = \u001b[38;5;28mself\u001b[39m.padding1(x)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m x = F.elu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     40\u001b[39m x = \u001b[38;5;28mself\u001b[39m.batchnorm2(x)\n\u001b[32m     41\u001b[39m x = F.dropout(x, \u001b[38;5;28mself\u001b[39m.dropout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [4, 1, 2, 32], expected input[1, 16, 33, 98] to have 1 channels, but got 16 channels instead"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.train_epochs = 1000\n",
    "        self.tune_epochs = 25\n",
    "        self.optuna_n_trials = 120\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = None\n",
    "        self.trial = None\n",
    "\n",
    "        self.train_loader = None\n",
    "        self.eval_loader = None\n",
    "        self.test_loader = None\n",
    "        self.dataset = None\n",
    "\n",
    "        self.storage = \"sqlite:///optuna_studies.db\"\n",
    "        self.study_name = \"ssvep_classifier_optimization\"\n",
    "        \n",
    "        self.checkpoint_path = \"./checkpoints/ssvep\"\n",
    "        os.makedirs(os.path.join(self.checkpoint_path, \"models\"), exist_ok=True)\n",
    "        self.checkpoint_model_path = os.path.join(self.checkpoint_path, \"models\")\n",
    "\n",
    "    def _train_loop(self, n_epochs: int, should_save=False, should_print=False):\n",
    "        assert isinstance(self.optimizer, torch.optim.Optimizer), \"optimizer is not a valid optimizer\"\n",
    "        assert isinstance(self.train_loader, DataLoader), \"train_laoder is not valid Datloader\"\n",
    "        if self.trial is None:\n",
    "            print(\"Warning: self.trial is none, we are probably in acutal training phase\")\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            self.model.to(self.device)\n",
    "            self.model.train()\n",
    "\n",
    "            avg_loss = 0\n",
    "            for x, y in self.train_loader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                y_pred = self.model(x)  # B x out_size\n",
    "                loss = self.criterion(y_pred, y)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_loss += loss.item()\n",
    "\n",
    "            avg_loss = avg_loss / len(self.train_loader)\n",
    "            evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "            \n",
    "            if self.trial is not None:\n",
    "                self.trial.report(evaluation, epoch)\n",
    "                if self.trial.should_prune():\n",
    "                    optuna.exceptions.TrialPruned()\n",
    "\n",
    "            if should_print:\n",
    "                print(f\"epoch {epoch}, evaluation {evaluation}, avg_loss {avg_loss}\")\n",
    "                \n",
    "            if should_save:\n",
    "                self.model.cpu()\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.checkpoint_model_path, f\"ssvep.pth\"))\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "    \n",
    "    def _prepare_training(self, is_trial):\n",
    "        if is_trial:\n",
    "            assert isinstance(self.trial, optuna.Trial), \"trial is none, cant' suggest params\"\n",
    "            \n",
    "            window_length = self.trial.suggest_categorical(\"window_length\", [128, 160])\n",
    "            stride_factor = self.trial.suggest_int(\"stride\", 2, 3)\n",
    "\n",
    "            dropout = self.trial.suggest_float(\"dropout\", 0, 0.4)\n",
    "            lr = self.trial.suggest_float(\"lr\", 3e-4, 3e-2, log=True)\n",
    "            batch_size = self.trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "            \n",
    "        else:\n",
    "            best_params = self._get_study().best_params\n",
    "            \n",
    "            window_length = best_params['window_length']\n",
    "            stride_factor = best_params['stride']\n",
    "            dropout = best_params[\"dropout\"]\n",
    "            lr = best_params[\"lr\"]\n",
    "            batch_size = best_params[\"batch_size\"]\n",
    "                \n",
    "        stride = int(window_length // stride_factor)\n",
    "        self.dataset = EEGDataset(path_1, TRIAL_LENGTH, window_length, stride=stride)\n",
    "        unique_freqs = torch.unique(self.dataset.labels)\n",
    "\n",
    "        n_timesteps = self.dataset.data[0].shape[0]\n",
    "        n_electrodes = self.dataset.data[0].shape[1] # data[x] shape TxC\n",
    "        out_size = len(unique_freqs)\n",
    "\n",
    "        self.model = SSVEPClassifier(n_electrodes, out_size, n_timesteps, dropout)\n",
    "        self.train_loader, self.val_loader, self.test_loader = split_and_get_loaders(self.dataset, batch_size)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "    \n",
    "    def _objective(self, trial: optuna.Trial):\n",
    "        self.trial = trial\n",
    "        self._prepare_training(True)\n",
    "        \n",
    "        self._train_loop(self.tune_epochs, should_save=False, should_print=False)\n",
    "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "        return evaluation\n",
    "\n",
    "    def _get_study(self):\n",
    "        return optuna.create_study(study_name=self.study_name, storage=self.storage, direction=\"maximize\", load_if_exists=True)\n",
    "        \n",
    "    def optimize(self, delete_existing=False):\n",
    "        if delete_existing:\n",
    "            try:\n",
    "                optuna.delete_study(study_name=self.study_name, storage=self.storage)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        study = self._get_study()\n",
    "        study.optimize(self._objective, n_trials=self.optuna_n_trials, timeout=60 * 10)\n",
    "\n",
    "        # Print optimization results\n",
    "        print(\"\\nStudy statistics:\")\n",
    "        print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "        print(f\"  Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "        print(f\"  Number of complete trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
    "\n",
    "        print(\"\\nBest trial:\")\n",
    "        trial = study.best_trial\n",
    "        print(f\"  Value: {trial.value}\")\n",
    "        print(\"\\nBest hyperparameters:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        return study.best_params\n",
    "        \n",
    "    def train(self):\n",
    "        self.trial = None\n",
    "        self._prepare_training(False)\n",
    "\n",
    "        self._train_loop(self.train_epochs, should_save=True, should_print=True)\n",
    "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
    "        print(\"done training\")\n",
    "        return evaluation\n",
    "\n",
    "trainer = Trainer()\n",
    "manual_write_study_params(trainer.study_name, trainer.storage)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a132bf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 09:44:24,195] A new study created in RDB with name: ssvep_classifier_optimization\n",
      "[W 2025-06-17 09:44:27,538] Trial 0 failed with parameters: {'window_length': 128, 'stride': 2, 'num_layers': 1, 'dropout': 0.17545707886915377, 'lr': 0.0006029016057316177, 'batch_size': 32} because of the following error: RuntimeError('Boolean value of Tensor with more than one value is ambiguous').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_290626/1753762340.py\", line 101, in _objective\n",
      "    self._prepare_training(True)\n",
      "  File \"/tmp/ipykernel_290626/1753762340.py\", line 95, in _prepare_training\n",
      "    self.model = SSVEPClassifier(n_electrodes, out_size, n_timesteps, dropout)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_290626/1025280560.py\", line 8, in __init__\n",
      "    self.conv1 = nn.Conv2d(1, n_electrodes, (1, 64), padding = 0)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 521, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 108, in __init__\n",
      "    if out_channels % groups != 0:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Boolean value of Tensor with more than one value is ambiguous\n",
      "[W 2025-06-17 09:44:27,541] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m delete_existing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete_existing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mTrainer.optimize\u001b[39m\u001b[34m(self, delete_existing)\u001b[39m\n\u001b[32m    115\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    117\u001b[39m study = \u001b[38;5;28mself\u001b[39m._get_study()\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Print optimization results\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStudy statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mTrainer._objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_objective\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial: optuna.Trial):\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.trial = trial\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m._train_loop(\u001b[38;5;28mself\u001b[39m.tune_epochs, should_save=\u001b[38;5;28;01mFalse\u001b[39;00m, should_print=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    104\u001b[39m     evaluation = evaluate_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mTrainer._prepare_training\u001b[39m\u001b[34m(self, is_trial)\u001b[39m\n\u001b[32m     92\u001b[39m n_timesteps = \u001b[38;5;28mself\u001b[39m.dataset.data[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m     93\u001b[39m out_size = \u001b[38;5;28mlen\u001b[39m(unique_freqs)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSSVEPClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_electrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loader, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.test_loader = split_and_get_loaders(\u001b[38;5;28mself\u001b[39m.dataset, batch_size)\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = torch.optim.Adam(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSSVEPClassifier.__init__\u001b[39m\u001b[34m(self, n_electrodes, out_dim, timesteps, dropout)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.dropout = dropout\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Layer 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mself\u001b[39m.conv1 = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_electrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.batchnorm1 = nn.BatchNorm2d(n_electrodes, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Layer 2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:521\u001b[39m, in \u001b[36mConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[32m    520\u001b[39m dilation_ = _pair(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:108\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "delete_existing = True\n",
    "trainer.optimize(delete_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a93294ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 09:43:46,116] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# manual_write_study_params(trainer.study_name, trainer.storage)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m.trial = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m._train_loop(\u001b[38;5;28mself\u001b[39m.train_epochs, should_save=\u001b[38;5;28;01mTrue\u001b[39;00m, should_print=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    140\u001b[39m     evaluation = evaluate_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mTrainer._prepare_training\u001b[39m\u001b[34m(self, is_trial)\u001b[39m\n\u001b[32m     92\u001b[39m n_timesteps = \u001b[38;5;28mself\u001b[39m.dataset.data[\u001b[32m0\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m     93\u001b[39m out_size = \u001b[38;5;28mlen\u001b[39m(unique_freqs)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSSVEPClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_electrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28mself\u001b[39m.train_loader, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.test_loader = split_and_get_loaders(\u001b[38;5;28mself\u001b[39m.dataset, batch_size)\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = torch.optim.Adam(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mSSVEPClassifier.__init__\u001b[39m\u001b[34m(self, n_electrodes, out_dim, timesteps, dropout)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.dropout = dropout\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Layer 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mself\u001b[39m.conv1 = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_electrodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.batchnorm1 = nn.BatchNorm2d(n_electrodes, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Layer 2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:521\u001b[39m, in \u001b[36mConv2d.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    519\u001b[39m padding_ = padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[32m    520\u001b[39m dilation_ = _pair(dilation)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:108\u001b[39m, in \u001b[36m_ConvNd.__init__\u001b[39m\u001b[34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33min_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_channels % groups != \u001b[32m0\u001b[39m:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mout_channels must be divisible by groups\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m valid_padding_strings = {\u001b[33m\"\u001b[39m\u001b[33msame\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m}\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# manual_write_study_params(trainer.study_name, trainer.storage)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icmtc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
