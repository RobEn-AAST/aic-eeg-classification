{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a563374",
   "metadata": {
    "cellUniqueIdByVincent": "b5c77",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "2a563374",
    "outputId": "2f5357e7-a3f2-46ab-a67e-a21a666bccbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.autograd import Function\n",
    "import optuna\n",
    "from modules import Trainer\n",
    "from modules.competition_dataset import EEGDataset\n",
    "from modules.utils import evaluate_model\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import ConcatDataset, random_split, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5bf7b0",
   "metadata": {
    "cellUniqueIdByVincent": "e6823",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8c5bf7b0",
    "outputId": "e981327e-6451-4cf6-8020-c4fb791c5b99"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# data_path = '/content/drive/MyDrive/ai_data/eeg_detection/data/mtcaic3'\n",
    "# model_path = '/content/drive/MyDrive/ai_data/eeg_detection/checkpoints/ssvep/models/ssvep.pth'\n",
    "# optuna_db_path = '/content/drive/MyDrive/ai_data/eeg_detection/checkpoints/ssvep/optuna/optuna_studies.db'\n",
    "data_path = './data/mtcaic3'\n",
    "model_path = './checkpoints/mi/models/the_honored_one.pth'\n",
    "optuna_db_path = './checkpoints/mi/optuna/the_honored_one.db'\n",
    "eeg_channels = [\n",
    "    \"C3\", # 2296.15\n",
    "    \"PZ\", # 1744.43\n",
    "    \"C4\", # 1556.46\n",
    "    # \"OZ\" # 444.98\n",
    "    # \"PO7\" # 381.63\n",
    "    # \"PO8\" # 275.78\n",
    "    # \"CZ\" # 200.43\n",
    "    # \"FZ\" # 111.51\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17b42a7",
   "metadata": {
    "cellUniqueIdByVincent": "285db",
    "id": "a17b42a7"
   },
   "outputs": [],
   "source": [
    "# Add this at the beginning of your notebook, after imports\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call this function before creating datasets and models\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42089fb8",
   "metadata": {
    "cellUniqueIdByVincent": "77c22",
    "id": "42089fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: mi, split: train, domain: time, data_fraction: 0.3\n",
      "Using 30.0% of data: 720/720 samples\n",
      "skipped: 0/720\n",
      "data shape: (12662, 3, 256), mean shape: (1, 3, 1)\n",
      "task: mi, split: validation, domain: time, data_fraction: 1\n",
      "skipped: 0/50\n",
      "data shape: (857, 3, 256), mean shape: (1, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "window_length = 128 * 2 # ensure divisble by 64 the kernel size\n",
    "stride = window_length // 3\n",
    "batch_size = 64\n",
    "\n",
    "dataset_train = EEGDataset(\n",
    "    data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    domain=\"time\",\n",
    "    data_fraction=0.3,\n",
    "    hardcoded_mean=True,\n",
    "    task=\"mi\",\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "dataset_val = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    split='validation',\n",
    "    read_labels=True,\n",
    "    hardcoded_mean=True,\n",
    "    data_fraction=1,\n",
    "    task=\"mi\",\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(dataset_val,   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd5f3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0656, -0.0231],\n",
       "         [-0.0680, -0.0245],\n",
       "         [-0.0684, -0.0269],\n",
       "         [-0.0686, -0.0246],\n",
       "         [-0.0668, -0.0260]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-1.8820e-01,  1.5262e-01,  3.2191e-01, -5.9028e-02, -2.9754e-01,\n",
       "          -2.4035e-02,  4.4117e-02,  2.7417e-01, -1.2048e-01,  2.5109e-02,\n",
       "          -6.9529e-02, -3.7081e-02,  1.7412e-01,  1.2067e-01, -5.8845e-02,\n",
       "           5.6947e-01,  6.5318e-02, -2.8768e-02,  4.3703e-04,  1.2804e-01,\n",
       "          -4.7969e-01,  1.1280e-01, -3.5237e-02, -2.3722e-01, -2.2489e-01,\n",
       "          -1.3670e-01, -2.1485e-01, -2.5705e-01,  8.1293e-02,  1.4516e-01],\n",
       "         [-9.8887e-02,  8.7126e-02,  1.8121e-01, -2.8861e-01, -9.2050e-02,\n",
       "          -8.3485e-02, -2.1213e-01,  1.6412e-01, -1.4923e-01, -2.7404e-01,\n",
       "          -1.1194e-01, -2.2734e-01, -5.7766e-04,  2.1102e-01,  2.2843e-01,\n",
       "           6.4041e-01, -2.7889e-02,  5.1864e-02,  4.2334e-01,  1.8291e-01,\n",
       "           3.2912e-02, -6.5786e-02, -2.6322e-02, -1.1465e-01, -2.0541e-01,\n",
       "          -1.0831e-01, -1.2442e-01,  2.5209e-01,  5.1401e-02, -1.6563e-01],\n",
       "         [ 4.3109e-02,  5.3890e-02,  1.9302e-01, -1.5999e-01,  9.3971e-02,\n",
       "          -3.3698e-01, -1.7417e-02,  1.6444e-01, -2.6899e-01, -2.1776e-01,\n",
       "          -5.0711e-02,  6.6719e-03,  2.3304e-02,  2.1488e-01,  2.2400e-01,\n",
       "           4.4794e-01,  2.1100e-02, -2.7025e-01, -1.7044e-01,  9.1249e-02,\n",
       "           1.1093e-01,  2.2241e-01, -5.4549e-02, -2.5384e-01, -1.6738e-01,\n",
       "          -1.5031e-01, -4.0627e-01, -2.0628e-01,  9.2534e-02,  1.6502e-01],\n",
       "         [-9.8848e-02,  3.2080e-01,  1.3353e-01, -3.9413e-01, -3.9437e-01,\n",
       "          -7.1977e-02, -2.7290e-01,  3.3233e-01, -2.7758e-01,  1.7171e-01,\n",
       "          -2.8095e-02,  1.2323e-01, -4.5530e-03,  1.7934e-01,  8.9688e-02,\n",
       "           6.0105e-01, -1.6691e-01,  1.5643e-01, -1.6194e-01,  1.6206e-01,\n",
       "           7.2477e-02,  2.7223e-01, -1.5175e-01, -2.7778e-01, -1.7774e-01,\n",
       "           3.6380e-02, -3.4871e-01, -7.3737e-02,  3.3752e-01, -1.5663e-01],\n",
       "         [-6.6571e-02,  7.2669e-02,  5.1844e-01, -1.5986e-01, -2.8519e-01,\n",
       "          -1.7968e-01, -2.9236e-01, -1.4708e-01, -4.9622e-01, -1.3784e-01,\n",
       "           6.5908e-02, -2.8974e-01,  1.9066e-01,  2.4635e-01,  5.6105e-02,\n",
       "           2.5370e-01,  7.2319e-02,  9.9889e-02,  5.5397e-03,  1.7154e-01,\n",
       "           1.2352e-01, -6.0449e-03,  9.1849e-02,  1.7968e-02, -2.3137e-01,\n",
       "          -2.0057e-01,  5.4445e-02, -8.6431e-03,  7.8236e-03, -3.3314e-01]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- Gradient Reversal Layer ---------------- #\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class GradientReversal(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.alpha)\n",
    "\n",
    "# ---------------- LSTM Head ---------------- #\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        # x: B x seq_len x feat_dim\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# ---------------- EEG Feature Extractor ---------------- #\n",
    "class EEGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, n_electrodes, kernLength, F1, D, F2, dropout):\n",
    "        super().__init__()\n",
    "        # For input B x C x T, apply 2D convs on [C x T]\n",
    "        self.block = nn.Sequential(\n",
    "            # Temporal conv across time\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding=(0, kernLength//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            # Depthwise spatial conv across electrodes\n",
    "            nn.Conv2d(F1, F1 * D, (n_electrodes, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout),\n",
    "            # Separable conv\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding=(0, 8), groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, 1, bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x C x T\n",
    "        x = x.unsqueeze(1)  # B x 1 x C x T\n",
    "        x = self.block(x)   # B x F2 x 1 x T_sub\n",
    "        x = x.squeeze(2)    # B x F2 x T_sub\n",
    "        x = x.permute(0, 2, 1)  # B x T_sub x F2\n",
    "        return x\n",
    "\n",
    "# ---------------- DANN SSVEP Classifier ---------------- #\n",
    "class DANN_SSVEPClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_electrodes=16,\n",
    "                 out_dim=4,\n",
    "                 dropout=0.25,\n",
    "                 kernLength=256,\n",
    "                 F1=96,\n",
    "                 D=1,\n",
    "                 F2=96,\n",
    "                 hidden_dim=100,\n",
    "                 layer_dim=1,\n",
    "                 grl_alpha=1.0,\n",
    "                 domain_classes=2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = EEGFeatureExtractor(\n",
    "            n_electrodes, kernLength, F1, D, F2, dropout\n",
    "        )\n",
    "        feat_dim = F2\n",
    "        # label predictor (LSTM over time)\n",
    "        self.label_lstm = LSTMModel(feat_dim, hidden_dim, layer_dim, out_dim)\n",
    "        # domain predictor\n",
    "        self.grl = GradientReversal(alpha=grl_alpha)\n",
    "        self.domain_fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, domain_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x C x T\n",
    "        feat_seq = self.feature_extractor(x)        # B x T_sub x feat_dim\n",
    "        class_out = self.label_lstm(feat_seq)\n",
    "        # domain prediction from time-averaged features\n",
    "        feat_avg = feat_seq.mean(dim=1)             # B x feat_dim\n",
    "        rev_feat = self.grl(feat_avg)\n",
    "        domain_out = self.domain_fc(rev_feat)\n",
    "        return class_out, domain_out\n",
    "\n",
    "\n",
    "dummy_x = torch.randn(5, 3, 256).to(device)\n",
    "model = DANN_SSVEPClassifier(\n",
    "    dropout=0.26211635308091535,\n",
    "    n_electrodes=3,\n",
    "    out_dim=2,\n",
    "    domain_classes=30,\n",
    "    kernLength=256,\n",
    "    F1 = 32,\n",
    "    D = 3,\n",
    "    F2 = 32,\n",
    "    hidden_dim=256,\n",
    "    layer_dim=3,\n",
    ").to(device)\n",
    "\n",
    "model(dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05756275",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses_label = []\n",
    "avg_losses_domain = []\n",
    "val_label_accuracies = []\n",
    "val_domain_accuracies = []\n",
    "train_label_accuracies = []\n",
    "train_domain_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12b164b5",
   "metadata": {
    "cellUniqueIdByVincent": "c9b4e",
    "id": "12b164b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: avg_loss_label: 0.687, avg_loss_domain: 3.393, train_label_acc: 53.59%, train_domain_acc: 4.26%, label_evaluation: 44.22, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "5: avg_loss_label: 0.679, avg_loss_domain: 3.383, train_label_acc: 56.49%, train_domain_acc: 4.45%, label_evaluation: 46.44, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "10: avg_loss_label: 0.675, avg_loss_domain: 3.383, train_label_acc: 56.87%, train_domain_acc: 4.48%, label_evaluation: 44.34, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "15: avg_loss_label: 0.673, avg_loss_domain: 3.382, train_label_acc: 56.79%, train_domain_acc: 4.48%, label_evaluation: 44.92, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "20: avg_loss_label: 0.672, avg_loss_domain: 3.379, train_label_acc: 57.12%, train_domain_acc: 4.98%, label_evaluation: 40.61, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "25: avg_loss_label: 0.668, avg_loss_domain: 3.379, train_label_acc: 58.90%, train_domain_acc: 4.72%, label_evaluation: 38.74, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "30: avg_loss_label: 0.668, avg_loss_domain: 3.378, train_label_acc: 58.36%, train_domain_acc: 4.90%, label_evaluation: 40.26, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "35: avg_loss_label: 0.659, avg_loss_domain: 3.376, train_label_acc: 59.69%, train_domain_acc: 5.24%, label_evaluation: 42.82, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "40: avg_loss_label: 0.659, avg_loss_domain: 3.375, train_label_acc: 59.26%, train_domain_acc: 4.99%, label_evaluation: 40.96, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "45: avg_loss_label: 0.654, avg_loss_domain: 3.375, train_label_acc: 59.94%, train_domain_acc: 4.90%, label_evaluation: 38.04, domain_evaluation: 0.00, lr: 0.0003746351873334935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m avg_loss_domain += loss_domain.item()\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Accuracy calculation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m _, pred_labels = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m _, pred_domains = torch.max(y_pred_domain, \u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m correct_label += (pred_labels == y_labels).sum().item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "except Exception:\n",
    "    print(\"skipping model loading...\")\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0003746351873334935)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt,\n",
    "    mode=\"min\",  # “min” if you want to reduce lr when the quantity monitored has stopped decreasing\n",
    "    factor=0.5,  # new_lr = lr * factor\n",
    "    patience=20,  # number of epochs with no improvement after which lr will be reduced\n",
    "    threshold=1e-4,  # threshold for measuring the new optimum, to only focus on significant changes\n",
    "    threshold_mode=\"rel\",  # `'rel'` means compare change relative to best value. Could use `'abs'`.\n",
    "    cooldown=0,  # epochs to wait before resuming normal operation after lr has been reduced\n",
    "    min_lr=1e-6,  # lower bound on the lr\n",
    ")\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss_label = 0\n",
    "    avg_loss_domain = 0\n",
    "    correct_label = 0\n",
    "    correct_domain = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).to(torch.int64) # shape: [Bx2], 0: label, 1: domain\n",
    "        y_labels = y[:, 0]\n",
    "        y_subj = y[:, 1]\n",
    "\n",
    "        y_pred_labels, y_pred_domain = model(x)\n",
    "\n",
    "        loss_label = criterion(y_pred_labels, y_labels)\n",
    "        loss_domain = criterion(y_pred_domain, y_subj)\n",
    "        loss = loss_label + loss_domain\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        avg_loss_label += loss_label.item()\n",
    "        avg_loss_domain += loss_domain.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        _, pred_labels = torch.max(y_pred_labels, 1)\n",
    "        _, pred_domains = torch.max(y_pred_domain, 1)\n",
    "        correct_label += (pred_labels == y_labels).sum().item()\n",
    "        correct_domain += (pred_domains == y_subj).sum().item()\n",
    "        total += y_labels.size(0)\n",
    "\n",
    "    avg_loss_label /= len(train_loader)\n",
    "    avg_loss_domain /= len(train_loader)\n",
    "    avg_losses_label.append(avg_loss_label)\n",
    "    avg_losses_domain.append(avg_loss_domain)\n",
    "    train_label_acc = 100.0 * correct_label / total\n",
    "    train_domain_acc = 100.0 * correct_domain / total\n",
    "    train_label_accuracies.append(train_label_acc)\n",
    "    train_domain_accuracies.append(train_domain_acc)\n",
    "    # scheduler.step(avg_loss_label)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        label_evaluation, domain_evaluation = evaluate_model(model, val_loader, device)\n",
    "        val_label_accuracies.append(label_evaluation)\n",
    "        val_domain_accuracies.append(domain_evaluation)\n",
    "        model.cpu()\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model.to(device)\n",
    "        print(f\"{epoch}: avg_loss_label: {avg_loss_label:.3F}, avg_loss_domain: {avg_loss_domain:.3F}, train_label_acc: {train_label_acc:.2f}%, train_domain_acc: {train_domain_acc:.2f}%, label_evaluation: {(label_evaluation*100):.2f}, domain_evaluation: {(domain_evaluation*100):.2f}, lr: {opt.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec2ee1",
   "metadata": {
    "cellUniqueIdByVincent": "e8792",
    "id": "2dec2ee1"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    # This method is called by _objective during an Optuna trial\n",
    "    def prepare_trial_run(self):\n",
    "        assert isinstance(self.trial, optuna.Trial), \"Trial not set!\"\n",
    "\n",
    "        # 1. Define Hyperparameters for this trial\n",
    "        #    a. Data/Loader parameters\n",
    "        window_length = self.trial.suggest_categorical(\"window_length\", [128, 256, 640]) # e.g. 64*2, 64*4, 64*10\n",
    "        batch_size = self.trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "\n",
    "        #    b. Model architecture parameters\n",
    "        kernLength = self.trial.suggest_categorical(\"kernLength\", [64, 128, 256])\n",
    "        F1 = self.trial.suggest_categorical(\"F1\", [8, 16, 32])\n",
    "        D = self.trial.suggest_categorical(\"D\", [1, 2, 3])\n",
    "        F2 = self.trial.suggest_categorical(\"F2\", [16, 32, 64]) # F2 must be F1 * D\n",
    "        hidden_dim = self.trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "        layer_dim = self.trial.suggest_categorical(\"layer_dim\", [1, 2, 3])\n",
    "        dropout = self.trial.suggest_float(\"dropout\", 0.1, 0.6)\n",
    "        \n",
    "        #    c. Optimizer parameters\n",
    "        lr = self.trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "        # 2. Prepare the data using these parameters\n",
    "        super()._prepare_data(is_trial=True, batch_size=batch_size, window_length=window_length)\n",
    "        \n",
    "        assert self.dataset is not None, \"Dataset was not created correctly\"\n",
    "        n_electrodes = self.dataset.datasets[0].data[0].shape[0] # Get shape from underlying dataset\n",
    "\n",
    "        # 3. Build the model and optimizer\n",
    "        self.model = SSVEPClassifier(\n",
    "            n_electrodes=n_electrodes, # Use value from data\n",
    "            dropout=dropout,\n",
    "            kernLength=kernLength,\n",
    "            F1=F1,\n",
    "            D=D,\n",
    "            F2=F1 * D, # F2 is dependent on F1 and D\n",
    "            hidden_dim=hidden_dim,\n",
    "            layer_dim=layer_dim,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    # This method is called by train() for the final run\n",
    "    def prepare_final_run(self):\n",
    "        # 1. Get the best hyperparameters from the completed study\n",
    "        study = self._get_study()\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # 2. Prepare data using the best params\n",
    "        super()._prepare_data(is_trial=False) # is_trial=False handles getting params from study\n",
    "        \n",
    "        assert self.dataset is not None, \"Dataset was not created correctly\"\n",
    "        n_electrodes = self.dataset.datasets[0].data[0].shape[0]\n",
    "\n",
    "        # 3. Build the final model and optimizer\n",
    "        self.model = SSVEPClassifier(\n",
    "            n_electrodes=n_electrodes,\n",
    "            out_dim=2,\n",
    "            dropout=best_params[\"dropout\"],\n",
    "            kernLength=best_params[\"kernLength\"],\n",
    "            F1=best_params[\"F1\"],\n",
    "            D=best_params[\"D\"],\n",
    "            F2=best_params[\"F1\"] * best_params[\"D\"],\n",
    "            hidden_dim=best_params[\"hidden_dim\"],\n",
    "            layer_dim=best_params[\"layer_dim\"],\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Optional: Load pre-existing weights if you are resuming\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.model_path))\n",
    "            print(f\"Loaded existing model weights from {self.model_path}\")\n",
    "        except Exception:\n",
    "            print(f\"No existing model weights found at {self.model_path}. Training from scratch.\")\n",
    "        \n",
    "        lr = 0.00018182233882257615 # best_params[\"lr\"]\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',        # “min” if you want to reduce lr when the quantity monitored has stopped decreasing\n",
    "            factor=0.5,        # new_lr = lr * factor\n",
    "            patience=20,        # number of epochs with no improvement after which lr will be reduced\n",
    "            threshold=1e-4,    # threshold for measuring the new optimum, to only focus on significant changes\n",
    "            threshold_mode='rel', # `'rel'` means compare change relative to best value. Could use `'abs'`.\n",
    "            cooldown=0,        # epochs to wait before resuming normal operation after lr has been reduced\n",
    "            min_lr=1e-6,       # lower bound on the lr\n",
    "        )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        data_path=data_path,\n",
    "        optuna_db_path=optuna_db_path,\n",
    "        model_path=model_path,\n",
    "        train_epochs=500, # Final training epochs\n",
    "        tune_epochs=50,   # Epochs per trial\n",
    "        optuna_n_trials=50,\n",
    "        task=\"mi\",\n",
    "        eeg_channels=eeg_channels,\n",
    "        data_fraction=0.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132bf51",
   "metadata": {
    "cellUniqueIdByVincent": "00f49",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "a132bf51",
    "outputId": "fce5db39-6f8c-4201-da84-09fc34a1779e"
   },
   "outputs": [],
   "source": [
    "delete_existing = False\n",
    "trainer.optimize(delete_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93294ea",
   "metadata": {
    "cellUniqueIdByVincent": "9a628",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "id": "a93294ea",
    "outputId": "e77ed180-3a33-492a-c29e-15a57befbbae"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d4e73",
   "metadata": {
    "cellUniqueIdByVincent": "72394",
    "id": "1d8d4e73",
    "outputId": "879ad0ae-f397-49f7-e3ba-7d299bdcecf0"
   },
   "outputs": [],
   "source": [
    "trainer._prepare_training(False)\n",
    "trainer.model.eval()\n",
    "f\"test accuracy: {evaluate_model(trainer.model, trainer.eval_loader, device)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icmtc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vincent": {
   "sessionId": "bb6251503a76299c2e1994d5_2025-06-21T14-24-00-772Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
