{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a563374",
   "metadata": {
    "cellUniqueIdByVincent": "b5c77",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "2a563374",
    "outputId": "2f5357e7-a3f2-46ab-a67e-a21a666bccbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (1.16.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\ahmad\\appdata\\roaming\\python\\python312\\site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (3.10.3)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (25.0)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (1.15.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\ahmad\\appdata\\roaming\\python\\python312\\site-packages (from pooch>=1.5->mne) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pooch>=1.5->mne) (2.32.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->mne) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "if not os.path.exists('./modules') and not os.path.exists('modules.zip'):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "if not os.path.exists('./modules') and os.path.exists('modules.zip'):\n",
    "    os.system('unzip modules.zip -d .')\n",
    "\n",
    "if not os.path.exists('./Models') and not os.path.exists('Models.zip'):\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "if not os.path.exists('./Models') and os.path.exists('Models.zip'):\n",
    "    os.system('unzip Models.zip -d .')\n",
    "\n",
    "!pip3 install optuna\n",
    "!pip3 install mne\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "from modules import Trainer\n",
    "from modules.competition_dataset_one_per_trial import EEGDataset\n",
    "from modules.utils import evaluate_model\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from Models.lfann_dann_gpt_generated import LFANN_DANN\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5bf7b0",
   "metadata": {
    "cellUniqueIdByVincent": "e6823",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8c5bf7b0",
    "outputId": "e981327e-6451-4cf6-8020-c4fb791c5b99"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/drive/\")\n",
    "# try:\n",
    "#     print()\n",
    "#     os.symlink(\"/content/drive/MyDrive/ai_data/eeg_detection/data\", \"./data\")\n",
    "#     os.symlink(\"/content/drive/MyDrive/ai_data/eeg_detection/checkpoints\", \"./checkpoints\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "data_path = \"./data/mtcaic3\"\n",
    "model_path = \"./checkpoints/mi/models/the_honored_one.pth\"\n",
    "optuna_db_path = \"./checkpoints/mi/optuna/the_honored_one.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a17b42a7",
   "metadata": {
    "cellUniqueIdByVincent": "285db",
    "id": "a17b42a7"
   },
   "outputs": [],
   "source": [
    "# Add this at the beginning of your notebook, after imports\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call this function before creating datasets and models\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42089fb8",
   "metadata": {
    "cellUniqueIdByVincent": "77c22",
    "id": "42089fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: torch.Size([2400, 3, 1125]), classes shape: torch.Size([2400, 2])\n",
      "data shape: torch.Size([50, 3, 1125]), classes shape: torch.Size([50, 2])\n",
      "data shape: torch.Size([50, 3, 1125]), classes shape: torch.Size([50, 2])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "eeg_channels = [\"C3\", \"PZ\", \"OZ\"] # [\"C3\", \"CZ\", \"C4\"]\n",
    "\n",
    "dataset_train = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    task=\"MI\",\n",
    "    split=\"train\",\n",
    "    tmin=0.5,  # skip first 0.5 s (125 samples)\n",
    "    win_len=int(4.5 * 250),  # 4.5 s window → 1125 samples\n",
    "    read_labels=True,\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "dataset_val = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    task=\"MI\",\n",
    "    split=\"validation\",\n",
    "    tmin=0.5,\n",
    "    win_len=int(4.5 * 250),\n",
    "    read_labels=True,\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(dataset_val,   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e76b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1125])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbd5f3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2]) torch.Size([5, 2])\n",
      "shit torch.Size([3, 1125])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0408, 0.0398],\n",
       "         [0.0477, 0.0364],\n",
       "         [0.0410, 0.0362]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2126, -0.2425,  0.1154,  0.1786, -0.2548, -0.1960,  0.1415, -0.0603,\n",
       "           0.1728, -0.0764,  0.0259,  0.1047, -0.0516, -0.5328, -0.0760, -0.4604,\n",
       "           0.0933, -0.2219, -0.3482,  0.0374, -0.0108,  0.2840,  0.1244,  0.0371,\n",
       "          -0.1087,  0.0408, -0.1810,  0.5047,  0.0199,  0.3573],\n",
       "         [-0.1168, -0.3141,  0.0854,  0.0682,  0.1217,  0.1356,  0.0760, -0.1201,\n",
       "           0.0260, -0.0507,  0.0843,  0.1459, -0.4855, -0.4441,  0.2663,  0.2306,\n",
       "           0.2954, -0.3287, -0.0633,  0.1073, -0.0595, -0.2343,  0.0434, -0.1195,\n",
       "           0.0137, -0.1080, -0.1419,  0.2733,  0.0249,  0.2188],\n",
       "         [-0.3697, -0.3438,  0.1704, -0.1303,  0.0367, -0.0027,  0.1749,  0.0914,\n",
       "          -0.1998, -0.0855,  0.1553,  0.1267, -0.1130, -0.5895,  0.0901,  0.0517,\n",
       "          -0.0385, -0.2807, -0.4176, -0.0926,  0.0929, -0.1906,  0.1936, -0.0084,\n",
       "           0.2421, -0.1091, -0.0352,  0.0433,  0.2944,  0.2003]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------- Gradient Reversal Layer ---------------- #\n",
    "class GradientReversalFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "class GradientReversal(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.alpha)\n",
    "\n",
    "# ---------------- LSTM Head ---------------- #\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        # x: B x seq_len x feat_dim\n",
    "        if h0 is None or c0 is None:\n",
    "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device)\n",
    "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# ---------------- EEG Feature Extractor ---------------- #\n",
    "class EEGFeatureExtractor(nn.Module):\n",
    "    def __init__(self, n_electrodes, kernLength, F1, D, F2, dropout):\n",
    "        super().__init__()\n",
    "        # For input B x C x T, apply 2D convs on [C x T]\n",
    "        self.block = nn.Sequential(\n",
    "            # Temporal conv across time\n",
    "            nn.Conv2d(1, F1, (1, kernLength), padding=(0, kernLength//2), bias=False),\n",
    "            nn.BatchNorm2d(F1),\n",
    "            # Depthwise spatial conv across electrodes\n",
    "            nn.Conv2d(F1, F1 * D, (n_electrodes, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout),\n",
    "            # Separable conv\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, 16), padding=(0, 8), groups=F1 * D, bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, 1, bias=False),\n",
    "            nn.BatchNorm2d(F2),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x C x T\n",
    "        x = x.unsqueeze(1)  # B x 1 x C x T\n",
    "        x = self.block(x)   # B x F2 x 1 x T_sub\n",
    "        x = x.squeeze(2)    # B x F2 x T_sub\n",
    "        x = x.permute(0, 2, 1)  # B x T_sub x F2\n",
    "        return x\n",
    "\n",
    "# ---------------- DANN SSVEP Classifier ---------------- #\n",
    "class DANN_SSVEPClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_electrodes=16,\n",
    "                 out_dim=4,\n",
    "                 dropout=0.25,\n",
    "                 kernLength=256,\n",
    "                 F1=96,\n",
    "                 D=1,\n",
    "                 F2=96,\n",
    "                 hidden_dim=100,\n",
    "                 layer_dim=1,\n",
    "                 grl_alpha=1.0,\n",
    "                 domain_classes=2):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = EEGFeatureExtractor(\n",
    "            n_electrodes, kernLength, F1, D, F2, dropout\n",
    "        )\n",
    "        feat_dim = F2\n",
    "        # label predictor (LSTM over time)\n",
    "        self.label_lstm = LSTMModel(feat_dim, hidden_dim, layer_dim, out_dim)\n",
    "        # domain predictor\n",
    "        self.grl = GradientReversal(alpha=grl_alpha)\n",
    "        self.domain_fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, domain_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B x C x T\n",
    "        feat_seq = self.feature_extractor(x)        # B x T_sub x feat_dim\n",
    "        class_out = self.label_lstm(feat_seq)\n",
    "        # domain prediction from time-averaged features\n",
    "        feat_avg = feat_seq.mean(dim=1)             # B x feat_dim\n",
    "        rev_feat = self.grl(feat_avg)\n",
    "        domain_out = self.domain_fc(rev_feat)\n",
    "        return class_out, domain_out\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dummy_x = torch.randn(5, 3, 256).to(device)  # B x C=3 x T=256\n",
    "    model = DANN_SSVEPClassifier(\n",
    "        n_electrodes=3,\n",
    "        out_dim=2,\n",
    "        kernLength=256,\n",
    "        F1=32,\n",
    "        D=3,\n",
    "        F2=32,\n",
    "        hidden_dim=256,\n",
    "        layer_dim=3,\n",
    "        grl_alpha=0.1,\n",
    "        domain_classes=2\n",
    "    ).to(device)\n",
    "    class_logits, domain_logits = model(dummy_x)\n",
    "    print(class_logits.shape, domain_logits.shape)  # (5,2), (5,2)\n",
    "\n",
    "\n",
    "print(\"shit\", dataset_train[0][0].shape)\n",
    "C, T = dataset_train[0][0].shape\n",
    "model = DANN_SSVEPClassifier(\n",
    "    n_electrodes=C,\n",
    "    dropout=0.26211635308091535,\n",
    "    out_dim=2,\n",
    "    kernLength=8,\n",
    "    F1 = 32,\n",
    "    D = 3,\n",
    "    F2 = 32,\n",
    "    hidden_dim=256,\n",
    "    layer_dim=2,\n",
    "    domain_classes=30,\n",
    ").to(device)\n",
    "\n",
    "inp = torch.randn(3, C, T).to(device)\n",
    "out = model(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71194cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_losses_label = []\n",
    "avg_losses_domain = []\n",
    "val_label_accuracies = []\n",
    "val_domain_accuracies = []\n",
    "train_label_accuracies = []\n",
    "train_domain_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b164b5",
   "metadata": {
    "cellUniqueIdByVincent": "c9b4e",
    "id": "12b164b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: avg_loss_label: 0.693, avg_loss_domain: 3.361, train_label_acc: 49.75%, train_domain_acc: 3.62%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "5: avg_loss_label: 0.694, avg_loss_domain: 3.264, train_label_acc: 49.71%, train_domain_acc: 6.88%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "5: avg_loss_label: 0.694, avg_loss_domain: 3.264, train_label_acc: 49.71%, train_domain_acc: 6.88%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "10: avg_loss_label: 0.693, avg_loss_domain: 3.440, train_label_acc: 49.58%, train_domain_acc: 3.96%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "10: avg_loss_label: 0.693, avg_loss_domain: 3.440, train_label_acc: 49.58%, train_domain_acc: 3.96%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "15: avg_loss_label: 0.693, avg_loss_domain: 3.620, train_label_acc: 50.04%, train_domain_acc: 4.62%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "15: avg_loss_label: 0.693, avg_loss_domain: 3.620, train_label_acc: 50.04%, train_domain_acc: 4.62%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "20: avg_loss_label: 0.694, avg_loss_domain: 3.085, train_label_acc: 50.67%, train_domain_acc: 9.71%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "20: avg_loss_label: 0.694, avg_loss_domain: 3.085, train_label_acc: 50.67%, train_domain_acc: 9.71%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "25: avg_loss_label: 0.693, avg_loss_domain: 3.455, train_label_acc: 50.25%, train_domain_acc: 5.92%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "25: avg_loss_label: 0.693, avg_loss_domain: 3.455, train_label_acc: 50.25%, train_domain_acc: 5.92%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "30: avg_loss_label: 0.693, avg_loss_domain: 3.187, train_label_acc: 50.21%, train_domain_acc: 9.04%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "30: avg_loss_label: 0.693, avg_loss_domain: 3.187, train_label_acc: 50.21%, train_domain_acc: 9.04%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "35: avg_loss_label: 0.693, avg_loss_domain: 3.302, train_label_acc: 50.88%, train_domain_acc: 6.17%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "35: avg_loss_label: 0.693, avg_loss_domain: 3.302, train_label_acc: 50.88%, train_domain_acc: 6.17%, label_evaluation: 52.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "40: avg_loss_label: 0.692, avg_loss_domain: 3.384, train_label_acc: 51.71%, train_domain_acc: 4.33%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "40: avg_loss_label: 0.692, avg_loss_domain: 3.384, train_label_acc: 51.71%, train_domain_acc: 4.33%, label_evaluation: 44.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "45: avg_loss_label: 0.691, avg_loss_domain: 3.394, train_label_acc: 51.08%, train_domain_acc: 3.92%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "45: avg_loss_label: 0.691, avg_loss_domain: 3.394, train_label_acc: 51.08%, train_domain_acc: 3.92%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "50: avg_loss_label: 0.691, avg_loss_domain: 3.397, train_label_acc: 50.88%, train_domain_acc: 4.21%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "50: avg_loss_label: 0.691, avg_loss_domain: 3.397, train_label_acc: 50.88%, train_domain_acc: 4.21%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "55: avg_loss_label: 0.691, avg_loss_domain: 3.401, train_label_acc: 51.96%, train_domain_acc: 3.54%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "55: avg_loss_label: 0.691, avg_loss_domain: 3.401, train_label_acc: 51.96%, train_domain_acc: 3.54%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "60: avg_loss_label: 0.690, avg_loss_domain: 3.396, train_label_acc: 53.21%, train_domain_acc: 4.21%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "60: avg_loss_label: 0.690, avg_loss_domain: 3.396, train_label_acc: 53.21%, train_domain_acc: 4.21%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "65: avg_loss_label: 0.689, avg_loss_domain: 3.395, train_label_acc: 53.04%, train_domain_acc: 3.92%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n",
      "65: avg_loss_label: 0.689, avg_loss_domain: 3.395, train_label_acc: 53.04%, train_domain_acc: 3.92%, label_evaluation: 48.00, domain_evaluation: 0.00, lr: 0.0003746351873334935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m loss = loss_label + loss_domain\n\u001b[32m     42\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m opt.step()\n\u001b[32m     46\u001b[39m avg_loss_label += loss_label.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "except Exception:\n",
    "    print(\"skipping model loading...\")\n",
    "\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0003746351873334935)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt,\n",
    "    mode=\"min\",  # “min” if you want to reduce lr when the quantity monitored has stopped decreasing\n",
    "    factor=0.5,  # new_lr = lr * factor\n",
    "    patience=20,  # number of epochs with no improvement after which lr will be reduced\n",
    "    threshold=1e-4,  # threshold for measuring the new optimum, to only focus on significant changes\n",
    "    threshold_mode=\"rel\",  # `'rel'` means compare change relative to best value. Could use `'abs'`.\n",
    "    cooldown=0,  # epochs to wait before resuming normal operation after lr has been reduced\n",
    "    min_lr=1e-6,  # lower bound on the lr\n",
    ")\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_loss_label = 0\n",
    "    avg_loss_domain = 0\n",
    "    correct_label = 0\n",
    "    correct_domain = 0\n",
    "    total = 0\n",
    "    model.train()\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).to(torch.int64) # shape: [Bx2], 0: label, 1: domain\n",
    "        y_labels = y[:, 0]\n",
    "        y_subj = y[:, 1]\n",
    "\n",
    "        y_pred_labels, y_pred_domain = model(x)\n",
    "\n",
    "        loss_label = criterion(y_pred_labels, y_labels)\n",
    "        loss_domain = criterion(y_pred_domain, y_subj)\n",
    "        loss = loss_label + loss_domain\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        avg_loss_label += loss_label.item()\n",
    "        avg_loss_domain += loss_domain.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        _, pred_labels = torch.max(y_pred_labels, 1)\n",
    "        _, pred_domains = torch.max(y_pred_domain, 1)\n",
    "        correct_label += (pred_labels == y_labels).sum().item()\n",
    "        correct_domain += (pred_domains == y_subj).sum().item()\n",
    "        total += y_labels.size(0)\n",
    "\n",
    "    avg_loss_label /= len(train_loader)\n",
    "    avg_loss_domain /= len(train_loader)\n",
    "    avg_losses_label.append(avg_loss_label)\n",
    "    avg_losses_domain.append(avg_loss_domain)\n",
    "    train_label_acc = 100.0 * correct_label / total\n",
    "    train_domain_acc = 100.0 * correct_domain / total\n",
    "    train_label_accuracies.append(train_label_acc)\n",
    "    train_domain_accuracies.append(train_domain_acc)\n",
    "    # scheduler.step(avg_loss_label)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        label_evaluation, domain_evaluation = evaluate_model(model, val_loader, device)\n",
    "        val_label_accuracies.append(label_evaluation)\n",
    "        val_domain_accuracies.append(domain_evaluation)\n",
    "        model.cpu()\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model.to(device)\n",
    "        print(f\"{epoch}: avg_loss_label: {avg_loss_label:.3F}, avg_loss_domain: {avg_loss_domain:.3F}, train_label_acc: {train_label_acc:.2f}%, train_domain_acc: {train_domain_acc:.2f}%, label_evaluation: {(label_evaluation*100):.2f}, domain_evaluation: {(domain_evaluation*100):.2f}, lr: {opt.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec2ee1",
   "metadata": {
    "cellUniqueIdByVincent": "e8792",
    "id": "2dec2ee1"
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    # This method is called by _objective during an Optuna trial\n",
    "    def prepare_trial_run(self):\n",
    "        assert isinstance(self.trial, optuna.Trial), \"Trial not set!\"\n",
    "\n",
    "        # 1. Define Hyperparameters for this trial\n",
    "        #    a. Data/Loader parameters\n",
    "        window_length = self.trial.suggest_categorical(\"window_length\", [128, 256, 640]) # e.g. 64*2, 64*4, 64*10\n",
    "        batch_size = self.trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "\n",
    "        #    b. Model architecture parameters\n",
    "        kernLength = self.trial.suggest_categorical(\"kernLength\", [64, 128, 256])\n",
    "        F1 = self.trial.suggest_categorical(\"F1\", [8, 16, 32])\n",
    "        D = self.trial.suggest_categorical(\"D\", [1, 2, 3])\n",
    "        F2 = self.trial.suggest_categorical(\"F2\", [16, 32, 64]) # F2 must be F1 * D\n",
    "        hidden_dim = self.trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
    "        layer_dim = self.trial.suggest_categorical(\"layer_dim\", [1, 2, 3])\n",
    "        dropout = self.trial.suggest_float(\"dropout\", 0.1, 0.6)\n",
    "        \n",
    "        #    c. Optimizer parameters\n",
    "        lr = self.trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "        # 2. Prepare the data using these parameters\n",
    "        super()._prepare_data(is_trial=True, batch_size=batch_size, window_length=window_length)\n",
    "        \n",
    "        assert self.dataset is not None, \"Dataset was not created correctly\"\n",
    "        n_electrodes = self.dataset.datasets[0].data[0].shape[0] # Get shape from underlying dataset\n",
    "\n",
    "        # 3. Build the model and optimizer\n",
    "        self.model = SSVEPClassifier(\n",
    "            n_electrodes=n_electrodes, # Use value from data\n",
    "            dropout=dropout,\n",
    "            kernLength=kernLength,\n",
    "            F1=F1,\n",
    "            D=D,\n",
    "            F2=F1 * D, # F2 is dependent on F1 and D\n",
    "            hidden_dim=hidden_dim,\n",
    "            layer_dim=layer_dim,\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    # This method is called by train() for the final run\n",
    "    def prepare_final_run(self):\n",
    "        # 1. Get the best hyperparameters from the completed study\n",
    "        study = self._get_study()\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # 2. Prepare data using the best params\n",
    "        super()._prepare_data(is_trial=False) # is_trial=False handles getting params from study\n",
    "        \n",
    "        assert self.dataset is not None, \"Dataset was not created correctly\"\n",
    "        n_electrodes = self.dataset.datasets[0].data[0].shape[0]\n",
    "\n",
    "        # 3. Build the final model and optimizer\n",
    "        self.model = SSVEPClassifier(\n",
    "            n_electrodes=n_electrodes,\n",
    "            out_dim=2,\n",
    "            dropout=best_params[\"dropout\"],\n",
    "            kernLength=best_params[\"kernLength\"],\n",
    "            F1=best_params[\"F1\"],\n",
    "            D=best_params[\"D\"],\n",
    "            F2=best_params[\"F1\"] * best_params[\"D\"],\n",
    "            hidden_dim=best_params[\"hidden_dim\"],\n",
    "            layer_dim=best_params[\"layer_dim\"],\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Optional: Load pre-existing weights if you are resuming\n",
    "        try:\n",
    "            self.model.load_state_dict(torch.load(self.model_path))\n",
    "            print(f\"Loaded existing model weights from {self.model_path}\")\n",
    "        except Exception:\n",
    "            print(f\"No existing model weights found at {self.model_path}. Training from scratch.\")\n",
    "        \n",
    "        lr = 0.00018182233882257615 # best_params[\"lr\"]\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',        # “min” if you want to reduce lr when the quantity monitored has stopped decreasing\n",
    "            factor=0.5,        # new_lr = lr * factor\n",
    "            patience=20,        # number of epochs with no improvement after which lr will be reduced\n",
    "            threshold=1e-4,    # threshold for measuring the new optimum, to only focus on significant changes\n",
    "            threshold_mode='rel', # `'rel'` means compare change relative to best value. Could use `'abs'`.\n",
    "            cooldown=0,        # epochs to wait before resuming normal operation after lr has been reduced\n",
    "            min_lr=1e-6,       # lower bound on the lr\n",
    "        )\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        data_path=data_path,\n",
    "        optuna_db_path=optuna_db_path,\n",
    "        model_path=model_path,\n",
    "        train_epochs=500, # Final training epochs\n",
    "        tune_epochs=50,   # Epochs per trial\n",
    "        optuna_n_trials=50,\n",
    "        task=\"mi\",\n",
    "        eeg_channels=eeg_channels,\n",
    "        data_fraction=0.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132bf51",
   "metadata": {
    "cellUniqueIdByVincent": "00f49",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "a132bf51",
    "outputId": "fce5db39-6f8c-4201-da84-09fc34a1779e"
   },
   "outputs": [],
   "source": [
    "delete_existing = False\n",
    "trainer.optimize(delete_existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93294ea",
   "metadata": {
    "cellUniqueIdByVincent": "9a628",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "id": "a93294ea",
    "outputId": "e77ed180-3a33-492a-c29e-15a57befbbae"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d4e73",
   "metadata": {
    "cellUniqueIdByVincent": "72394",
    "id": "1d8d4e73",
    "outputId": "879ad0ae-f397-49f7-e3ba-7d299bdcecf0"
   },
   "outputs": [],
   "source": [
    "trainer._prepare_training(False)\n",
    "trainer.model.eval()\n",
    "f\"test accuracy: {evaluate_model(trainer.model, trainer.eval_loader, device)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vincent": {
   "sessionId": "bb6251503a76299c2e1994d5_2025-06-21T14-24-00-772Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
