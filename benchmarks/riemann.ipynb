{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f517e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from modules.competition_dataset import EEGDataset\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from scipy.linalg import expm, logm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.utils.mean import mean_riemann\n",
    "from pyriemann.utils.distance import distance_riemann\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.linalg import logm, inv\n",
    "from pyriemann.utils.mean import mean_riemann\n",
    "from scipy.optimize import minimize\n",
    "from pyriemann.spatialfilters import CSP\n",
    "from pyriemann.classification import FgMDM\n",
    "from scipy.signal import sosfiltfilt, butter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c412b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/mtcaic3'\n",
    "lda_model_path = './checkpoints/mi/models/lda_mi.pkl'\n",
    "\n",
    "# Add this at the beginning of your notebook, after imports\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f62b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 1000\n",
    "stride = window_length // 2\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac48c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: mi, split: train, domain: time, data_fraction: 0.4\n",
      "Using 40.0% of data: 960/960 samples\n",
      "skipped: 16/960\n",
      "task: mi, split: validation, domain: time, data_fraction: 1.0\n",
      "skipped: 1/50\n",
      "task: mi, split: test, domain: time, data_fraction: 1.0\n",
      "skipped: 0/50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_channels = [\n",
    "    \"FZ\",\n",
    "    \"C3\",\n",
    "    \"CZ\",\n",
    "    \"C4\",\n",
    "    # \"PZ\",\n",
    "    \"PO7\",\n",
    "    # \"OZ\",\n",
    "    \"PO8\",\n",
    "]\n",
    "\n",
    "dataset_train = EEGDataset(\n",
    "    data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    task=\"mi\",\n",
    "    split=\"train\",\n",
    "    data_fraction=0.4,\n",
    "    tmin=250,\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "dataset_val = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    task='mi',\n",
    "    split='validation',\n",
    "    read_labels=True,\n",
    "    tmin=250,\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "dataset_test = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    task='mi',\n",
    "    split='test',\n",
    "    read_labels=False,\n",
    "    tmin=250,\n",
    "    eeg_channels=eeg_channels,\n",
    ")\n",
    "\n",
    "dataset_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "246cdd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0235, 0.0523, 0.0713, 0.0441, 0.0258, 0.0490]),\n",
       " tensor([0.9722, 0.9950, 1.0636, 1.0019, 0.9947, 0.9900]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = torch.cat([torch.stack([x for x,_ in ds]) for ds in (dataset_train, dataset_val, dataset_test)])\n",
    "X_val_train = torch.cat([torch.stack([x for x,_ in ds]) for ds in (dataset_train, dataset_val)])\n",
    "y_val_train = torch.cat([torch.stack([y for _,y in ds]) for ds in (dataset_train, dataset_val)])\n",
    "\n",
    "mean = all_data.mean((0, 2))\n",
    "std = all_data.std((0, 2))\n",
    "\n",
    "X_val_train = (X_val_train - mean[None, :, None]) / std[None, :, None]\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "653aa401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- F-scores for each channel (higher score indicates more informativeness) ---\n",
      "  CZ: 7069.08\n",
      "  C4: 4741.66\n",
      "  PO8: 2923.19\n",
      "  PO7: 2572.96\n",
      "  C3: 2323.86\n",
      "  FZ: 1957.84\n",
      "\n",
      "--- Recommended Top 3 Channels based on F-score: ['CZ', 'C4', 'PO8'] ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Concatenate all splits (add dataset_val and dataset_test if needed)\n",
    "X_all = np.concatenate([\n",
    "    dataset_train.data.numpy(),\n",
    "    dataset_val.data.numpy(),\n",
    "    dataset_test.data.numpy(),\n",
    "], axis=0)  # shape: [N_total, C, ...]\n",
    "y_all = np.concatenate([\n",
    "    dataset_train.labels.numpy(),\n",
    "    dataset_val.labels.numpy(),\n",
    "    dataset_test.labels.numpy(),\n",
    "], axis=0)  # shape: [N_total]\n",
    "\n",
    "# Detect shape and adapt\n",
    "if X_all.ndim == 3:\n",
    "    # [B, C, T]\n",
    "    num_samples, num_channels, time_points = X_all.shape\n",
    "    channel_f_scores = []\n",
    "    for i in range(num_channels):\n",
    "        channel_data = X_all[:, i, :]  # [N_total, T]\n",
    "        f_scores_per_timepoint, _ = f_classif(channel_data, y_all)\n",
    "        aggregated_f_score = np.sum(f_scores_per_timepoint)\n",
    "        channel_f_scores.append(aggregated_f_score)\n",
    "elif X_all.ndim == 4:\n",
    "    # [B, C, F, T]\n",
    "    num_samples, num_channels, freq_points, time_points = X_all.shape\n",
    "    channel_f_scores = []\n",
    "    for i in range(num_channels):\n",
    "        # Average over freq and time for each channel\n",
    "        channel_data = X_all[:, i, :, :].mean(axis=(1, 2))  # [N_total]\n",
    "        f_score, _ = f_classif(channel_data.reshape(-1, 1), y_all)\n",
    "        channel_f_scores.append(f_score[0])\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported data shape: {X_all.shape}\")\n",
    "\n",
    "# Optionally, map to channel names\n",
    "original_channel_names = eeg_channels\n",
    "channel_scores_dict = {original_channel_names[i]: channel_f_scores[i] for i in range(num_channels)}\n",
    "\n",
    "print(\"\\n--- F-scores for each channel (higher score indicates more informativeness) ---\")\n",
    "sorted_channels = sorted(channel_scores_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "for channel, score in sorted_channels:\n",
    "    print(f\"  {channel}: {score:.2f}\")\n",
    "\n",
    "top_3_channels = [channel for channel, score in sorted_channels[:3]]\n",
    "print(f\"\\n--- Recommended Top 3 Channels based on F-score: {top_3_channels} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "afa38ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for train/val/test\n",
    "X_train = np.stack([x.numpy() for x, y in dataset_train])  # shape: [N, C, T]\n",
    "y_train = np.array([y[0] for x, y in dataset_train])\n",
    "\n",
    "X_val = np.stack([x.numpy() for x, y in dataset_val])\n",
    "y_val = np.array([y[0] for x, y in dataset_val])\n",
    "\n",
    "X_test = np.stack([x.numpy() for x, y in dataset_test])\n",
    "y_test = np.array([y[0] for x, y in dataset_test])\n",
    "\n",
    "# Cheaty :)\n",
    "X_train_full = np.concatenate([X_train, X_val], axis=0)\n",
    "y_train_full = np.concatenate([y_train, y_val], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "541ba886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc = 0.562 | Val Acc = 0.471\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"clf\", FgMDM()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "train_acc = accuracy_score(y_train, pipeline.predict(X_train))\n",
    "val_acc = accuracy_score(y_val, pipeline.predict(X_val))\n",
    "\n",
    "print(f\"Train Acc = {train_acc:.3f} | Val Acc = {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0d0dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d= 4, eps=1.0e-06, C=0.01: train=0.529, val=0.494\n",
      "d= 4, eps=1.0e-06, C=0.1: train=0.546, val=0.506\n",
      "d= 4, eps=1.0e-06, C=1: train=0.543, val=0.471\n",
      "d= 4, eps=1.0e-06, C=10: train=0.540, val=0.447\n",
      "d= 4, eps=1.0e-04, C=0.01: train=0.569, val=0.424\n",
      "d= 4, eps=1.0e-04, C=0.1: train=0.554, val=0.459\n",
      "d= 4, eps=1.0e-04, C=1: train=0.545, val=0.447\n",
      "d= 4, eps=1.0e-04, C=10: train=0.540, val=0.494\n",
      "d= 4, eps=1.0e-02, C=0.01: train=0.555, val=0.494\n",
      "d= 4, eps=1.0e-02, C=0.1: train=0.537, val=0.435\n",
      "d= 4, eps=1.0e-02, C=1: train=0.557, val=0.388\n",
      "d= 4, eps=1.0e-02, C=10: train=0.522, val=0.471\n",
      "d= 8, eps=1.0e-06, C=0.01: train=0.565, val=0.400\n",
      "d= 8, eps=1.0e-06, C=0.1: train=0.565, val=0.412\n",
      "d= 8, eps=1.0e-06, C=1: train=0.562, val=0.388\n",
      "d= 8, eps=1.0e-06, C=10: train=0.568, val=0.435\n",
      "d= 8, eps=1.0e-04, C=0.01: train=0.572, val=0.435\n",
      "d= 8, eps=1.0e-04, C=0.1: train=0.568, val=0.424\n",
      "d= 8, eps=1.0e-04, C=1: train=0.566, val=0.400\n",
      "d= 8, eps=1.0e-04, C=10: train=0.572, val=0.424\n",
      "d= 8, eps=1.0e-02, C=0.01: train=0.569, val=0.447\n",
      "d= 8, eps=1.0e-02, C=0.1: train=0.567, val=0.388\n",
      "d= 8, eps=1.0e-02, C=1: train=0.561, val=0.388\n",
      "d= 8, eps=1.0e-02, C=10: train=0.572, val=0.388\n",
      "d=12, eps=1.0e-06, C=0.01: train=0.556, val=0.376\n",
      "d=12, eps=1.0e-06, C=0.1: train=0.564, val=0.400\n",
      "d=12, eps=1.0e-06, C=1: train=0.562, val=0.400\n",
      "d=12, eps=1.0e-06, C=10: train=0.562, val=0.388\n",
      "d=12, eps=1.0e-04, C=0.01: train=0.550, val=0.400\n",
      "d=12, eps=1.0e-04, C=0.1: train=0.559, val=0.388\n",
      "d=12, eps=1.0e-04, C=1: train=0.565, val=0.388\n",
      "d=12, eps=1.0e-04, C=10: train=0.574, val=0.388\n",
      "d=12, eps=1.0e-02, C=0.01: train=0.565, val=0.376\n",
      "d=12, eps=1.0e-02, C=0.1: train=0.563, val=0.388\n",
      "d=12, eps=1.0e-02, C=1: train=0.564, val=0.400\n",
      "d=12, eps=1.0e-02, C=10: train=0.562, val=0.388\n",
      "Best config: {'d': 4, 'eps': 1e-06, 'C': 0.1, 'score': 0.5058823529411764}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.utils.mean import mean_riemann\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from scipy.linalg import inv, logm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def fit_rsf(X, y, d, eps, maxiter=500):\n",
    "    # 1) compute class means\n",
    "    covs = Covariances(estimator=\"lwf\").transform(X)\n",
    "    C1 = mean_riemann(covs[y == 0])\n",
    "    C2 = mean_riemann(covs[y == 1])\n",
    "    Nc = C1.shape[0]\n",
    "    W0 = np.random.randn(Nc, d)\n",
    "\n",
    "    def J(w):\n",
    "        W = w.reshape(Nc, d)\n",
    "        S1 = W.T @ C1 @ W + eps * np.eye(d)\n",
    "        S2 = W.T @ C2 @ W + eps * np.eye(d)\n",
    "        return np.linalg.norm(logm(inv(S1) @ S2), \"fro\") ** 2\n",
    "\n",
    "    def grad(w):\n",
    "        W = w.reshape(Nc, d)\n",
    "        S1 = W.T @ C1 @ W + eps * np.eye(d)\n",
    "        S2 = W.T @ C2 @ W + eps * np.eye(d)\n",
    "        A = inv(S1) @ S2\n",
    "        L = logm(A)\n",
    "        term1 = 2 * C1.dot(W).dot(inv(S1)).dot(L).dot(inv(S1))\n",
    "        term2 = 2 * C2.dot(W).dot(inv(S2)).dot(L).dot(inv(S2))\n",
    "        return (term1 - term2).ravel()\n",
    "\n",
    "    res = minimize(lambda w: -J(w), W0.ravel(), jac=lambda w: -grad(w), method=\"BFGS\", options={\"maxiter\": maxiter})\n",
    "    return res.x.reshape(Nc, d)\n",
    "\n",
    "\n",
    "def evaluate_rsf(X_train, y_train, X_val, y_val, d, eps, C_lr):\n",
    "    W = fit_rsf(X_train, y_train, d, eps)\n",
    "    # build the rest of the pipeline\n",
    "    # 1) project\n",
    "    Xf_train = np.einsum(\"cd,ect->edt\", W, X_train)\n",
    "    Xf_val = np.einsum(\"cd,ect->edt\", W, X_val)\n",
    "    # 2) covariances\n",
    "    cov_train = Covariances(\"lwf\").transform(Xf_train)\n",
    "    cov_val = Covariances(\"lwf\").transform(Xf_val)\n",
    "    # 3) tangent‑space\n",
    "    ts = TangentSpace()\n",
    "    feat_train = ts.fit_transform(cov_train)\n",
    "    feat_val = ts.transform(cov_val)\n",
    "    # 4) classify\n",
    "    clf = LogisticRegression(C=C_lr, max_iter=500)\n",
    "    clf.fit(feat_train, y_train)\n",
    "    return clf.score(feat_train, y_train), clf.score(feat_val, y_val)\n",
    "\n",
    "\n",
    "# define your grid\n",
    "grid_d = [4, 8, 12]\n",
    "grid_eps = [1e-6, 1e-4, 1e-2]\n",
    "grid_C = [0.01, 0.1, 1, 10]\n",
    "\n",
    "best = {\"score\": -np.inf}\n",
    "for d, eps, C_lr in product(grid_d, grid_eps, grid_C):\n",
    "    tr, va = evaluate_rsf(X_train, y_train, X_val, y_val, d, eps, C_lr)\n",
    "    print(f\"d={d:2d}, eps={eps:.1e}, C={C_lr}: train={tr:.3f}, val={va:.3f}\")\n",
    "    if va > best[\"score\"]:\n",
    "        best = {\"d\": d, \"eps\": eps, \"C\": C_lr, \"score\": va}\n",
    "\n",
    "print(\"Best config:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cc4ebb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged RSF W shape: (6, 8)\n"
     ]
    }
   ],
   "source": [
    "covs = Covariances(estimator=\"lwf\").transform(X_train)  # shape: (n_epochs, C, C)\n",
    "# 1. Compute class‐mean covariances C1, C2 from your epochs X_epochs (shape: n_epochs × C × T)\n",
    "C1 = mean_riemann(covs[y_train == 0])  # class 0 mean SPD\n",
    "C2 = mean_riemann(covs[y_train == 1])  # class 1 mean SPD\n",
    "\n",
    "Nc = C1.shape[0]  # number of channels\n",
    "d = 8  # desired RSF dimension (e.g. 8)\n",
    "W0 = np.random.randn(Nc, d)  # random initialization\n",
    "\n",
    "\n",
    "# 2. Objective: maximize δ_R^2(WᵀC1W, WᵀC2W) :contentReference[oaicite:0]{index=0}\n",
    "def J(W_flat):\n",
    "    W = W_flat.reshape(Nc, d)\n",
    "    eps = 1e-6\n",
    "    S1 = W.T @ C1 @ W + np.eye(d) * eps\n",
    "    S2 = W.T @ C2 @ W + np.eye(d) * eps\n",
    "    # affine‐invariant Riemannian distance squared:\n",
    "    return np.linalg.norm(logm(inv(S1) @ S2), \"fro\") ** 2\n",
    "\n",
    "\n",
    "# 3. Gradient ∇J(W) from Eq. (5) :contentReference[oaicite:1]{index=1}\n",
    "def grad_J(W_flat):\n",
    "    W = W_flat.reshape(Nc, d)\n",
    "    eps = 1e-6\n",
    "    S1 = W.T @ C1 @ W + np.eye(d) * eps\n",
    "    S2 = W.T @ C2 @ W + np.eye(d) * eps\n",
    "    A = inv(S1) @ S2\n",
    "    L = logm(A)\n",
    "    # full formula is a bit long; here’s the structure:\n",
    "    #    ∇J = 2 · C1 · W · S1^{-1} · L · S1^{-1}\n",
    "    #         − 2 · C2 · W · S2^{-1} · L · S2^{-1}\n",
    "    term1 = 2 * C1.dot(W).dot(inv(S1)).dot(L).dot(inv(S1))\n",
    "    term2 = 2 * C2.dot(W).dot(inv(S2)).dot(L).dot(inv(S2))\n",
    "    return (term1 - term2).ravel()\n",
    "\n",
    "\n",
    "# 4. Run BFGS to maximize J (we minimize -J)\n",
    "res = minimize(lambda w: -J(w), W0.ravel(), jac=lambda w: -grad_J(w), method=\"BFGS\", options={\"maxiter\": 1000, \"gtol\": 1e-8})\n",
    "\n",
    "W_opt = res.x.reshape(Nc, d)\n",
    "print(\"Converged RSF W shape:\", W_opt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2c784e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc = 0.566 | Val Acc = 0.494\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RSFTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Project multichannel epochs X (n_epochs, C, T) to d channels via W_opt.\"\"\"\n",
    "    def __init__(self, W):\n",
    "        self.W = W  # shape (C, d)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X: (n_epochs, C, T) → Xf: (n_epochs, d, T)\n",
    "        return np.einsum('cd,ect->edt', self.W, X)\n",
    "\n",
    "# Build the pipeline\n",
    "rsf_pipe = make_pipeline(\n",
    "    RSFTransformer(W_opt),                     # step 1: apply RSF\n",
    "    Covariances(estimator='lwf'),              # step 2: covariances on d channels\n",
    "    TangentSpace(metric='riemann'),            # step 3: tangent‑space embedding\n",
    "    LogisticRegression(random_state=42, tol=0.0001, solver='sag', penalty='l2', max_iter=1000, class_weight='balanced', C=0.001)          # step 4: classifier\n",
    ")\n",
    "\n",
    "# Fit + evaluate\n",
    "rsf_pipe.fit(X_train, y_train)\n",
    "train_acc = rsf_pipe.score(X_train, y_train)\n",
    "val_acc   = rsf_pipe.score(X_val,   y_val)\n",
    "\n",
    "print(f\"Train Acc = {train_acc:.3f} | Val Acc = {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "adff2805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Performance Report ===\n",
      "                 SVM: Train Acc = 0.653 | Val Acc = 0.435\n",
      " Logistic Regression: Train Acc = 0.567 | Val Acc = 0.506\n",
      "                 MDM: Train Acc = 0.515 | Val Acc = 0.518\n"
     ]
    }
   ],
   "source": [
    "pipeline_svm = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"tangent\", TangentSpace(metric=\"riemann\")),\n",
    "        (\"clf\", SVC(random_state=42, tol=0.001, kernel='rbf', gamma=0.01, class_weight='balanced', C=10)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"tangent\", TangentSpace(metric=\"riemann\")),\n",
    "        (\"clf\", LogisticRegression(random_state=42, tol=0.0001, solver='sag', penalty='l2', max_iter=1000, class_weight='balanced', C=0.001)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_mdm = Pipeline([\n",
    "    ('cov', Covariances(estimator=\"lwf\")),\n",
    "    (\"clf\", MDM()),\n",
    "])\n",
    "\n",
    "pipelines = {\n",
    "    \"SVM\": pipeline_svm,\n",
    "    \"Logistic Regression\": pipeline_lr,\n",
    "    \"MDM\": pipeline_mdm,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    train_acc = accuracy_score(y_train, pipe.predict(X_train))\n",
    "    val_acc = accuracy_score(y_val, pipe.predict(X_val))\n",
    "    results[name] = (train_acc, val_acc)\n",
    "\n",
    "print(\"=== Model Performance Report ===\")\n",
    "for name, (train_acc, val_acc) in results.items():\n",
    "    print(f\"{name:>20}: Train Acc = {train_acc:.3f} | Val Acc = {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# Best CV score:    0.5679161628375655\n",
    "# Best parameters:  {'clf__tol': 0.001, 'clf__kernel': 'rbf', 'clf__gamma': 0.01, 'clf__class_weight': 'balanced', 'clf__C': 10}\n",
    "# Validation accuracy: 0.5145502645502645\n",
    "\n",
    "param_grid_svm = {\n",
    "    \"clf__kernel\": [\"rbf\", \"linear\"],  # Drop poly - research shows overfitting\n",
    "    \"clf__C\": [0.01, 0.1, 1, 10, 50],  # Add lower values\n",
    "    \"clf__gamma\": [0.001, 0.01, 0.1, \"scale\"],  # Finer granularity\n",
    "    \"clf__class_weight\": [\"balanced\"],\n",
    "    \"clf__tol\": [1e-3, 1e-4],\n",
    "    # Remove degree/coef0 (irrelevant for non-poly kernels)\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"tangent\", TangentSpace(metric=\"riemann\")),\n",
    "        (\"clf\", SVC(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    n_iter=50,\n",
    "    param_distributions=param_grid_svm,\n",
    "    # param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 3) Inspect best params & CV score\n",
    "print(\"Best CV score:   \", grid.best_score_)\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "\n",
    "# # 4) Evaluate on validation set\n",
    "best_model = grid.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "# Best CV score:    0.5507187961843343\n",
    "# Best parameters:  {'clf__tol': 0.0001, 'clf__solver': 'sag', 'clf__penalty': 'l2', 'clf__max_iter': 1000, 'clf__class_weight': 'balanced', 'clf__C': 0.001}\n",
    "# Validation accuracy: 0.5066137566137566\n",
    "\n",
    "param_grid_lr = {\n",
    "    \"clf__penalty\": [\"l2\", None],\n",
    "    \"clf__C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "    \"clf__solver\": [\"lbfgs\", \"sag\"],\n",
    "    \"clf__max_iter\": [1000],\n",
    "    \"clf__class_weight\": [\"balanced\"],\n",
    "    \"clf__tol\": [1e-4]\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"tangent\", TangentSpace(metric=\"riemann\")),\n",
    "        (\"clf\", LogisticRegression(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grid_lr = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    n_iter=70,\n",
    "    param_distributions=param_grid_lr,\n",
    "    # param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# 3) Inspect best params & CV score\n",
    "print(\"Best CV score:   \", grid_lr.best_score_)\n",
    "print(\"Best parameters: \", grid_lr.best_params_)\n",
    "\n",
    "# # 4) Evaluate on validation set\n",
    "best_model = grid_lr.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MDM\n",
    "\n",
    "# param_grid_mdm = {\n",
    "#     \"clf__metric\": [\"riemann\"],\n",
    "#     \"clf__n_means\": [3, 5, 7],  # Number of power means\n",
    "#     \"clf__h_values\": [\n",
    "#         [-1, 0, 1], \n",
    "#         [-0.5, 0, 0.5],\n",
    "#         [-1, -0.2, 0.2, 1]\n",
    "#     ],  # Power parameters\n",
    "#     \"clf__mean_type\": [\"power\"]\n",
    "# }\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"cov\", Covariances(estimator=\"lwf\")),\n",
    "        (\"clf\", MDM(metric=\"riemann\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# grid_mdm = RandomizedSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     n_iter=70,\n",
    "#     param_distributions=param_grid_mdm,\n",
    "#     # param_grid=param_grid,\n",
    "#     cv=3,\n",
    "#     scoring=\"accuracy\",\n",
    "#     n_jobs=-1,\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 3) Inspect best params & CV score\n",
    "# print(\"Best CV score:   \", pipeline.best_score_)\n",
    "# print(\"Best parameters: \", pipeline.best_params_)\n",
    "\n",
    "# # 4) Evaluate on validation set\n",
    "# best_model = pipeline.best_estimator_\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation accuracy:\", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icmtc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
