{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a563374",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "2a563374",
        "outputId": "8d012690-7e7a-45e3-c097-d4c37d60caab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-91e8027d-5e85-4042-bc63-52fdc15303dc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-91e8027d-5e85-4042-bc63-52fdc15303dc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving modules.zip to modules.zip\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "if not os.path.exists('./modules') and not os.path.exists('modules.zip'):\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "if not os.path.exists('./modules') and os.path.exists('modules.zip'):\n",
        "    os.system('unzip modules.zip -d .')\n",
        "\n",
        "!pip3 install optuna\n",
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import optuna\n",
        "from modules.external_dataset import EEGDataset\n",
        "from modules.utils import split_and_get_loaders, evaluate_model, manual_write_study_params\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fb6b091d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb6b091d",
        "outputId": "31a42dde-e0c6-4d94-f26a-7c4fc5e74cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download datasetaset files: \n",
            " /kaggle/input/steadystate-visual-evoked-potential-signals/SSVEP (BrainWheel)\n"
          ]
        }
      ],
      "source": [
        "#! need to modify those for the competition itself\n",
        "TRIAL_LENGTH = 640  # frequency of changing.. frequency\n",
        "# Download dataset\n",
        "path_1 = kagglehub.dataset_download(\"girgismicheal/steadystate-visual-evoked-potential-signals\")\n",
        "path_1 += \"/SSVEP (BrainWheel)\"\n",
        "print(\"Download datasetaset files:\", \"\\n\", path_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b83a09b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b83a09b",
        "outputId": "035fb053-4de3-4f7a-dfc9-575cb3993088"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3402,  1.0259,  0.3528, -0.1217],\n",
              "        [-0.8049,  0.5517,  0.4551,  0.3997],\n",
              "        [ 0.0154,  0.7810, -0.2736,  0.7535],\n",
              "        [-0.3659,  0.6799,  0.4175,  0.3663],\n",
              "        [-0.4647,  0.2742,  0.1245, -0.2464]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model) # Batch-first for consistency with transformer\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerHead(nn.Module):\n",
        "    def __init__(self, input_dim, nhead, num_encoder_layers, dim_feedforward, output_dim, dropout=0.1, max_sequence_length=512):\n",
        "        super().__init__()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        self.fc = nn.Linear(input_dim, output_dim) # Output layer\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, feature_dim] (which is F2 from EEGNet) Should apply positional encoding\n",
        "        \"\"\"\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "class DepthWiseConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, dim_mult=1, padding=0, bias=False):\n",
        "        super(DepthWiseConv2D, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels * dim_mult, padding=padding, kernel_size=kernel_size, groups=in_channels, bias=bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.depthwise(x)\n",
        "\n",
        "\n",
        "class SeperableConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, bias=False):\n",
        "        super(SeperableConv2D, self).__init__()\n",
        "        self.depthwise = DepthWiseConv2D(in_channels, kernel_size, padding=padding)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class SSVEPClassifier(nn.Module):\n",
        "    def __init__(self, n_electrodes=16, n_samples=128, out_dim=4, dropout=0.25, kernLength=256, F1=96, D=1, F2=96,\n",
        "                 nhead=8, num_encoder_layers=2, dim_feedforward=2048,\n",
        "                 max_sequence_length=None): # This should be at least n_samples // (2 * 4) = n_samples // 8\n",
        "        super().__init__()\n",
        "        max_sequence_length = max_sequence_length or n_samples // 8\n",
        "\n",
        "        # B x C x T\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(F1),\n",
        "            #\n",
        "            DepthWiseConv2D(F1, (n_electrodes, 1), dim_mult=D, bias=False),\n",
        "            nn.BatchNorm2d(F1*D),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d((1, 2)),\n",
        "            nn.Dropout(dropout),\n",
        "            #\n",
        "            SeperableConv2D(F1 * D, F2, kernel_size=(1, 16), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(F2),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d((1, 4)),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.transformer_head = TransformerHead(\n",
        "            input_dim=F2,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            output_dim=out_dim,\n",
        "            dropout=dropout,\n",
        "            max_sequence_length=max_sequence_length\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"expected input shape: BxCxT\"\"\"\n",
        "        x = x.unsqueeze(1)\n",
        "        y = self.block_1(x)\n",
        "\n",
        "        y = y.squeeze(2)\n",
        "        y = y.permute(0, 2, 1) # B x (T / 8) x F2\n",
        "\n",
        "        y = self.transformer_head(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "dummy_x = torch.randn(5, 14, 320)\n",
        "model = SSVEPClassifier(n_electrodes=dummy_x.shape[1], n_samples=dummy_x.shape[2])\n",
        "model(dummy_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2dec2ee1",
      "metadata": {
        "id": "2dec2ee1"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.train_epochs = 1000\n",
        "        self.tune_epochs = 50\n",
        "        self.optuna_n_trials = 120\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = None\n",
        "        self.trial = None\n",
        "\n",
        "        self.train_loader = None\n",
        "        self.eval_loader = None\n",
        "        self.test_loader = None\n",
        "        self.dataset = None\n",
        "\n",
        "        self.storage = \"sqlite:///optuna_studies.db\"\n",
        "        self.study_name = \"ssvep_classifier_optimization\"\n",
        "\n",
        "        self.checkpoint_path = \"./checkpoints/ssvep\"\n",
        "        os.makedirs(os.path.join(self.checkpoint_path, \"models\"), exist_ok=True)\n",
        "        self.checkpoint_model_path = os.path.join(self.checkpoint_path, \"models\")\n",
        "\n",
        "    def _train_loop(self, n_epochs: int, should_save=False, should_print=False):\n",
        "        assert isinstance(self.optimizer, torch.optim.Optimizer), \"optimizer is not a valid optimizer\"\n",
        "        assert isinstance(self.train_loader, DataLoader), \"train_laoder is not valid Datloader\"\n",
        "        if self.trial is None:\n",
        "            print(\"Warning: self.trial is none, we are probably in acutal training phase\")\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            self.model.to(self.device)\n",
        "            self.model.train()\n",
        "\n",
        "            avg_loss = 0\n",
        "            for x, y in self.train_loader:\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                y_pred = self.model(x)  # B x out_size\n",
        "                loss = self.criterion(y_pred, y)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                avg_loss += loss.item()\n",
        "\n",
        "            avg_loss = avg_loss / len(self.train_loader)\n",
        "            evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
        "\n",
        "            if self.trial is not None:\n",
        "                self.trial.report(evaluation, epoch)\n",
        "                if self.trial.should_prune():\n",
        "                    optuna.exceptions.TrialPruned()\n",
        "\n",
        "            if should_print:\n",
        "                print(f\"epoch {epoch}, evaluation {evaluation}, avg_loss {avg_loss}\")\n",
        "\n",
        "            if should_save:\n",
        "                self.model.cpu()\n",
        "                torch.save(self.model.state_dict(), os.path.join(self.checkpoint_model_path, f\"ssvep.pth\"))\n",
        "                self.model.to(self.device)\n",
        "\n",
        "    def _prepare_training(self, is_trial, my_lr=None):\n",
        "        if is_trial:\n",
        "            assert isinstance(self.trial, optuna.Trial), \"trial is none, cant' suggest params\"\n",
        "\n",
        "            # EEGNet network params\n",
        "            dropout = self.trial.suggest_float(\"dropout\", 0, 0.5)\n",
        "            kernLength = self.trial.suggest_categorical(\"kernLength\", [128, 256, 512])\n",
        "            F1 = self.trial.suggest_categorical(\"F1\", [64, 96, 128])\n",
        "            D = self.trial.suggest_categorical(\"D\", [1, 2, 3])\n",
        "            F2 = self.trial.suggest_categorical(\"F2\", [64, 96, 128]) # F2 is input_dim for transformer, so keep it as categorical\n",
        "\n",
        "            # Transformer-specific network params\n",
        "            nhead = self.trial.suggest_categorical(\"nhead\", [4, 8, 16]) # Should divide F2\n",
        "            num_encoder_layers = self.trial.suggest_int(\"num_encoder_layers\", 1, 4)\n",
        "            dim_feedforward = self.trial.suggest_categorical(\"dim_feedforward\", [256, 512, 1024])\n",
        "\n",
        "            window_length = self.trial.suggest_categorical(\"window_length\", [160, 320, 640])\n",
        "\n",
        "            # training params\n",
        "            lr = self.trial.suggest_float(\"lr\", 3e-4, 3e-2, log=True)\n",
        "            batch_size = self.trial.suggest_categorical(\"batch_size\", [32, 64])\n",
        "\n",
        "        else:\n",
        "            best_params = self._get_study().best_params\n",
        "\n",
        "            window_length = best_params['window_length']\n",
        "            dropout = best_params[\"dropout\"]\n",
        "            lr = best_params[\"lr\"]\n",
        "            batch_size = best_params[\"batch_size\"]\n",
        "\n",
        "            nhead = best_params['nhead']\n",
        "            num_encoder_layers = best_params[\"num_encoder_layers\"]\n",
        "            dim_feedforward = best_params[\"dim_feedforward\"]\n",
        "\n",
        "            kernLength = best_params.get(\"kernLength\", 256)\n",
        "            F1 = best_params.get(\"F1\", 96)\n",
        "            D = best_params.get(\"D\", 1)\n",
        "            F2 = best_params.get(\"F2\", 96)\n",
        "\n",
        "        lr = my_lr or lr\n",
        "        stride = int(window_length // 3)\n",
        "        self.dataset = EEGDataset(path_1, TRIAL_LENGTH, window_length, stride=stride)\n",
        "        unique_freqs = torch.unique(self.dataset.labels)\n",
        "\n",
        "        n_samples = self.dataset.data[0].shape[1] # data[x] shape CxT (this is 'window_length' here)\n",
        "        n_electrodes = self.dataset.data[0].shape[0]\n",
        "        out_size = len(unique_freqs)\n",
        "        self.model = SSVEPClassifier(\n",
        "            n_electrodes=n_electrodes,\n",
        "            n_samples=n_samples, # This is 'window_length' for the current dataset\n",
        "            out_dim=out_size,\n",
        "            dropout=dropout,\n",
        "            kernLength=kernLength,\n",
        "            F1=F1,\n",
        "            D=D,\n",
        "            F2=F2, # F2 for the transformer's input_dim\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "        )\n",
        "\n",
        "        self.train_loader, self.val_loader, self.test_loader = split_and_get_loaders(self.dataset, batch_size)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "\n",
        "\n",
        "    def _objective(self, trial: optuna.Trial):\n",
        "        self.trial = trial\n",
        "        self._prepare_training(True)\n",
        "\n",
        "        self._train_loop(self.tune_epochs, should_save=False, should_print=False)\n",
        "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
        "        return evaluation\n",
        "\n",
        "    def _get_study(self):\n",
        "        return optuna.create_study(study_name=self.study_name, storage=self.storage, direction=\"maximize\", load_if_exists=True)\n",
        "\n",
        "    def optimize(self, delete_existing=False):\n",
        "        if delete_existing:\n",
        "            try:\n",
        "                optuna.delete_study(study_name=self.study_name, storage=self.storage)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        study = self._get_study()\n",
        "        study.optimize(self._objective, n_trials=self.optuna_n_trials, timeout=60 * 10)\n",
        "\n",
        "        # Print optimization results\n",
        "        print(\"\\nStudy statistics:\")\n",
        "        print(f\"  Number of finished trials: {len(study.trials)}\")\n",
        "        print(f\"  Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
        "        print(f\"  Number of complete trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
        "\n",
        "        print(\"\\nBest trial:\")\n",
        "        trial = study.best_trial\n",
        "        print(f\"  Value: {trial.value}\")\n",
        "        print(\"\\nBest hyperparameters:\")\n",
        "        for key, value in trial.params.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "        return study.best_params\n",
        "\n",
        "    def train(self):\n",
        "        self.trial = None\n",
        "        self._prepare_training(False)\n",
        "\n",
        "        self._train_loop(self.train_epochs, should_save=True, should_print=True)\n",
        "        evaluation = evaluate_model(self.model, self.val_loader, self.device)\n",
        "        print(\"done training\")\n",
        "        return evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a132bf51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "a132bf51",
        "outputId": "68441792-5d8b-41f9-918e-19a88cf4ff69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-20 01:58:39,489] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n",
            "[W 2025-06-20 01:58:47,375] Trial 7 failed with parameters: {'dropout': 0.45328661235969725, 'kernLength': 512, 'F1': 96, 'D': 3, 'F2': 128, 'nhead': 8, 'num_encoder_layers': 1, 'dim_feedforward': 512, 'window_length': 160, 'lr': 0.003203741430612504, 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-21-1404354344.py\", line 133, in _objective\n",
            "    self._train_loop(self.tune_epochs, should_save=False, should_print=False)\n",
            "  File \"/tmp/ipython-input-21-1404354344.py\", line 46, in _train_loop\n",
            "    avg_loss += loss.item()\n",
            "                ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-06-20 01:58:47,376] Trial 7 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-1470883721.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdelete_existing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete_existing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-21-1404354344.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, delete_existing)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptuna_n_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Print optimization results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1404354344.py\u001b[0m in \u001b[0;36m_objective\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1404354344.py\u001b[0m in \u001b[0;36m_train_loop\u001b[0;34m(self, n_epochs, should_save, should_print)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = Trainer()\n",
        "delete_existing = False\n",
        "trainer.optimize(delete_existing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a93294ea",
      "metadata": {
        "id": "a93294ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c19d843a-90f7-4352-d601-e9966f41611f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-20 01:58:49,365] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: self.trial is none, we are probably in acutal training phase\n",
            "epoch 0, evaluation 0.2125, avg_loss 1.4173949718475343\n",
            "epoch 1, evaluation 0.2375, avg_loss 1.4021853685379029\n",
            "epoch 2, evaluation 0.26875, avg_loss 1.391485148668289\n",
            "epoch 3, evaluation 0.30625, avg_loss 1.3985726237297058\n",
            "epoch 4, evaluation 0.2875, avg_loss 1.3951397955417633\n",
            "epoch 5, evaluation 0.275, avg_loss 1.3945764303207397\n",
            "epoch 6, evaluation 0.28125, avg_loss 1.3931984603404999\n",
            "epoch 7, evaluation 0.275, avg_loss 1.4004414200782775\n",
            "epoch 8, evaluation 0.2625, avg_loss 1.3852189779281616\n",
            "epoch 9, evaluation 0.26875, avg_loss 1.3753587126731872\n",
            "epoch 10, evaluation 0.2625, avg_loss 1.379552412033081\n",
            "epoch 11, evaluation 0.31875, avg_loss 1.3828181862831115\n",
            "epoch 12, evaluation 0.30625, avg_loss 1.379877758026123\n",
            "epoch 13, evaluation 0.29375, avg_loss 1.3731835603713989\n",
            "epoch 14, evaluation 0.275, avg_loss 1.3852452695369721\n",
            "epoch 15, evaluation 0.33125, avg_loss 1.3707289338111877\n",
            "epoch 16, evaluation 0.28125, avg_loss 1.3684512734413148\n",
            "epoch 17, evaluation 0.2875, avg_loss 1.3691934883594512\n",
            "epoch 18, evaluation 0.28125, avg_loss 1.3631089806556702\n",
            "epoch 19, evaluation 0.275, avg_loss 1.355223798751831\n",
            "epoch 20, evaluation 0.25625, avg_loss 1.3639056086540222\n",
            "epoch 21, evaluation 0.2625, avg_loss 1.345160537958145\n",
            "epoch 22, evaluation 0.2875, avg_loss 1.346603125333786\n",
            "epoch 23, evaluation 0.23125, avg_loss 1.3204126596450805\n",
            "epoch 24, evaluation 0.35, avg_loss 1.292775571346283\n",
            "epoch 25, evaluation 0.35625, avg_loss 1.3096367776393891\n",
            "epoch 26, evaluation 0.31875, avg_loss 1.269268524646759\n",
            "epoch 27, evaluation 0.3875, avg_loss 1.3000224471092223\n",
            "epoch 28, evaluation 0.24375, avg_loss 1.2636177361011505\n",
            "epoch 29, evaluation 0.3875, avg_loss 1.2549881756305694\n",
            "epoch 30, evaluation 0.26875, avg_loss 1.2445982456207276\n",
            "epoch 31, evaluation 0.30625, avg_loss 1.2223362803459168\n",
            "epoch 32, evaluation 0.36875, avg_loss 1.204422527551651\n",
            "epoch 33, evaluation 0.36875, avg_loss 1.2020215451717378\n",
            "epoch 34, evaluation 0.2625, avg_loss 1.1233983218669892\n",
            "epoch 35, evaluation 0.2875, avg_loss 1.147817987203598\n",
            "epoch 36, evaluation 0.2625, avg_loss 1.1138427555561066\n",
            "epoch 37, evaluation 0.275, avg_loss 1.1173182278871536\n",
            "epoch 38, evaluation 0.35625, avg_loss 1.0828715592622757\n",
            "epoch 39, evaluation 0.36875, avg_loss 1.067209729552269\n",
            "epoch 40, evaluation 0.40625, avg_loss 1.063572734594345\n",
            "epoch 41, evaluation 0.38125, avg_loss 1.0316463321447373\n",
            "epoch 42, evaluation 0.4, avg_loss 0.980945673584938\n",
            "epoch 43, evaluation 0.44375, avg_loss 0.9867597103118897\n",
            "epoch 44, evaluation 0.36875, avg_loss 0.9887208938598633\n",
            "epoch 45, evaluation 0.34375, avg_loss 0.9495224833488465\n",
            "epoch 46, evaluation 0.44375, avg_loss 0.8928380519151687\n",
            "epoch 47, evaluation 0.4375, avg_loss 0.9017550230026246\n",
            "epoch 48, evaluation 0.44375, avg_loss 0.8635654866695404\n",
            "epoch 49, evaluation 0.325, avg_loss 0.8541275858879089\n",
            "epoch 50, evaluation 0.35625, avg_loss 0.8220918595790863\n",
            "epoch 51, evaluation 0.30625, avg_loss 0.8194748729467392\n",
            "epoch 52, evaluation 0.43125, avg_loss 0.7877416104078293\n",
            "epoch 53, evaluation 0.40625, avg_loss 0.795462504029274\n",
            "epoch 54, evaluation 0.39375, avg_loss 0.8263750106096268\n",
            "epoch 55, evaluation 0.49375, avg_loss 0.7891674190759659\n",
            "epoch 56, evaluation 0.475, avg_loss 0.7188325449824333\n",
            "epoch 57, evaluation 0.45625, avg_loss 0.6899623394012451\n",
            "epoch 58, evaluation 0.36875, avg_loss 0.6519298940896988\n",
            "epoch 59, evaluation 0.43125, avg_loss 0.6619983315467834\n",
            "epoch 60, evaluation 0.4625, avg_loss 0.6150133475661278\n",
            "epoch 61, evaluation 0.41875, avg_loss 0.5772008970379829\n",
            "epoch 62, evaluation 0.4, avg_loss 0.6195607334375381\n",
            "epoch 63, evaluation 0.38125, avg_loss 0.5605963483452797\n",
            "epoch 64, evaluation 0.29375, avg_loss 0.5637644693255425\n",
            "epoch 65, evaluation 0.49375, avg_loss 0.5535258412361145\n",
            "epoch 66, evaluation 0.4875, avg_loss 0.5638683065772057\n",
            "epoch 67, evaluation 0.46875, avg_loss 0.4836000338196754\n",
            "epoch 68, evaluation 0.39375, avg_loss 0.4919188842177391\n",
            "epoch 69, evaluation 0.4875, avg_loss 0.48417250961065295\n",
            "epoch 70, evaluation 0.44375, avg_loss 0.4308510571718216\n",
            "epoch 71, evaluation 0.53125, avg_loss 0.44296304136514664\n",
            "epoch 72, evaluation 0.46875, avg_loss 0.4443368256092072\n",
            "epoch 73, evaluation 0.49375, avg_loss 0.44047786518931387\n",
            "epoch 74, evaluation 0.475, avg_loss 0.3729737788438797\n",
            "epoch 75, evaluation 0.425, avg_loss 0.42976936399936677\n",
            "epoch 76, evaluation 0.48125, avg_loss 0.410090309381485\n",
            "epoch 77, evaluation 0.48125, avg_loss 0.4135055534541607\n",
            "epoch 78, evaluation 0.5, avg_loss 0.3743348218500614\n",
            "epoch 79, evaluation 0.46875, avg_loss 0.32662620693445205\n",
            "epoch 80, evaluation 0.48125, avg_loss 0.3730308562517166\n",
            "epoch 81, evaluation 0.4875, avg_loss 0.3248614765703678\n",
            "epoch 82, evaluation 0.4625, avg_loss 0.29133469350636004\n",
            "epoch 83, evaluation 0.4625, avg_loss 0.2984800025820732\n",
            "epoch 84, evaluation 0.46875, avg_loss 0.2852967411279678\n",
            "epoch 85, evaluation 0.45, avg_loss 0.27601107731461527\n",
            "epoch 86, evaluation 0.49375, avg_loss 0.30743349231779576\n",
            "epoch 87, evaluation 0.5, avg_loss 0.27605427242815495\n",
            "epoch 88, evaluation 0.475, avg_loss 0.2644296832382679\n",
            "epoch 89, evaluation 0.475, avg_loss 0.24679783433675767\n",
            "epoch 90, evaluation 0.4875, avg_loss 0.2201620165258646\n",
            "epoch 91, evaluation 0.4625, avg_loss 0.2341023486107588\n",
            "epoch 92, evaluation 0.46875, avg_loss 0.22517700977623462\n",
            "epoch 93, evaluation 0.475, avg_loss 0.2395172767341137\n",
            "epoch 94, evaluation 0.475, avg_loss 0.20367607213556765\n",
            "epoch 95, evaluation 0.50625, avg_loss 0.21544541977345943\n",
            "epoch 96, evaluation 0.49375, avg_loss 0.2247901599854231\n",
            "epoch 97, evaluation 0.44375, avg_loss 0.21205732338130473\n",
            "epoch 98, evaluation 0.4875, avg_loss 0.20145826935768127\n",
            "epoch 99, evaluation 0.4875, avg_loss 0.1850992102175951\n",
            "epoch 100, evaluation 0.51875, avg_loss 0.1826724201440811\n",
            "epoch 101, evaluation 0.43125, avg_loss 0.1766661711037159\n",
            "epoch 102, evaluation 0.51875, avg_loss 0.1881860475987196\n",
            "epoch 103, evaluation 0.4875, avg_loss 0.16884609051048755\n",
            "epoch 104, evaluation 0.50625, avg_loss 0.1663302905857563\n",
            "epoch 105, evaluation 0.49375, avg_loss 0.1351799700409174\n",
            "epoch 106, evaluation 0.475, avg_loss 0.1833444671705365\n",
            "epoch 107, evaluation 0.4625, avg_loss 0.1686719361692667\n",
            "epoch 108, evaluation 0.50625, avg_loss 0.13597652241587638\n",
            "epoch 109, evaluation 0.51875, avg_loss 0.1359155263751745\n",
            "epoch 110, evaluation 0.49375, avg_loss 0.1520901659503579\n",
            "epoch 111, evaluation 0.50625, avg_loss 0.1555460068397224\n",
            "epoch 112, evaluation 0.50625, avg_loss 0.13404635936021805\n",
            "epoch 113, evaluation 0.5125, avg_loss 0.1476376311853528\n",
            "epoch 114, evaluation 0.53125, avg_loss 0.1366999873891473\n",
            "epoch 115, evaluation 0.4875, avg_loss 0.12547798557206988\n",
            "epoch 116, evaluation 0.48125, avg_loss 0.11336193419992924\n",
            "epoch 117, evaluation 0.5125, avg_loss 0.13437795750796794\n",
            "epoch 118, evaluation 0.525, avg_loss 0.09663575310260057\n",
            "epoch 119, evaluation 0.5125, avg_loss 0.1406179590150714\n",
            "epoch 120, evaluation 0.51875, avg_loss 0.11018717102706432\n",
            "epoch 121, evaluation 0.50625, avg_loss 0.0902531798928976\n",
            "epoch 122, evaluation 0.53125, avg_loss 0.11716285469010472\n",
            "epoch 123, evaluation 0.50625, avg_loss 0.09246869776397944\n",
            "epoch 124, evaluation 0.50625, avg_loss 0.12253094017505646\n",
            "epoch 125, evaluation 0.48125, avg_loss 0.11985999336466194\n",
            "epoch 126, evaluation 0.53125, avg_loss 0.14405763130635024\n",
            "epoch 127, evaluation 0.50625, avg_loss 0.10487297028303147\n",
            "epoch 128, evaluation 0.49375, avg_loss 0.11456643380224704\n",
            "epoch 129, evaluation 0.53125, avg_loss 0.09312596945092082\n",
            "epoch 130, evaluation 0.48125, avg_loss 0.13843777747824787\n",
            "epoch 131, evaluation 0.525, avg_loss 0.13298262478783726\n",
            "epoch 132, evaluation 0.5375, avg_loss 0.09663028288632632\n",
            "epoch 133, evaluation 0.54375, avg_loss 0.07891462286934256\n",
            "epoch 134, evaluation 0.54375, avg_loss 0.10242314608767629\n",
            "epoch 135, evaluation 0.50625, avg_loss 0.07562556779012083\n",
            "epoch 136, evaluation 0.53125, avg_loss 0.08129772953689099\n",
            "epoch 137, evaluation 0.48125, avg_loss 0.09690874963998794\n",
            "epoch 138, evaluation 0.525, avg_loss 0.08757054572924972\n",
            "epoch 139, evaluation 0.5, avg_loss 0.09393156673759222\n",
            "epoch 140, evaluation 0.51875, avg_loss 0.07249254686757922\n",
            "epoch 141, evaluation 0.53125, avg_loss 0.08552294224500656\n",
            "epoch 142, evaluation 0.51875, avg_loss 0.07388475900515915\n",
            "epoch 143, evaluation 0.49375, avg_loss 0.09155741147696972\n",
            "epoch 144, evaluation 0.50625, avg_loss 0.08262434471398591\n",
            "epoch 145, evaluation 0.525, avg_loss 0.08896316485479475\n",
            "epoch 146, evaluation 0.5125, avg_loss 0.09303786447271704\n",
            "epoch 147, evaluation 0.53125, avg_loss 0.04683453515172005\n",
            "epoch 148, evaluation 0.525, avg_loss 0.09036377239972353\n",
            "epoch 149, evaluation 0.53125, avg_loss 0.10765342460945249\n",
            "epoch 150, evaluation 0.5375, avg_loss 0.07917709965258837\n",
            "epoch 151, evaluation 0.5, avg_loss 0.10978734530508519\n",
            "epoch 152, evaluation 0.50625, avg_loss 0.0808164382353425\n",
            "epoch 153, evaluation 0.53125, avg_loss 0.07898429818451405\n",
            "epoch 154, evaluation 0.5625, avg_loss 0.07218448352068663\n",
            "epoch 155, evaluation 0.54375, avg_loss 0.06543707577511668\n",
            "epoch 156, evaluation 0.5125, avg_loss 0.055044158408418295\n",
            "epoch 157, evaluation 0.5125, avg_loss 0.05277774310670793\n",
            "epoch 158, evaluation 0.53125, avg_loss 0.04922729814425111\n",
            "epoch 159, evaluation 0.53125, avg_loss 0.045742188673466445\n",
            "epoch 160, evaluation 0.53125, avg_loss 0.07490719743072986\n",
            "epoch 161, evaluation 0.525, avg_loss 0.06387953935191035\n",
            "epoch 162, evaluation 0.51875, avg_loss 0.07372282817959785\n",
            "epoch 163, evaluation 0.5125, avg_loss 0.07727475706487894\n",
            "epoch 164, evaluation 0.5375, avg_loss 0.04816614724695682\n",
            "epoch 165, evaluation 0.53125, avg_loss 0.059904414042830464\n",
            "epoch 166, evaluation 0.53125, avg_loss 0.034811200061813\n",
            "epoch 167, evaluation 0.55625, avg_loss 0.05629802858456969\n",
            "epoch 168, evaluation 0.54375, avg_loss 0.06377454134635627\n",
            "epoch 169, evaluation 0.51875, avg_loss 0.0622961915563792\n",
            "epoch 170, evaluation 0.55625, avg_loss 0.062117456644773486\n",
            "epoch 171, evaluation 0.575, avg_loss 0.0722207008395344\n",
            "epoch 172, evaluation 0.51875, avg_loss 0.05678757950663567\n",
            "epoch 173, evaluation 0.525, avg_loss 0.05822766025085002\n",
            "epoch 174, evaluation 0.525, avg_loss 0.04090306917205453\n",
            "epoch 175, evaluation 0.525, avg_loss 0.040791904693469404\n",
            "epoch 176, evaluation 0.55, avg_loss 0.04198444113135338\n",
            "epoch 177, evaluation 0.575, avg_loss 0.04422278441488743\n",
            "epoch 178, evaluation 0.525, avg_loss 0.05083583160303533\n",
            "epoch 179, evaluation 0.51875, avg_loss 0.022874441696330905\n",
            "epoch 180, evaluation 0.5, avg_loss 0.04555034930817783\n",
            "epoch 181, evaluation 0.55, avg_loss 0.03419765247963369\n",
            "epoch 182, evaluation 0.5375, avg_loss 0.029514809418469668\n",
            "epoch 183, evaluation 0.525, avg_loss 0.03343194010667503\n",
            "epoch 184, evaluation 0.55, avg_loss 0.06427408456802368\n",
            "epoch 185, evaluation 0.53125, avg_loss 0.0655571149662137\n",
            "epoch 186, evaluation 0.5375, avg_loss 0.028643607255071403\n",
            "epoch 187, evaluation 0.51875, avg_loss 0.04239657749421895\n",
            "epoch 188, evaluation 0.5375, avg_loss 0.05066670079249889\n",
            "epoch 189, evaluation 0.53125, avg_loss 0.03174825306050479\n",
            "epoch 190, evaluation 0.54375, avg_loss 0.03529904712922871\n",
            "epoch 191, evaluation 0.5625, avg_loss 0.04822237389162183\n",
            "epoch 192, evaluation 0.55625, avg_loss 0.054229978518560526\n",
            "epoch 193, evaluation 0.5375, avg_loss 0.06867636297829449\n",
            "epoch 194, evaluation 0.5625, avg_loss 0.034364383993670346\n",
            "epoch 195, evaluation 0.5625, avg_loss 0.05315941413864493\n",
            "epoch 196, evaluation 0.54375, avg_loss 0.048130457289516926\n",
            "epoch 197, evaluation 0.54375, avg_loss 0.04932630925904959\n",
            "epoch 198, evaluation 0.54375, avg_loss 0.05791730640921742\n",
            "epoch 199, evaluation 0.55625, avg_loss 0.05594734200276434\n",
            "epoch 200, evaluation 0.5375, avg_loss 0.038064204575493935\n",
            "epoch 201, evaluation 0.55, avg_loss 0.03734175798017532\n",
            "epoch 202, evaluation 0.5375, avg_loss 0.0350019461940974\n",
            "epoch 203, evaluation 0.5375, avg_loss 0.041198453726246954\n",
            "epoch 204, evaluation 0.54375, avg_loss 0.03463625099975616\n",
            "epoch 205, evaluation 0.5125, avg_loss 0.04323465107008815\n",
            "epoch 206, evaluation 0.54375, avg_loss 0.05026995094958693\n",
            "epoch 207, evaluation 0.525, avg_loss 0.033022617013193666\n",
            "epoch 208, evaluation 0.53125, avg_loss 0.03962521853391081\n",
            "epoch 209, evaluation 0.525, avg_loss 0.03369865284767002\n",
            "epoch 210, evaluation 0.5375, avg_loss 0.024122101115062834\n",
            "epoch 211, evaluation 0.54375, avg_loss 0.04349542900454253\n",
            "epoch 212, evaluation 0.53125, avg_loss 0.030743560451082886\n",
            "epoch 213, evaluation 0.5375, avg_loss 0.036809944664128125\n",
            "epoch 214, evaluation 0.55, avg_loss 0.019908348028548063\n",
            "epoch 215, evaluation 0.46875, avg_loss 0.02820421608630568\n",
            "epoch 216, evaluation 0.5125, avg_loss 0.044415557151660325\n",
            "epoch 217, evaluation 0.525, avg_loss 0.03670278650242835\n",
            "epoch 218, evaluation 0.50625, avg_loss 0.06607392916921526\n",
            "epoch 219, evaluation 0.54375, avg_loss 0.05905659960117191\n",
            "epoch 220, evaluation 0.54375, avg_loss 0.028855122998356818\n",
            "epoch 221, evaluation 0.55, avg_loss 0.024761548009701075\n",
            "epoch 222, evaluation 0.56875, avg_loss 0.025993873609695583\n",
            "epoch 223, evaluation 0.50625, avg_loss 0.03128421055153012\n",
            "epoch 224, evaluation 0.5625, avg_loss 0.016356180200818925\n",
            "epoch 225, evaluation 0.5375, avg_loss 0.03534161713905633\n",
            "epoch 226, evaluation 0.5375, avg_loss 0.035387874813750386\n",
            "epoch 227, evaluation 0.53125, avg_loss 0.039615087932907043\n",
            "epoch 228, evaluation 0.55, avg_loss 0.03497379180043936\n",
            "epoch 229, evaluation 0.51875, avg_loss 0.030507522693369538\n",
            "epoch 230, evaluation 0.53125, avg_loss 0.0450131309684366\n",
            "epoch 231, evaluation 0.58125, avg_loss 0.029595633666031063\n",
            "epoch 232, evaluation 0.55625, avg_loss 0.019406220398377628\n",
            "epoch 233, evaluation 0.55, avg_loss 0.034074518573470415\n",
            "epoch 234, evaluation 0.53125, avg_loss 0.04622450335882604\n",
            "epoch 235, evaluation 0.58125, avg_loss 0.03939347439445555\n",
            "epoch 236, evaluation 0.56875, avg_loss 0.04312432940350845\n",
            "epoch 237, evaluation 0.58125, avg_loss 0.05572380636585876\n",
            "epoch 238, evaluation 0.5625, avg_loss 0.05091822531539947\n",
            "epoch 239, evaluation 0.54375, avg_loss 0.06038803970441222\n",
            "epoch 240, evaluation 0.575, avg_loss 0.024985701532568783\n",
            "epoch 241, evaluation 0.55625, avg_loss 0.02099794935202226\n",
            "epoch 242, evaluation 0.5875, avg_loss 0.01834769337438047\n",
            "epoch 243, evaluation 0.525, avg_loss 0.04528940790332854\n",
            "epoch 244, evaluation 0.55625, avg_loss 0.03792665749788284\n",
            "epoch 245, evaluation 0.54375, avg_loss 0.03492477600229904\n",
            "epoch 246, evaluation 0.575, avg_loss 0.03253414102364331\n",
            "epoch 247, evaluation 0.5375, avg_loss 0.034608847612980755\n",
            "epoch 248, evaluation 0.54375, avg_loss 0.02405631577130407\n",
            "epoch 249, evaluation 0.4875, avg_loss 0.015366049844305962\n",
            "epoch 250, evaluation 0.56875, avg_loss 0.03857352916384116\n",
            "epoch 251, evaluation 0.5625, avg_loss 0.023775763006415217\n",
            "epoch 252, evaluation 0.51875, avg_loss 0.04061974553624168\n",
            "epoch 253, evaluation 0.5375, avg_loss 0.03662476686295122\n",
            "epoch 254, evaluation 0.59375, avg_loss 0.027357064466923476\n",
            "epoch 255, evaluation 0.58125, avg_loss 0.021097519411705436\n",
            "epoch 256, evaluation 0.55, avg_loss 0.019160592963453384\n",
            "epoch 257, evaluation 0.56875, avg_loss 0.035102000588085505\n",
            "epoch 258, evaluation 0.55, avg_loss 0.03567725270986557\n",
            "epoch 259, evaluation 0.58125, avg_loss 0.02965004474390298\n",
            "epoch 260, evaluation 0.575, avg_loss 0.024182752310298383\n",
            "epoch 261, evaluation 0.55, avg_loss 0.014684303733520209\n",
            "epoch 262, evaluation 0.525, avg_loss 0.011373300047125667\n",
            "epoch 263, evaluation 0.525, avg_loss 0.030257865076418967\n",
            "epoch 264, evaluation 0.525, avg_loss 0.04541102036600932\n",
            "epoch 265, evaluation 0.5625, avg_loss 0.0701089971815236\n",
            "epoch 266, evaluation 0.54375, avg_loss 0.026856466883327813\n",
            "epoch 267, evaluation 0.53125, avg_loss 0.026996600488200784\n",
            "epoch 268, evaluation 0.58125, avg_loss 0.0445958164986223\n",
            "epoch 269, evaluation 0.55625, avg_loss 0.024734588945284484\n",
            "epoch 270, evaluation 0.55, avg_loss 0.03728298303904012\n",
            "epoch 271, evaluation 0.575, avg_loss 0.020899355818983167\n",
            "epoch 272, evaluation 0.575, avg_loss 0.025009258615318684\n",
            "epoch 273, evaluation 0.55, avg_loss 0.02139616428175941\n",
            "epoch 274, evaluation 0.59375, avg_loss 0.01675629128003493\n",
            "epoch 275, evaluation 0.5625, avg_loss 0.024030348437372596\n",
            "epoch 276, evaluation 0.56875, avg_loss 0.023830648383591325\n",
            "epoch 277, evaluation 0.55625, avg_loss 0.026634611014742404\n",
            "epoch 278, evaluation 0.5375, avg_loss 0.036558939563110474\n",
            "epoch 279, evaluation 0.575, avg_loss 0.021578623610548676\n",
            "epoch 280, evaluation 0.575, avg_loss 0.018891518970485778\n",
            "epoch 281, evaluation 0.58125, avg_loss 0.020910773240029813\n",
            "epoch 282, evaluation 0.58125, avg_loss 0.014121430111117661\n",
            "epoch 283, evaluation 0.575, avg_loss 0.021226462966296822\n",
            "epoch 284, evaluation 0.525, avg_loss 0.010901913046836853\n",
            "epoch 285, evaluation 0.55, avg_loss 0.006288264796603471\n",
            "epoch 286, evaluation 0.525, avg_loss 0.02984780187252909\n",
            "epoch 287, evaluation 0.56875, avg_loss 0.02694366019568406\n",
            "epoch 288, evaluation 0.55, avg_loss 0.019521280517801644\n",
            "epoch 289, evaluation 0.5875, avg_loss 0.012510483630467206\n",
            "epoch 290, evaluation 0.55625, avg_loss 0.031439006689470264\n",
            "epoch 291, evaluation 0.55625, avg_loss 0.01944722607731819\n",
            "epoch 292, evaluation 0.55625, avg_loss 0.024367425125092268\n",
            "epoch 293, evaluation 0.55, avg_loss 0.02471379751805216\n",
            "epoch 294, evaluation 0.54375, avg_loss 0.047435689042322336\n",
            "epoch 295, evaluation 0.58125, avg_loss 0.02079519445542246\n",
            "epoch 296, evaluation 0.56875, avg_loss 0.038330586720258\n",
            "epoch 297, evaluation 0.60625, avg_loss 0.0378739274921827\n",
            "epoch 298, evaluation 0.5375, avg_loss 0.03707200379576534\n",
            "epoch 299, evaluation 0.56875, avg_loss 0.005817676591686904\n",
            "epoch 300, evaluation 0.55, avg_loss 0.02262173768831417\n",
            "epoch 301, evaluation 0.53125, avg_loss 0.024007512431126088\n",
            "epoch 302, evaluation 0.575, avg_loss 0.0279348952753935\n",
            "epoch 303, evaluation 0.55, avg_loss 0.052572695503477006\n",
            "epoch 304, evaluation 0.56875, avg_loss 0.028013964684214444\n",
            "epoch 305, evaluation 0.54375, avg_loss 0.03493311261991039\n",
            "epoch 306, evaluation 0.56875, avg_loss 0.020402389741502703\n",
            "epoch 307, evaluation 0.49375, avg_loss 0.011282300378661602\n",
            "epoch 308, evaluation 0.53125, avg_loss 0.042662490892689676\n",
            "epoch 309, evaluation 0.5625, avg_loss 0.03177113874116912\n",
            "epoch 310, evaluation 0.55, avg_loss 0.03013591511407867\n",
            "epoch 311, evaluation 0.56875, avg_loss 0.048459667293354866\n",
            "epoch 312, evaluation 0.5625, avg_loss 0.020866262359777465\n",
            "epoch 313, evaluation 0.525, avg_loss 0.037397087214048955\n",
            "epoch 314, evaluation 0.53125, avg_loss 0.05159990016836673\n",
            "epoch 315, evaluation 0.5875, avg_loss 0.03296981236198917\n",
            "epoch 316, evaluation 0.5375, avg_loss 0.013644544349517673\n",
            "epoch 317, evaluation 0.55, avg_loss 0.028459862899035217\n",
            "epoch 318, evaluation 0.525, avg_loss 0.02808283574413508\n",
            "epoch 319, evaluation 0.59375, avg_loss 0.008323530381312593\n",
            "epoch 320, evaluation 0.55625, avg_loss 0.012894082529237494\n",
            "epoch 321, evaluation 0.55, avg_loss 0.02570916968397796\n",
            "epoch 322, evaluation 0.525, avg_loss 0.011233302624896168\n",
            "epoch 323, evaluation 0.55, avg_loss 0.015063326107338071\n",
            "epoch 324, evaluation 0.54375, avg_loss 0.01104548762086779\n",
            "epoch 325, evaluation 0.525, avg_loss 0.019855023419950157\n",
            "epoch 326, evaluation 0.575, avg_loss 0.014938737225020305\n",
            "epoch 327, evaluation 0.6, avg_loss 0.006707929557887837\n",
            "epoch 328, evaluation 0.55, avg_loss 0.008612150303088128\n",
            "epoch 329, evaluation 0.525, avg_loss 0.010135489818640053\n",
            "epoch 330, evaluation 0.5625, avg_loss 0.012315068586030976\n",
            "epoch 331, evaluation 0.5375, avg_loss 0.010480606008786707\n",
            "epoch 332, evaluation 0.55625, avg_loss 0.017895867547485976\n",
            "epoch 333, evaluation 0.54375, avg_loss 0.011798666761023924\n",
            "epoch 334, evaluation 0.54375, avg_loss 0.02607296907226555\n",
            "epoch 335, evaluation 0.575, avg_loss 0.032864892040379345\n",
            "epoch 336, evaluation 0.54375, avg_loss 0.03369451437611133\n",
            "epoch 337, evaluation 0.53125, avg_loss 0.01496595807839185\n",
            "epoch 338, evaluation 0.55625, avg_loss 0.02184894296224229\n",
            "epoch 339, evaluation 0.525, avg_loss 0.013313474762253464\n",
            "epoch 340, evaluation 0.54375, avg_loss 0.006385791831417009\n",
            "epoch 341, evaluation 0.55625, avg_loss 0.01621055993018672\n",
            "epoch 342, evaluation 0.54375, avg_loss 0.014836691628443078\n",
            "epoch 343, evaluation 0.525, avg_loss 0.021859000640688464\n",
            "epoch 344, evaluation 0.525, avg_loss 0.0062843527179211375\n",
            "epoch 345, evaluation 0.5375, avg_loss 0.013193948386469857\n",
            "epoch 346, evaluation 0.54375, avg_loss 0.016404635575599968\n",
            "epoch 347, evaluation 0.58125, avg_loss 0.00781028873170726\n",
            "epoch 348, evaluation 0.5375, avg_loss 0.021524004440288992\n",
            "epoch 349, evaluation 0.55, avg_loss 0.016743738699005917\n",
            "epoch 350, evaluation 0.53125, avg_loss 0.00695831430493854\n",
            "epoch 351, evaluation 0.55, avg_loss 0.004549095960101113\n",
            "epoch 352, evaluation 0.55625, avg_loss 0.004724572726991028\n",
            "epoch 353, evaluation 0.55, avg_loss 0.01100906659848988\n",
            "epoch 354, evaluation 0.54375, avg_loss 0.020489849045407026\n",
            "epoch 355, evaluation 0.51875, avg_loss 0.02363051530555822\n",
            "epoch 356, evaluation 0.55625, avg_loss 0.017654702393338086\n",
            "epoch 357, evaluation 0.5625, avg_loss 0.02796621495508589\n",
            "epoch 358, evaluation 0.525, avg_loss 0.022536666854284702\n",
            "epoch 359, evaluation 0.54375, avg_loss 0.045730398158775644\n",
            "epoch 360, evaluation 0.5, avg_loss 0.024221748585114257\n",
            "epoch 361, evaluation 0.50625, avg_loss 0.059491197019815444\n",
            "epoch 362, evaluation 0.55, avg_loss 0.020829298091121017\n",
            "epoch 363, evaluation 0.5125, avg_loss 0.01615810026996769\n",
            "epoch 364, evaluation 0.55, avg_loss 0.02189756496809423\n",
            "epoch 365, evaluation 0.50625, avg_loss 0.020587885193526743\n",
            "epoch 366, evaluation 0.51875, avg_loss 0.017573892406653614\n",
            "epoch 367, evaluation 0.51875, avg_loss 0.02534876561257988\n",
            "epoch 368, evaluation 0.54375, avg_loss 0.0289791205257643\n",
            "epoch 369, evaluation 0.50625, avg_loss 0.020672719448339195\n",
            "epoch 370, evaluation 0.51875, avg_loss 0.02261202639201656\n",
            "epoch 371, evaluation 0.5125, avg_loss 0.00901148624252528\n",
            "epoch 372, evaluation 0.51875, avg_loss 0.019911495386622846\n",
            "epoch 373, evaluation 0.525, avg_loss 0.022734247712651267\n",
            "epoch 374, evaluation 0.60625, avg_loss 0.020078695012489333\n",
            "epoch 375, evaluation 0.54375, avg_loss 0.017109844012884422\n",
            "epoch 376, evaluation 0.5375, avg_loss 0.013366737612523138\n",
            "epoch 377, evaluation 0.5375, avg_loss 0.006391907861689106\n",
            "epoch 378, evaluation 0.53125, avg_loss 0.006889743346255273\n",
            "epoch 379, evaluation 0.5375, avg_loss 0.009692481980891898\n",
            "epoch 380, evaluation 0.55, avg_loss 0.026751748949754982\n",
            "epoch 381, evaluation 0.5125, avg_loss 0.01505578009528108\n",
            "epoch 382, evaluation 0.5625, avg_loss 0.019730273337336256\n",
            "epoch 383, evaluation 0.525, avg_loss 0.014975205261725933\n",
            "epoch 384, evaluation 0.5375, avg_loss 0.030381220698473044\n",
            "epoch 385, evaluation 0.6, avg_loss 0.019645917211892083\n",
            "epoch 386, evaluation 0.5125, avg_loss 0.04344450054923073\n",
            "epoch 387, evaluation 0.5, avg_loss 0.034374925191514194\n",
            "epoch 388, evaluation 0.5375, avg_loss 0.028643668361473827\n",
            "epoch 389, evaluation 0.53125, avg_loss 0.04405743487877771\n",
            "epoch 390, evaluation 0.55, avg_loss 0.019220687652705237\n",
            "epoch 391, evaluation 0.54375, avg_loss 0.0037008696352131666\n",
            "epoch 392, evaluation 0.55, avg_loss 0.017496776720508932\n",
            "epoch 393, evaluation 0.54375, avg_loss 0.008506202267017215\n",
            "epoch 394, evaluation 0.54375, avg_loss 0.017609345237724484\n",
            "epoch 395, evaluation 0.54375, avg_loss 0.011965127574512735\n",
            "epoch 396, evaluation 0.5375, avg_loss 0.029080536594847217\n",
            "epoch 397, evaluation 0.53125, avg_loss 0.028621307283174247\n",
            "epoch 398, evaluation 0.53125, avg_loss 0.014199319493491203\n",
            "epoch 399, evaluation 0.55625, avg_loss 0.01632230554241687\n",
            "epoch 400, evaluation 0.53125, avg_loss 0.03495417021913454\n",
            "epoch 401, evaluation 0.5375, avg_loss 0.01431866783532314\n",
            "epoch 402, evaluation 0.54375, avg_loss 0.006977627071319148\n",
            "epoch 403, evaluation 0.5375, avg_loss 0.015360455506015568\n",
            "epoch 404, evaluation 0.55, avg_loss 0.018306962493807078\n",
            "epoch 405, evaluation 0.51875, avg_loss 0.03308662934578024\n",
            "epoch 406, evaluation 0.55, avg_loss 0.00898379401769489\n",
            "epoch 407, evaluation 0.54375, avg_loss 0.01593960222089663\n",
            "epoch 408, evaluation 0.53125, avg_loss 0.006642228487180546\n",
            "epoch 409, evaluation 0.5625, avg_loss 0.0035847028717398643\n",
            "epoch 410, evaluation 0.53125, avg_loss 0.017312464356655254\n",
            "epoch 411, evaluation 0.55625, avg_loss 0.02984509105444886\n",
            "epoch 412, evaluation 0.525, avg_loss 0.013127935962984338\n",
            "epoch 413, evaluation 0.55, avg_loss 0.01505554570176173\n",
            "epoch 414, evaluation 0.55, avg_loss 0.006609768426278606\n",
            "epoch 415, evaluation 0.5625, avg_loss 0.0035862319404259325\n",
            "epoch 416, evaluation 0.53125, avg_loss 0.017342126258881763\n",
            "epoch 417, evaluation 0.54375, avg_loss 0.009465115732746198\n",
            "epoch 418, evaluation 0.55, avg_loss 0.04326650558796245\n",
            "epoch 419, evaluation 0.55, avg_loss 0.01383715370320715\n",
            "epoch 420, evaluation 0.5125, avg_loss 0.044945116562303156\n",
            "epoch 421, evaluation 0.5625, avg_loss 0.01993626346811652\n",
            "epoch 422, evaluation 0.5625, avg_loss 0.03454268627101555\n",
            "epoch 423, evaluation 0.53125, avg_loss 0.027734947932185606\n",
            "epoch 424, evaluation 0.575, avg_loss 0.00905643246951513\n",
            "epoch 425, evaluation 0.53125, avg_loss 0.023469543555984275\n",
            "epoch 426, evaluation 0.55, avg_loss 0.011546742741484196\n",
            "epoch 427, evaluation 0.55, avg_loss 0.004087525865179487\n",
            "epoch 428, evaluation 0.5375, avg_loss 0.036101575032807885\n",
            "epoch 429, evaluation 0.5625, avg_loss 0.013691966619808227\n",
            "epoch 430, evaluation 0.54375, avg_loss 0.012726629711687565\n",
            "epoch 431, evaluation 0.575, avg_loss 0.009970824583433568\n",
            "epoch 432, evaluation 0.59375, avg_loss 0.015224570559803396\n",
            "epoch 433, evaluation 0.55, avg_loss 0.025386790448101236\n",
            "epoch 434, evaluation 0.54375, avg_loss 0.01251558123040013\n",
            "epoch 435, evaluation 0.53125, avg_loss 0.020913482154719533\n",
            "epoch 436, evaluation 0.58125, avg_loss 0.009462772682309151\n",
            "epoch 437, evaluation 0.55625, avg_loss 0.012878657138207928\n",
            "epoch 438, evaluation 0.54375, avg_loss 0.010410137052531354\n",
            "epoch 439, evaluation 0.55, avg_loss 0.008978703909087926\n",
            "epoch 440, evaluation 0.58125, avg_loss 0.00345144149614498\n",
            "epoch 441, evaluation 0.54375, avg_loss 0.01699033154291101\n",
            "epoch 442, evaluation 0.5625, avg_loss 0.03800759778241627\n",
            "epoch 443, evaluation 0.55625, avg_loss 0.0061542628449387845\n",
            "epoch 444, evaluation 0.5625, avg_loss 0.008874162833672017\n",
            "epoch 445, evaluation 0.55, avg_loss 0.014771173865301534\n",
            "epoch 446, evaluation 0.55, avg_loss 0.013554966828087345\n",
            "epoch 447, evaluation 0.56875, avg_loss 0.009893615532200783\n",
            "epoch 448, evaluation 0.58125, avg_loss 0.003954570065252483\n",
            "epoch 449, evaluation 0.58125, avg_loss 0.011954464364680461\n",
            "epoch 450, evaluation 0.575, avg_loss 0.0055762174655683335\n",
            "epoch 451, evaluation 0.575, avg_loss 0.006806214503012597\n",
            "epoch 452, evaluation 0.58125, avg_loss 0.011833085212856531\n",
            "epoch 453, evaluation 0.575, avg_loss 0.003760655305813998\n",
            "epoch 454, evaluation 0.5625, avg_loss 0.01574866339797154\n",
            "epoch 455, evaluation 0.575, avg_loss 0.00692718054051511\n",
            "epoch 456, evaluation 0.55625, avg_loss 0.010732515595736913\n",
            "epoch 457, evaluation 0.59375, avg_loss 0.010737636685371399\n",
            "epoch 458, evaluation 0.5875, avg_loss 0.01654595709987916\n",
            "epoch 459, evaluation 0.575, avg_loss 0.005846668838057667\n",
            "epoch 460, evaluation 0.54375, avg_loss 0.012995501211844385\n",
            "epoch 461, evaluation 0.56875, avg_loss 0.009907135169487446\n",
            "epoch 462, evaluation 0.56875, avg_loss 0.01034465667034965\n",
            "epoch 463, evaluation 0.56875, avg_loss 0.005624999228166417\n",
            "epoch 464, evaluation 0.575, avg_loss 0.022535983892157673\n",
            "epoch 465, evaluation 0.55625, avg_loss 0.005721791987889446\n",
            "epoch 466, evaluation 0.575, avg_loss 0.010353708351613023\n",
            "epoch 467, evaluation 0.58125, avg_loss 0.005555302518769167\n",
            "epoch 468, evaluation 0.5625, avg_loss 0.00865049848507624\n",
            "epoch 469, evaluation 0.5625, avg_loss 0.0029249510378576817\n",
            "epoch 470, evaluation 0.55625, avg_loss 0.0050139029626734555\n",
            "epoch 471, evaluation 0.55625, avg_loss 0.011024377439753152\n",
            "epoch 472, evaluation 0.58125, avg_loss 0.01741589143348392\n",
            "epoch 473, evaluation 0.5625, avg_loss 0.026669570183730684\n",
            "epoch 474, evaluation 0.5375, avg_loss 0.014103762106969953\n",
            "epoch 475, evaluation 0.58125, avg_loss 0.006935311685083434\n",
            "epoch 476, evaluation 0.5375, avg_loss 0.006249687180388719\n",
            "epoch 477, evaluation 0.56875, avg_loss 0.030494645575527102\n",
            "epoch 478, evaluation 0.54375, avg_loss 0.005222012978629209\n",
            "epoch 479, evaluation 0.59375, avg_loss 0.02766895852400921\n",
            "epoch 480, evaluation 0.5625, avg_loss 0.024324778589652853\n",
            "epoch 481, evaluation 0.51875, avg_loss 0.0034867525362642484\n",
            "epoch 482, evaluation 0.5375, avg_loss 0.010987606656271964\n",
            "epoch 483, evaluation 0.54375, avg_loss 0.014484441446256824\n",
            "epoch 484, evaluation 0.575, avg_loss 0.018997869035229088\n",
            "epoch 485, evaluation 0.55625, avg_loss 0.030303923718747683\n",
            "epoch 486, evaluation 0.5375, avg_loss 0.018018296276568434\n",
            "epoch 487, evaluation 0.55, avg_loss 0.010217055576504208\n",
            "epoch 488, evaluation 0.5375, avg_loss 0.008693048421991988\n",
            "epoch 489, evaluation 0.56875, avg_loss 0.02323279626725707\n",
            "epoch 490, evaluation 0.5375, avg_loss 0.015132077573798596\n",
            "epoch 491, evaluation 0.525, avg_loss 0.010829981285496615\n",
            "epoch 492, evaluation 0.56875, avg_loss 0.004718328436138108\n",
            "epoch 493, evaluation 0.56875, avg_loss 0.004292181340861134\n",
            "epoch 494, evaluation 0.54375, avg_loss 0.003080954909091815\n",
            "epoch 495, evaluation 0.575, avg_loss 0.010345665708882734\n",
            "epoch 496, evaluation 0.55, avg_loss 0.0052869934326736255\n",
            "epoch 497, evaluation 0.54375, avg_loss 0.005002038014936261\n",
            "epoch 498, evaluation 0.55, avg_loss 0.0022699662076774985\n",
            "epoch 499, evaluation 0.56875, avg_loss 0.020570752074127084\n",
            "epoch 500, evaluation 0.5375, avg_loss 0.010718556967913174\n",
            "epoch 501, evaluation 0.51875, avg_loss 0.029954555962467565\n",
            "epoch 502, evaluation 0.5875, avg_loss 0.011476839196984657\n",
            "epoch 503, evaluation 0.55, avg_loss 0.008930101612349972\n",
            "epoch 504, evaluation 0.54375, avg_loss 0.0035744483466260135\n",
            "epoch 505, evaluation 0.55625, avg_loss 0.011570559654501267\n",
            "epoch 506, evaluation 0.5625, avg_loss 0.00671407014306169\n",
            "epoch 507, evaluation 0.55625, avg_loss 0.006220850374666043\n",
            "epoch 508, evaluation 0.5375, avg_loss 0.011924185042153113\n",
            "epoch 509, evaluation 0.5625, avg_loss 0.005334698539809324\n",
            "epoch 510, evaluation 0.55625, avg_loss 0.009254769532708451\n",
            "epoch 511, evaluation 0.56875, avg_loss 0.004127593446173705\n",
            "epoch 512, evaluation 0.575, avg_loss 0.013677351563819684\n",
            "epoch 513, evaluation 0.58125, avg_loss 0.017018910669139586\n",
            "epoch 514, evaluation 0.5375, avg_loss 0.009406560438219459\n",
            "epoch 515, evaluation 0.54375, avg_loss 0.008807331559364685\n",
            "epoch 516, evaluation 0.5625, avg_loss 0.007834826508769765\n",
            "epoch 517, evaluation 0.56875, avg_loss 0.010765848483424633\n",
            "epoch 518, evaluation 0.54375, avg_loss 0.014994121086783708\n",
            "epoch 519, evaluation 0.55625, avg_loss 0.030593495615175926\n",
            "epoch 520, evaluation 0.54375, avg_loss 0.008065943969995715\n",
            "epoch 521, evaluation 0.51875, avg_loss 0.012257995930849575\n",
            "epoch 522, evaluation 0.55625, avg_loss 0.016479472038918176\n",
            "epoch 523, evaluation 0.55, avg_loss 0.023111209576018154\n",
            "epoch 524, evaluation 0.5375, avg_loss 0.006729793365229853\n",
            "epoch 525, evaluation 0.55625, avg_loss 0.004906467214459554\n",
            "epoch 526, evaluation 0.5875, avg_loss 0.01640347032225691\n",
            "epoch 527, evaluation 0.55, avg_loss 0.023222611178061923\n",
            "epoch 528, evaluation 0.5625, avg_loss 0.0014871645485982298\n",
            "epoch 529, evaluation 0.5875, avg_loss 0.0072942308819619935\n",
            "epoch 530, evaluation 0.53125, avg_loss 0.012460317485965789\n",
            "epoch 531, evaluation 0.5375, avg_loss 0.002389651833800599\n",
            "epoch 532, evaluation 0.53125, avg_loss 0.011342003088793717\n",
            "epoch 533, evaluation 0.54375, avg_loss 0.016762084656511432\n",
            "epoch 534, evaluation 0.56875, avg_loss 0.006549281146726571\n",
            "epoch 535, evaluation 0.5375, avg_loss 0.0110236338339746\n",
            "epoch 536, evaluation 0.5625, avg_loss 0.007042106465087272\n",
            "epoch 537, evaluation 0.5375, avg_loss 0.0032962836528895425\n",
            "epoch 538, evaluation 0.54375, avg_loss 0.0033786357438657434\n",
            "epoch 539, evaluation 0.54375, avg_loss 0.002232583437580615\n",
            "epoch 540, evaluation 0.55625, avg_loss 0.007911095849703998\n",
            "epoch 541, evaluation 0.5625, avg_loss 0.0069109107105759906\n",
            "epoch 542, evaluation 0.55625, avg_loss 0.021385713247582316\n",
            "epoch 543, evaluation 0.5875, avg_loss 0.01671436180768069\n",
            "epoch 544, evaluation 0.56875, avg_loss 0.03211609252903145\n",
            "epoch 545, evaluation 0.55, avg_loss 0.029877733779721895\n",
            "epoch 546, evaluation 0.575, avg_loss 0.03194178535195533\n",
            "epoch 547, evaluation 0.55, avg_loss 0.01794276320724748\n",
            "epoch 548, evaluation 0.56875, avg_loss 0.010244987808982841\n",
            "epoch 549, evaluation 0.53125, avg_loss 0.013068929323344492\n",
            "epoch 550, evaluation 0.54375, avg_loss 0.015037969127297402\n",
            "epoch 551, evaluation 0.56875, avg_loss 0.0020909747574478386\n",
            "epoch 552, evaluation 0.5375, avg_loss 0.0072685136256041005\n",
            "epoch 553, evaluation 0.54375, avg_loss 0.006391334469662979\n",
            "epoch 554, evaluation 0.55625, avg_loss 0.031608764358679764\n",
            "epoch 555, evaluation 0.55625, avg_loss 0.02858841064153239\n",
            "epoch 556, evaluation 0.58125, avg_loss 0.01225341207755264\n",
            "epoch 557, evaluation 0.5375, avg_loss 0.003281112911645323\n",
            "epoch 558, evaluation 0.5625, avg_loss 0.009422984084812925\n",
            "epoch 559, evaluation 0.56875, avg_loss 0.0043965742865111675\n",
            "epoch 560, evaluation 0.56875, avg_loss 0.007282998732989654\n",
            "epoch 561, evaluation 0.5375, avg_loss 0.015523966785985976\n",
            "epoch 562, evaluation 0.56875, avg_loss 0.004287307271442842\n",
            "epoch 563, evaluation 0.5625, avg_loss 0.00357892315951176\n",
            "epoch 564, evaluation 0.55, avg_loss 0.0043171551311388615\n",
            "epoch 565, evaluation 0.53125, avg_loss 0.015744078368879855\n",
            "epoch 566, evaluation 0.525, avg_loss 0.004251674315310083\n",
            "epoch 567, evaluation 0.5625, avg_loss 0.027664233947871254\n",
            "epoch 568, evaluation 0.5875, avg_loss 0.012090485566295684\n",
            "epoch 569, evaluation 0.5375, avg_loss 0.056658453587442634\n",
            "epoch 570, evaluation 0.55625, avg_loss 0.037302783524501136\n",
            "epoch 571, evaluation 0.5875, avg_loss 0.008574700300232507\n",
            "epoch 572, evaluation 0.54375, avg_loss 0.014289266837295145\n",
            "epoch 573, evaluation 0.55, avg_loss 0.0035886409372324125\n",
            "epoch 574, evaluation 0.575, avg_loss 0.017834147656685673\n",
            "epoch 575, evaluation 0.5625, avg_loss 0.03387048874865286\n",
            "epoch 576, evaluation 0.575, avg_loss 0.02661569052725099\n",
            "epoch 577, evaluation 0.54375, avg_loss 0.03608148749917746\n",
            "epoch 578, evaluation 0.56875, avg_loss 0.01219136961735785\n",
            "epoch 579, evaluation 0.58125, avg_loss 0.006931058166082948\n",
            "epoch 580, evaluation 0.575, avg_loss 0.0088372819009237\n",
            "epoch 581, evaluation 0.575, avg_loss 0.013798151456285268\n",
            "epoch 582, evaluation 0.55625, avg_loss 0.019848914761678316\n",
            "epoch 583, evaluation 0.55625, avg_loss 0.026764912155340426\n",
            "epoch 584, evaluation 0.55, avg_loss 0.00789450021693483\n",
            "epoch 585, evaluation 0.5375, avg_loss 0.016257935293833726\n",
            "epoch 586, evaluation 0.54375, avg_loss 0.011296652589226141\n",
            "epoch 587, evaluation 0.575, avg_loss 0.024228301792754793\n",
            "epoch 588, evaluation 0.58125, avg_loss 0.008297044059145265\n",
            "epoch 589, evaluation 0.575, avg_loss 0.008825430509750732\n",
            "epoch 590, evaluation 0.59375, avg_loss 0.014906975466874427\n",
            "epoch 591, evaluation 0.575, avg_loss 0.007572297714068555\n",
            "epoch 592, evaluation 0.55625, avg_loss 0.001847877720138058\n",
            "epoch 593, evaluation 0.55625, avg_loss 0.001047464698785916\n",
            "epoch 594, evaluation 0.5625, avg_loss 0.0017270361684495583\n",
            "epoch 595, evaluation 0.5625, avg_loss 0.0014453330644755625\n",
            "epoch 596, evaluation 0.575, avg_loss 0.006049973152403254\n",
            "epoch 597, evaluation 0.5375, avg_loss 0.0014389527757884934\n",
            "epoch 598, evaluation 0.56875, avg_loss 0.0020629639620892704\n",
            "epoch 599, evaluation 0.5875, avg_loss 0.006567282907781191\n",
            "epoch 600, evaluation 0.58125, avg_loss 0.0020951532846083865\n",
            "epoch 601, evaluation 0.575, avg_loss 0.003112481039715931\n",
            "epoch 602, evaluation 0.5875, avg_loss 0.002675218920921907\n",
            "epoch 603, evaluation 0.575, avg_loss 0.012312873682822101\n",
            "epoch 604, evaluation 0.55625, avg_loss 0.0028686610006843694\n",
            "epoch 605, evaluation 0.575, avg_loss 0.009120289795100689\n",
            "epoch 606, evaluation 0.56875, avg_loss 0.0034214529587188736\n",
            "epoch 607, evaluation 0.56875, avg_loss 0.0022028616978786884\n",
            "epoch 608, evaluation 0.5625, avg_loss 0.002972138294717297\n",
            "epoch 609, evaluation 0.55625, avg_loss 0.004963508137734607\n",
            "epoch 610, evaluation 0.5625, avg_loss 0.010719965120370035\n",
            "epoch 611, evaluation 0.5875, avg_loss 0.02326841988542583\n",
            "epoch 612, evaluation 0.575, avg_loss 0.008249323919881136\n",
            "epoch 613, evaluation 0.53125, avg_loss 0.006403005818719975\n",
            "epoch 614, evaluation 0.5375, avg_loss 0.018506192677887157\n",
            "epoch 615, evaluation 0.54375, avg_loss 0.008682591049000621\n",
            "epoch 616, evaluation 0.575, avg_loss 0.00739029872638639\n",
            "epoch 617, evaluation 0.5625, avg_loss 0.003386433243576903\n",
            "epoch 618, evaluation 0.54375, avg_loss 0.017943054388160816\n",
            "epoch 619, evaluation 0.55625, avg_loss 0.024808947968995198\n",
            "epoch 620, evaluation 0.56875, avg_loss 0.020063502719858663\n",
            "epoch 621, evaluation 0.53125, avg_loss 0.012954685866134241\n",
            "epoch 622, evaluation 0.55, avg_loss 0.012474585778545589\n",
            "epoch 623, evaluation 0.56875, avg_loss 0.014099913206882775\n",
            "epoch 624, evaluation 0.54375, avg_loss 0.008118096573161893\n",
            "epoch 625, evaluation 0.575, avg_loss 0.013732407550560311\n",
            "epoch 626, evaluation 0.5625, avg_loss 0.003884848102461547\n",
            "epoch 627, evaluation 0.56875, avg_loss 0.0067177501245168966\n",
            "epoch 628, evaluation 0.5375, avg_loss 0.013533130139694548\n",
            "epoch 629, evaluation 0.58125, avg_loss 0.01165775365079753\n",
            "epoch 630, evaluation 0.55625, avg_loss 0.006010626413626596\n",
            "epoch 631, evaluation 0.59375, avg_loss 0.0012356941733742133\n",
            "epoch 632, evaluation 0.5625, avg_loss 0.0019763495860388503\n",
            "epoch 633, evaluation 0.55625, avg_loss 0.0014197943703038618\n",
            "epoch 634, evaluation 0.55, avg_loss 0.005315810471074656\n",
            "epoch 635, evaluation 0.575, avg_loss 0.0038825623065349645\n",
            "epoch 636, evaluation 0.5625, avg_loss 0.009795502036286052\n",
            "epoch 637, evaluation 0.55, avg_loss 0.00893652098311577\n",
            "epoch 638, evaluation 0.6125, avg_loss 0.007180378471093718\n",
            "epoch 639, evaluation 0.58125, avg_loss 0.0032737139379605653\n",
            "epoch 640, evaluation 0.5625, avg_loss 0.0027161725447513163\n",
            "epoch 641, evaluation 0.56875, avg_loss 0.004426471068290994\n",
            "epoch 642, evaluation 0.5625, avg_loss 0.0029818847135175018\n",
            "epoch 643, evaluation 0.5625, avg_loss 0.0037390140816569327\n",
            "epoch 644, evaluation 0.54375, avg_loss 0.011603969741554466\n",
            "epoch 645, evaluation 0.55625, avg_loss 0.013955338724190369\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-23-1381260104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# manual_write_study_params(trainer.study_name, trainer.storage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-21-1404354344.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1404354344.py\u001b[0m in \u001b[0;36m_train_loop\u001b[0;34m(self, n_epochs, should_save, should_print)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# manual_write_study_params(trainer.study_name, trainer.storage)\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "icmtc_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}