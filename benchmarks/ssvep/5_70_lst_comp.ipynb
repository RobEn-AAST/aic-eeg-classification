{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a563374",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "2a563374",
        "outputId": "53432013-182e-48b1-c2b3-fd43207f4cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (4.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/zeyadcode/.pyenv/versions/3.12.11/envs/icmtc_venv/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeyadcode/.pyenv/versions/icmtc_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os\n",
        "if not os.path.exists('./modules') and not os.path.exists('modules.zip'):\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "if not os.path.exists('./modules') and os.path.exists('modules.zip'):\n",
        "    os.system('unzip modules.zip -d .')\n",
        "\n",
        "!pip3 install optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import optuna\n",
        "from modules import Trainer\n",
        "from modules.competition_dataset import EEGDataset\n",
        "from modules.utils import split_and_get_loaders, evaluate_model, get_closest_divisor\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8c5bf7b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = './data/mtcaic3'\n",
        "model_path = './checkpoints/ssvep/models/70_lstm_ssvep.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a17b42a7",
      "metadata": {
        "id": "a17b42a7"
      },
      "outputs": [],
      "source": [
        "# Add this at the beginning of your notebook, after imports\n",
        "def set_random_seeds(seed=42):\n",
        "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Call this function before creating datasets and models\n",
        "set_random_seeds(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8b83a09b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b83a09b",
        "outputId": "62628525-392a-45a8-8d28-30c10c6b2296"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Convolution.cpp:1037.)\n",
            "  return F.conv2d(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1870, -0.0344,  0.1441,  0.0280],\n",
              "        [-0.0968, -0.0085,  0.1039,  0.0302],\n",
              "        [-0.1788,  0.0431,  0.0535,  0.0239],\n",
              "        [-0.1983,  0.0379,  0.0793, -0.0029],\n",
              "        [-0.2682, -0.0221,  0.1113, -0.0013]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None):\n",
        "        if h0 is None or c0 is None:\n",
        "            h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "            c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class DepthWiseConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, dim_mult=1, padding=0, bias=False):\n",
        "        super(DepthWiseConv2D, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(in_channels, in_channels * dim_mult, padding=padding, kernel_size=kernel_size, groups=in_channels, bias=bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.depthwise(x)\n",
        "\n",
        "\n",
        "class SeperableConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, bias=False):\n",
        "        super(SeperableConv2D, self).__init__()\n",
        "        self.depthwise = DepthWiseConv2D(in_channels, kernel_size, padding=padding)\n",
        "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "class SSVEPClassifier(nn.Module):\n",
        "    # EEG Net Based\n",
        "    # todo look at this https://paperswithcode.com/paper/a-transformer-based-deep-neural-network-model\n",
        "    def __init__(self, n_electrodes=16, n_samples=128, out_dim=4, dropout=0.25, kernLength=256, F1=96, D=1, F2=96, hidden_dim=100, layer_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # B x C x T\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(F1),\n",
        "            #\n",
        "            DepthWiseConv2D(F1, (n_electrodes, 1), dim_mult=D, bias=False),\n",
        "            nn.BatchNorm2d(F1*D),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d((1, 2)), # todo try making this max pool\n",
        "            nn.Dropout(dropout),\n",
        "            #\n",
        "            SeperableConv2D(F1 * D, F2, kernel_size=(1, 16), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(F2),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d((1, 4)),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.lstm_head = LSTMModel(F2, hidden_dim, layer_dim, out_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"expected input shape: BxCxT\"\"\"\n",
        "        x = x.unsqueeze(1)\n",
        "        y = self.block_1(x) # B x F1 x 1 x time_sub\n",
        "\n",
        "        y = y.squeeze(2) # B x F1 x time_sub\n",
        "        y = y.permute(0, 2, 1) # B x time_sub x F1\n",
        "        y = self.lstm_head(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "dummy_x = torch.randn(5, 14, 320)\n",
        "model = SSVEPClassifier(n_electrodes=dummy_x.shape[1], n_samples=dummy_x.shape[2])\n",
        "model(dummy_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42089fb8",
      "metadata": {
        "id": "42089fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "175\n"
          ]
        }
      ],
      "source": [
        "window_length = get_closest_divisor(160)\n",
        "print(window_length)\n",
        "stride = window_length // 3\n",
        "batch_size = 64\n",
        "\n",
        "dataset = EEGDataset(data_path, window_length=window_length, stride=stride)\n",
        "train_loader, val_loader, test_loader = split_and_get_loaders(dataset, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aw8bHOjIgqv",
      "metadata": {
        "id": "5aw8bHOjIgqv"
      },
      "outputs": [],
      "source": [
        "model = SSVEPClassifier(\n",
        "    n_electrodes=dummy_x.shape[1],\n",
        "    n_samples=dummy_x.shape[2],\n",
        "    dropout=0.33066508963955576,\n",
        "    kernLength=256,\n",
        "    F1 = 128,\n",
        "    D = 2,\n",
        "    F2 = 96,\n",
        "    hidden_dim=256,\n",
        "    layer_dim=3,\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b164b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b164b5",
        "outputId": "57e8da82-5cf2-4c36-e202-52b3ef93af60"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.00030241790493218325)\n",
        "avg_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = 0\n",
        "    model.train()\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y_pred = model(x).to(device)\n",
        "\n",
        "        loss = criterion(y_pred, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= len(train_loader)\n",
        "    avg_losses.append(avg_loss)\n",
        "\n",
        "    evaluation = evaluate_model(model, val_loader, device)\n",
        "    val_accuracies.append(evaluation)\n",
        "    print(f'epoch: {epoch}, avg_loss: {avg_loss}, val_evaluation: {evaluation}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OBfEejXsGwBy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "OBfEejXsGwBy",
        "outputId": "91f81846-7357-407e-934f-637a9ee97ab1"
      },
      "outputs": [],
      "source": [
        "# maxpool\n",
        "plt.plot(range(len(avg_losses)), avg_losses, \"b-\", label=\"trainingg loss\")\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies, \"r-\", label=\"validation accuracies\")\n",
        "plt.legend()\n",
        "print(f\"min avg_losses: {min(avg_losses)}\")\n",
        "print(f\"max val_accuracies: {max(val_accuracies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2dec2ee1",
      "metadata": {
        "id": "2dec2ee1"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def _prepare_training(self, is_trial, stride_factor=2, do_not_modify_network=True):\n",
        "        super()._prepare_training(is_trial, stride_factor, do_not_modify_network)\n",
        "        assert self.dataset is not None\n",
        "        \n",
        "        if is_trial:\n",
        "            assert isinstance(self.trial, optuna.Trial), \"trial is none, cant' suggest params\"\n",
        "\n",
        "            if do_not_modify_network:\n",
        "                best_params = self._get_study().best_params if do_not_modify_network else None\n",
        "                assert best_params is not None, \"best_params is None, can't use them\"\n",
        "                \n",
        "                kernLength = best_params[\"kernLength\"]\n",
        "                F1 = best_params[\"F1\"]\n",
        "                D = best_params[\"D\"]\n",
        "                F2 = best_params[\"F2\"]\n",
        "                hidden_dim = best_params[\"hidden_dim\"]\n",
        "                layer_dim = best_params[\"layer_dim\"]\n",
        "                \n",
        "            else:\n",
        "                kernLength = self.trial.suggest_categorical(\"kernLength\", [128, 256, 512])\n",
        "                F1 = self.trial.suggest_categorical(\"F1\", [64, 96, 128])\n",
        "                D = self.trial.suggest_categorical(\"D\", [1, 2, 3])\n",
        "                F2 = self.trial.suggest_categorical(\"F2\", [64, 96, 128])\n",
        "                hidden_dim = self.trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
        "                layer_dim = self.trial.suggest_categorical(\"layer_dim\", [1, 2, 3, 4])\n",
        "\n",
        "            dropout = self.trial.suggest_float(\"dropout\", 0, 0.5)\n",
        "            lr = self.trial.suggest_float(\"lr\", 3e-4, 3e-2, log=True)\n",
        "\n",
        "        else:\n",
        "            best_params = self._get_study().best_params\n",
        "            kernLength = best_params[\"kernLength\"]\n",
        "            F1 = best_params[\"F1\"]\n",
        "            D = best_params[\"D\"]\n",
        "            F2 = best_params[\"F2\"]\n",
        "            hidden_dim = best_params[\"hidden_dim\"]\n",
        "            layer_dim = best_params[\"layer_dim\"]\n",
        "            dropout = best_params[\"dropout\"]\n",
        "            lr = best_params[\"lr\"]\n",
        "            \n",
        "        n_samples = self.dataset.data[0].shape[1]  # data[x] shape CxT\n",
        "        n_electrodes = self.dataset.data[0].shape[0]\n",
        "\n",
        "        n_samples = self.dataset.data[0].shape[1]  # data[x] shape CxT\n",
        "        n_electrodes = self.dataset.data[0].shape[0]\n",
        "\n",
        "        self.model = SSVEPClassifier(\n",
        "            n_electrodes=n_electrodes, n_samples=n_samples, out_dim=4, dropout=dropout, kernLength=kernLength, F1=F1, D=D, F2=F1, hidden_dim=hidden_dim, layer_dim=layer_dim\n",
        "        )\n",
        "        if model_path is not None:\n",
        "            self.model.load_state_dict(torch.load(model_path))\n",
        "            print(f\"loaded model weights from {model_path}\")\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr)\n",
        "        \n",
        "\n",
        "# todo mke it accept stride size, model_path, database_path, should_load_model\n",
        "\n",
        "trainer = CustomTrainer(data_path, train_epochs=10000, optuna_n_trials=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a132bf51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a132bf51",
        "outputId": "a9aa3e55-b05b-451b-898e-4c92b2eb3fb6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-19 20:47:11,718] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n",
            "[I 2025-06-19 20:52:59,283] Trial 15 finished with value: 0.5607876712328768 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.027864116927831084, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 4, 'lr': 0.00031228685999486617}. Best is trial 11 with value: 0.5625.\n",
            "[I 2025-06-19 20:58:32,786] Trial 16 finished with value: 0.3264126712328767 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.4821978058864569, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 4, 'lr': 0.0011883837399415897}. Best is trial 11 with value: 0.5625.\n",
            "[I 2025-06-19 21:02:50,556] Trial 17 finished with value: 0.5834760273972602 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.05276658823165456, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0009340930303094033}. Best is trial 17 with value: 0.5834760273972602.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped, T.shape: (8, 348), self.window_length: 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-19 21:06:53,216] Trial 18 finished with value: 0.516875 and parameters: {'window_length': 350, 'batch_size': 64, 'dropout': 0.07601412338776302, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.001026060389437682}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:11:08,615] Trial 19 finished with value: 0.5380993150684932 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.0690609097641802, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 128, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0021895134509308536}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:18:38,071] Trial 20 finished with value: 0.5366010273972602 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.21214002674930119, 'kernLength': 256, 'F1': 128, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 3, 'lr': 0.0007009681317028536}. Best is trial 17 with value: 0.5834760273972602.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped, T.shape: (8, 348), self.window_length: 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-19 21:23:56,769] Trial 21 finished with value: 0.2834375 and parameters: {'window_length': 350, 'batch_size': 64, 'dropout': 0.07768407679589648, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.008744327559435102}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:27:38,777] Trial 22 finished with value: 0.45601851851851855 and parameters: {'window_length': 175, 'batch_size': 64, 'dropout': 0.24733667912308432, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 128, 'hidden_dim': 128, 'layer_dim': 2, 'lr': 0.0007590623631616397}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:35:07,341] Trial 23 finished with value: 0.4835188356164384 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.05033536560317159, 'kernLength': 256, 'F1': 128, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 3, 'lr': 0.001756712667776652}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:40:40,714] Trial 24 finished with value: 0.5575770547945206 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.013186736386600627, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 4, 'lr': 0.0003503502571122257}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:46:14,636] Trial 25 finished with value: 0.5340325342465754 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.058722882826506465, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 4, 'lr': 0.00032109850640409294}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:51:49,084] Trial 26 finished with value: 0.5314640410958904 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.0018713294934434854, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 4, 'lr': 0.0006595712996069931}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 21:55:59,456] Trial 27 finished with value: 0.5768407534246576 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.09671220549794325, 'kernLength': 256, 'F1': 64, 'D': 2, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0009496046993860075}. Best is trial 17 with value: 0.5834760273972602.\n",
            "[I 2025-06-19 22:01:41,436] Trial 28 finished with value: 0.6127996575342466 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.1103041385689191, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.001446527896878632}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:08:08,055] Trial 29 finished with value: 0.5875850340136054 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.10910138621086878, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0016025178853464187}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:14:34,667] Trial 30 finished with value: 0.5482568027210885 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.15953093926533618, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0014224650262076085}. Best is trial 28 with value: 0.6127996575342466.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped, T.shape: (8, 348), self.window_length: 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-19 22:23:15,114] Trial 31 finished with value: 0.49597772277227725 and parameters: {'window_length': 350, 'batch_size': 32, 'dropout': 0.13263392186529155, 'kernLength': 512, 'F1': 128, 'D': 3, 'F2': 128, 'hidden_dim': 128, 'layer_dim': 2, 'lr': 0.002221902346079088}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:28:47,970] Trial 32 finished with value: 0.34288594470046085 and parameters: {'window_length': 175, 'batch_size': 32, 'dropout': 0.21382958645461678, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 64, 'hidden_dim': 64, 'layer_dim': 2, 'lr': 0.004492754781135221}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:34:21,202] Trial 33 finished with value: 0.30184331797235026 and parameters: {'window_length': 175, 'batch_size': 32, 'dropout': 0.10899856059028316, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 128, 'hidden_dim': 64, 'layer_dim': 2, 'lr': 0.007719820042002409}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:40:47,487] Trial 34 finished with value: 0.5493197278911565 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.0963808912410917, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0016106953240132007}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:47:13,632] Trial 35 finished with value: 0.5907738095238095 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.04765409202209127, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0009785649993910852}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 22:53:45,296] Trial 36 finished with value: 0.3968962585034014 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.15438680878659658, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.002702419583202579}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:02:01,522] Trial 37 finished with value: 0.5756802721088435 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.20028232724939898, 'kernLength': 512, 'F1': 96, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0008353403341422332}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:08:27,326] Trial 38 finished with value: 0.5686649659863946 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.04241859113606651, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 64, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.001913695011995178}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:15:46,855] Trial 39 finished with value: 0.31909013605442177 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.16600442377432384, 'kernLength': 512, 'F1': 96, 'D': 3, 'F2': 96, 'hidden_dim': 128, 'layer_dim': 3, 'lr': 0.003245550486899207}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:22:12,944] Trial 40 finished with value: 0.5539965986394558 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.13304791805281246, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.001302422737365304}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:28:39,501] Trial 41 finished with value: 0.24914965986394558 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.09017688600801221, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.029480990054087663}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:34:09,057] Trial 42 finished with value: 0.5384778911564626 and parameters: {'window_length': 250, 'batch_size': 32, 'dropout': 0.03857924839402109, 'kernLength': 512, 'F1': 96, 'D': 1, 'F2': 96, 'hidden_dim': 64, 'layer_dim': 2, 'lr': 0.0005214854086865871}. Best is trial 28 with value: 0.6127996575342466.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped, T.shape: (8, 348), self.window_length: 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-19 23:38:40,672] Trial 43 finished with value: 0.5176361386138614 and parameters: {'window_length': 350, 'batch_size': 32, 'dropout': 0.11759007158529765, 'kernLength': 512, 'F1': 64, 'D': 3, 'F2': 64, 'hidden_dim': 128, 'layer_dim': 1, 'lr': 0.0009870293520543205}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:42:46,750] Trial 44 finished with value: 0.5363869863013698 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.09343791190460347, 'kernLength': 128, 'F1': 64, 'D': 3, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0009343370216390094}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:47:37,625] Trial 45 finished with value: 0.5928938356164384 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.18176937346202754, 'kernLength': 512, 'F1': 64, 'D': 1, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0006075298979437705}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:52:28,393] Trial 46 finished with value: 0.592679794520548 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.14278340763460748, 'kernLength': 512, 'F1': 64, 'D': 1, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0005831787615168507}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-19 23:57:19,578] Trial 47 finished with value: 0.59375 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.1771561850447629, 'kernLength': 512, 'F1': 64, 'D': 1, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0005969741537435476}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-20 00:02:10,771] Trial 48 finished with value: 0.5539383561643836 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.23929500223872122, 'kernLength': 512, 'F1': 64, 'D': 1, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 2, 'lr': 0.0004251352349791034}. Best is trial 28 with value: 0.6127996575342466.\n",
            "[I 2025-06-20 00:09:34,685] Trial 49 finished with value: 0.572345890410959 and parameters: {'window_length': 250, 'batch_size': 64, 'dropout': 0.1863453473858135, 'kernLength': 512, 'F1': 128, 'D': 1, 'F2': 96, 'hidden_dim': 256, 'layer_dim': 1, 'lr': 0.0006274937830123262}. Best is trial 28 with value: 0.6127996575342466.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Study statistics:\n",
            "  Number of finished trials: 50\n",
            "  Number of pruned trials: 0\n",
            "  Number of complete trials: 47\n",
            "\n",
            "Best trial:\n",
            "  Value: 0.6127996575342466\n",
            "\n",
            "Best hyperparameters:\n",
            "  window_length: 250\n",
            "  batch_size: 64\n",
            "  dropout: 0.1103041385689191\n",
            "  kernLength: 512\n",
            "  F1: 64\n",
            "  D: 3\n",
            "  F2: 96\n",
            "  hidden_dim: 256\n",
            "  layer_dim: 2\n",
            "  lr: 0.001446527896878632\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'window_length': 250,\n",
              " 'batch_size': 64,\n",
              " 'dropout': 0.1103041385689191,\n",
              " 'kernLength': 512,\n",
              " 'F1': 64,\n",
              " 'D': 3,\n",
              " 'F2': 96,\n",
              " 'hidden_dim': 256,\n",
              " 'layer_dim': 2,\n",
              " 'lr': 0.001446527896878632}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "delete_existing = False\n",
        "trainer.optimize(delete_existing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a93294ea",
      "metadata": {
        "id": "a93294ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-20 00:09:34,987] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n",
            "[I 2025-06-20 00:09:46,329] Using an existing study with name 'ssvep_classifier_optimization' instead of creating a new one.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: self.trial is none, we are probably in acutal training phase\n",
            "epoch 0, evaluation 0.2917380136986301, avg_loss 1.3841414694058694\n",
            "epoch 1, evaluation 0.3073630136986301, avg_loss 1.3780235868389323\n",
            "epoch 2, evaluation 0.3092893835616438, avg_loss 1.3685882847187882\n",
            "epoch 3, evaluation 0.3210616438356164, avg_loss 1.3565816968174305\n",
            "epoch 4, evaluation 0.3285530821917808, avg_loss 1.340835875171726\n",
            "epoch 5, evaluation 0.3428938356164384, avg_loss 1.3247399742320431\n",
            "epoch 6, evaluation 0.3589469178082192, avg_loss 1.3005372795007997\n",
            "epoch 7, evaluation 0.3812071917808219, avg_loss 1.277446111177994\n",
            "epoch 8, evaluation 0.3929794520547945, avg_loss 1.250127545049635\n",
            "epoch 9, evaluation 0.4019691780821918, avg_loss 1.225795670687142\n",
            "epoch 10, evaluation 0.4252996575342466, avg_loss 1.1947784702656632\n",
            "epoch 11, evaluation 0.4353595890410959, avg_loss 1.1680164892794722\n",
            "epoch 12, evaluation 0.4548373287671233, avg_loss 1.1408713910539272\n",
            "epoch 13, evaluation 0.4674657534246575, avg_loss 1.108100170806303\n",
            "epoch 14, evaluation 0.4713184931506849, avg_loss 1.0768279554480213\n",
            "epoch 15, evaluation 0.4886558219178082, avg_loss 1.051260601666014\n",
            "epoch 16, evaluation 0.5068493150684932, avg_loss 1.0332731762174832\n",
            "epoch 17, evaluation 0.4995719178082192, avg_loss 1.003910225326732\n",
            "epoch 18, evaluation 0.5237585616438356, avg_loss 0.9798888659073135\n",
            "epoch 19, evaluation 0.5321061643835616, avg_loss 0.9488545100567705\n",
            "epoch 20, evaluation 0.5378852739726028, avg_loss 0.930983619366662\n",
            "epoch 21, evaluation 0.5455907534246576, avg_loss 0.9196683431075792\n",
            "epoch 22, evaluation 0.5667808219178082, avg_loss 0.8958433226003485\n",
            "epoch 23, evaluation 0.556720890410959, avg_loss 0.8745374651278479\n",
            "epoch 24, evaluation 0.5620719178082192, avg_loss 0.8544001955097005\n",
            "epoch 25, evaluation 0.5802654109589042, avg_loss 0.8418931880239713\n",
            "epoch 26, evaluation 0.5665667808219178, avg_loss 0.828203887454534\n",
            "epoch 27, evaluation 0.5916095890410958, avg_loss 0.8121557757005853\n",
            "epoch 28, evaluation 0.5980308219178082, avg_loss 0.7871722716396138\n",
            "epoch 29, evaluation 0.5706335616438356, avg_loss 0.7820029927512346\n",
            "epoch 30, evaluation 0.6089469178082192, avg_loss 0.7632692451194182\n",
            "epoch 31, evaluation 0.5961044520547946, avg_loss 0.7478966699818433\n",
            "epoch 32, evaluation 0.6033818493150684, avg_loss 0.7362159233982281\n",
            "epoch 33, evaluation 0.6053082191780822, avg_loss 0.7190687279580004\n",
            "epoch 34, evaluation 0.6183647260273972, avg_loss 0.7117920546208398\n",
            "epoch 35, evaluation 0.615154109589041, avg_loss 0.7026617330009655\n",
            "epoch 36, evaluation 0.6232876712328768, avg_loss 0.6920900743896679\n",
            "epoch 37, evaluation 0.6179366438356164, avg_loss 0.6764381243010699\n",
            "epoch 38, evaluation 0.6282106164383562, avg_loss 0.6720363942243285\n",
            "epoch 39, evaluation 0.6386986301369864, avg_loss 0.655505686594268\n",
            "epoch 40, evaluation 0.6401969178082192, avg_loss 0.6500551500562894\n",
            "epoch 41, evaluation 0.6365582191780822, avg_loss 0.6423619893647856\n",
            "epoch 42, evaluation 0.621361301369863, avg_loss 0.6328165509943234\n",
            "epoch 43, evaluation 0.6491866438356164, avg_loss 0.6264386816550109\n",
            "epoch 44, evaluation 0.6414811643835616, avg_loss 0.6193403859259718\n",
            "epoch 45, evaluation 0.643193493150685, avg_loss 0.5995468575065419\n",
            "epoch 46, evaluation 0.652611301369863, avg_loss 0.597078765650927\n",
            "epoch 47, evaluation 0.6436215753424658, avg_loss 0.582675993139461\n",
            "epoch 48, evaluation 0.659888698630137, avg_loss 0.5887639845831919\n",
            "epoch 49, evaluation 0.6652397260273972, avg_loss 0.575536896414676\n",
            "epoch 50, evaluation 0.6434075342465754, avg_loss 0.574418140973075\n",
            "epoch 51, evaluation 0.662457191780822, avg_loss 0.564750735234406\n",
            "epoch 52, evaluation 0.6667380136986302, avg_loss 0.5592262954024946\n",
            "epoch 53, evaluation 0.6502568493150684, avg_loss 0.5502155519137948\n",
            "epoch 54, evaluation 0.6564640410958904, avg_loss 0.5446497984861923\n",
            "epoch 55, evaluation 0.663527397260274, avg_loss 0.5400976269932116\n",
            "epoch 56, evaluation 0.650470890410959, avg_loss 0.5331030957779642\n",
            "epoch 57, evaluation 0.6605308219178082, avg_loss 0.5238881913282103\n",
            "epoch 58, evaluation 0.6476883561643836, avg_loss 0.5220209648043422\n",
            "epoch 59, evaluation 0.6714469178082192, avg_loss 0.5106253452220205\n",
            "epoch 60, evaluation 0.668236301369863, avg_loss 0.5021253531262025\n",
            "epoch 61, evaluation 0.6774400684931506, avg_loss 0.5075387157626071\n",
            "epoch 62, evaluation 0.6800085616438356, avg_loss 0.4951039880009021\n",
            "epoch 63, evaluation 0.6748715753424658, avg_loss 0.4969504216970023\n",
            "epoch 64, evaluation 0.6590325342465754, avg_loss 0.491406180899022\n",
            "epoch 65, evaluation 0.6705907534246576, avg_loss 0.4892535000534381\n",
            "epoch 66, evaluation 0.6757277397260274, avg_loss 0.4732204055382033\n",
            "epoch 67, evaluation 0.668236301369863, avg_loss 0.46859150145013456\n",
            "epoch 68, evaluation 0.6667380136986302, avg_loss 0.4752326786518097\n",
            "epoch 69, evaluation 0.6742294520547946, avg_loss 0.4644528408171767\n",
            "epoch 70, evaluation 0.6860017123287672, avg_loss 0.46744372198137185\n",
            "epoch 71, evaluation 0.686429794520548, avg_loss 0.45805028868933856\n",
            "epoch 72, evaluation 0.6832191780821918, avg_loss 0.44945255967520054\n",
            "epoch 73, evaluation 0.6742294520547946, avg_loss 0.45300553643097313\n",
            "epoch 74, evaluation 0.6628852739726028, avg_loss 0.44992809896751984\n",
            "epoch 75, evaluation 0.6892123287671232, avg_loss 0.44823327448408484\n",
            "epoch 76, evaluation 0.6774400684931506, avg_loss 0.43990206950801913\n",
            "epoch 77, evaluation 0.6750856164383562, avg_loss 0.4298843051417399\n",
            "epoch 78, evaluation 0.6853595890410958, avg_loss 0.4343805430804269\n",
            "epoch 79, evaluation 0.6840753424657534, avg_loss 0.4265957745455079\n",
            "epoch 80, evaluation 0.6872859589041096, avg_loss 0.4279796435166213\n",
            "epoch 81, evaluation 0.6842893835616438, avg_loss 0.41498616297366253\n",
            "epoch 82, evaluation 0.694777397260274, avg_loss 0.4151413349782006\n",
            "epoch 83, evaluation 0.6810787671232876, avg_loss 0.4246232164108147\n",
            "epoch 84, evaluation 0.6926369863013698, avg_loss 0.420108463855113\n",
            "epoch 85, evaluation 0.6763698630136986, avg_loss 0.40696618400387846\n",
            "epoch 86, evaluation 0.6894263698630136, avg_loss 0.3969733480679787\n",
            "epoch 87, evaluation 0.682791095890411, avg_loss 0.4061589660280842\n",
            "epoch 88, evaluation 0.691138698630137, avg_loss 0.4047962687278198\n",
            "epoch 89, evaluation 0.6883561643835616, avg_loss 0.3993216983847699\n",
            "epoch 90, evaluation 0.6725171232876712, avg_loss 0.3843830701153157\n",
            "epoch 91, evaluation 0.6881421232876712, avg_loss 0.3898283055273153\n",
            "epoch 92, evaluation 0.7026969178082192, avg_loss 0.38897184916472033\n",
            "epoch 93, evaluation 0.6979880136986302, avg_loss 0.3884312159934286\n",
            "epoch 94, evaluation 0.6939212328767124, avg_loss 0.3908094985505282\n",
            "epoch 95, evaluation 0.6924229452054794, avg_loss 0.37955092267464785\n",
            "epoch 96, evaluation 0.6960616438356164, avg_loss 0.37688597314438577\n",
            "epoch 97, evaluation 0.6964897260273972, avg_loss 0.37454539113125557\n",
            "epoch 98, evaluation 0.6862157534246576, avg_loss 0.36215212430994387\n",
            "epoch 99, evaluation 0.7054794520547946, avg_loss 0.37617246624776873\n",
            "epoch 100, evaluation 0.6956335616438356, avg_loss 0.36562541081743727\n",
            "epoch 101, evaluation 0.6917808219178082, avg_loss 0.36615524847628705\n",
            "epoch 102, evaluation 0.691138698630137, avg_loss 0.36868463729397727\n",
            "epoch 103, evaluation 0.694777397260274, avg_loss 0.3625941673577842\n",
            "epoch 104, evaluation 0.6934931506849316, avg_loss 0.3585154036849232\n",
            "epoch 105, evaluation 0.706763698630137, avg_loss 0.35116714612912325\n",
            "epoch 106, evaluation 0.7044092465753424, avg_loss 0.35000086492401056\n",
            "epoch 107, evaluation 0.7014126712328768, avg_loss 0.3505674104569322\n",
            "epoch 108, evaluation 0.6894263698630136, avg_loss 0.3540759677098969\n",
            "epoch 109, evaluation 0.6962756849315068, avg_loss 0.33786789860765815\n",
            "epoch 110, evaluation 0.693707191780822, avg_loss 0.3389766371351177\n",
            "epoch 111, evaluation 0.6988441780821918, avg_loss 0.3335460747702647\n",
            "epoch 112, evaluation 0.702054794520548, avg_loss 0.33411151039398324\n",
            "epoch 113, evaluation 0.694777397260274, avg_loss 0.3403621303328013\n",
            "epoch 114, evaluation 0.6954195205479452, avg_loss 0.33793970477783075\n",
            "epoch 115, evaluation 0.7003424657534246, avg_loss 0.3367547822200646\n",
            "epoch 116, evaluation 0.6986301369863014, avg_loss 0.3432944571567794\n",
            "epoch 117, evaluation 0.6872859589041096, avg_loss 0.3289053538593195\n",
            "epoch 118, evaluation 0.7059075342465754, avg_loss 0.33018858235771376\n",
            "epoch 119, evaluation 0.6977739726027398, avg_loss 0.32644633789688854\n",
            "epoch 120, evaluation 0.7106164383561644, avg_loss 0.3208032797453767\n",
            "epoch 121, evaluation 0.697345890410959, avg_loss 0.3160061277575412\n",
            "epoch 122, evaluation 0.7014126712328768, avg_loss 0.3165530130014581\n",
            "epoch 123, evaluation 0.7033390410958904, avg_loss 0.3131664052353067\n",
            "epoch 124, evaluation 0.6952054794520548, avg_loss 0.3142646279880556\n",
            "epoch 125, evaluation 0.6988441780821918, avg_loss 0.31369182861457434\n",
            "epoch 126, evaluation 0.708904109589041, avg_loss 0.3152366705870224\n",
            "epoch 127, evaluation 0.686429794520548, avg_loss 0.302664006968676\n",
            "epoch 128, evaluation 0.6913527397260274, avg_loss 0.30740573744652633\n",
            "epoch 129, evaluation 0.7054794520547946, avg_loss 0.304436586620444\n",
            "epoch 130, evaluation 0.7016267123287672, avg_loss 0.2977589193794687\n",
            "epoch 131, evaluation 0.6945633561643836, avg_loss 0.29428878035080636\n",
            "epoch 132, evaluation 0.700556506849315, avg_loss 0.29253758975004746\n",
            "epoch 133, evaluation 0.702054794520548, avg_loss 0.29469778456930384\n",
            "epoch 134, evaluation 0.6979880136986302, avg_loss 0.29652079929739744\n",
            "epoch 135, evaluation 0.6952054794520548, avg_loss 0.2931993878493875\n",
            "epoch 136, evaluation 0.7014126712328768, avg_loss 0.2952633738012637\n",
            "epoch 137, evaluation 0.7054794520547946, avg_loss 0.28926265249818056\n",
            "epoch 138, evaluation 0.7003424657534246, avg_loss 0.29235228686514547\n",
            "epoch 139, evaluation 0.704195205479452, avg_loss 0.2891714410256531\n",
            "epoch 140, evaluation 0.7071917808219178, avg_loss 0.28770446001978245\n",
            "epoch 141, evaluation 0.698416095890411, avg_loss 0.2820640247252028\n",
            "epoch 142, evaluation 0.715111301369863, avg_loss 0.2724281767414788\n",
            "epoch 143, evaluation 0.7097602739726028, avg_loss 0.26528047916747755\n",
            "epoch 144, evaluation 0.7138270547945206, avg_loss 0.2763848664144338\n",
            "epoch 145, evaluation 0.6939212328767124, avg_loss 0.2753918646503303\n",
            "epoch 146, evaluation 0.699486301369863, avg_loss 0.26945605452282956\n",
            "epoch 147, evaluation 0.6964897260273972, avg_loss 0.2786785070673894\n",
            "epoch 148, evaluation 0.7074058219178082, avg_loss 0.26828582307039683\n",
            "epoch 149, evaluation 0.7059075342465754, avg_loss 0.26363717956563176\n",
            "epoch 150, evaluation 0.6934931506849316, avg_loss 0.26946289387799927\n",
            "epoch 151, evaluation 0.6939212328767124, avg_loss 0.25573980616310893\n",
            "epoch 152, evaluation 0.684931506849315, avg_loss 0.27057871409391954\n",
            "epoch 153, evaluation 0.6934931506849316, avg_loss 0.25542567852189985\n",
            "epoch 154, evaluation 0.7146832191780822, avg_loss 0.25816684009160024\n",
            "epoch 155, evaluation 0.7054794520547946, avg_loss 0.2601722030063807\n",
            "epoch 156, evaluation 0.7082619863013698, avg_loss 0.2500557904526339\n",
            "epoch 157, evaluation 0.702054794520548, avg_loss 0.253452714444217\n",
            "epoch 158, evaluation 0.7133989726027398, avg_loss 0.23435424727908635\n",
            "epoch 159, evaluation 0.7044092465753424, avg_loss 0.2595632824352232\n",
            "epoch 160, evaluation 0.700556506849315, avg_loss 0.24214445319721253\n",
            "epoch 161, evaluation 0.7007705479452054, avg_loss 0.24679449056669817\n",
            "epoch 162, evaluation 0.6823630136986302, avg_loss 0.2467342337688147\n",
            "epoch 163, evaluation 0.7086900684931506, avg_loss 0.23764717667284657\n",
            "epoch 164, evaluation 0.699486301369863, avg_loss 0.2456975906077078\n",
            "epoch 165, evaluation 0.704195205479452, avg_loss 0.24974487497139786\n",
            "epoch 166, evaluation 0.694777397260274, avg_loss 0.24315604178582206\n",
            "epoch 167, evaluation 0.7065496575342466, avg_loss 0.2417648201524201\n",
            "epoch 168, evaluation 0.705693493150685, avg_loss 0.24149440544641623\n",
            "epoch 169, evaluation 0.7071917808219178, avg_loss 0.23783413484945135\n",
            "epoch 170, evaluation 0.7044092465753424, avg_loss 0.22973123070294574\n",
            "epoch 171, evaluation 0.7084760273972602, avg_loss 0.2242461309594623\n",
            "epoch 172, evaluation 0.7101883561643836, avg_loss 0.2265523976441157\n",
            "epoch 173, evaluation 0.7119006849315068, avg_loss 0.22523692894285008\n",
            "epoch 174, evaluation 0.716181506849315, avg_loss 0.2282558998819125\n",
            "epoch 175, evaluation 0.7029109589041096, avg_loss 0.21705902880531247\n",
            "epoch 176, evaluation 0.7110445205479452, avg_loss 0.23342471102536735\n",
            "epoch 177, evaluation 0.7133989726027398, avg_loss 0.2268754477470608\n",
            "epoch 178, evaluation 0.7133989726027398, avg_loss 0.21573235854759054\n",
            "epoch 179, evaluation 0.707833904109589, avg_loss 0.22139206260947858\n",
            "epoch 180, evaluation 0.7063356164383562, avg_loss 0.21432424947872\n",
            "epoch 181, evaluation 0.7059075342465754, avg_loss 0.2223824502047846\n",
            "epoch 182, evaluation 0.708904109589041, avg_loss 0.22586707035868855\n",
            "epoch 183, evaluation 0.7029109589041096, avg_loss 0.2207688253183486\n",
            "epoch 184, evaluation 0.7024828767123288, avg_loss 0.21024524248757606\n",
            "epoch 185, evaluation 0.7084760273972602, avg_loss 0.21055717299045143\n",
            "epoch 186, evaluation 0.7121147260273972, avg_loss 0.20477483960784088\n",
            "epoch 187, evaluation 0.7016267123287672, avg_loss 0.2113810714776233\n",
            "epoch 188, evaluation 0.724529109589041, avg_loss 0.21026592394810611\n",
            "epoch 189, evaluation 0.7097602739726028, avg_loss 0.20951754862474184\n",
            "epoch 190, evaluation 0.719820205479452, avg_loss 0.21497676393995852\n",
            "epoch 191, evaluation 0.7142551369863014, avg_loss 0.20544615161873528\n",
            "epoch 192, evaluation 0.712970890410959, avg_loss 0.20690009497990042\n",
            "epoch 193, evaluation 0.7144691780821918, avg_loss 0.20840630432811835\n",
            "epoch 194, evaluation 0.7144691780821918, avg_loss 0.20439837999263052\n",
            "epoch 195, evaluation 0.724529109589041, avg_loss 0.20468297184018766\n",
            "epoch 196, evaluation 0.7183219178082192, avg_loss 0.20097512509989537\n",
            "epoch 197, evaluation 0.6999143835616438, avg_loss 0.1949261762201786\n",
            "epoch 198, evaluation 0.7155393835616438, avg_loss 0.2032675246944872\n",
            "epoch 199, evaluation 0.7095462328767124, avg_loss 0.1941562340800035\n",
            "epoch 200, evaluation 0.7099743150684932, avg_loss 0.2013312037465936\n",
            "epoch 201, evaluation 0.7157534246575342, avg_loss 0.1955607444679333\n",
            "epoch 202, evaluation 0.7084760273972602, avg_loss 0.19716162548984512\n",
            "epoch 203, evaluation 0.711472602739726, avg_loss 0.1962101750328379\n",
            "epoch 204, evaluation 0.7106164383561644, avg_loss 0.19948866247373112\n",
            "epoch 205, evaluation 0.7168236301369864, avg_loss 0.18784011751413346\n",
            "epoch 206, evaluation 0.7170376712328768, avg_loss 0.18869860543537947\n",
            "epoch 207, evaluation 0.7097602739726028, avg_loss 0.1913612242851217\n",
            "epoch 208, evaluation 0.6977739726027398, avg_loss 0.19833284749065416\n",
            "epoch 209, evaluation 0.7080479452054794, avg_loss 0.18776265932089192\n",
            "epoch 210, evaluation 0.696917808219178, avg_loss 0.18822370038951858\n",
            "epoch 211, evaluation 0.7206763698630136, avg_loss 0.1774911357815993\n",
            "epoch 212, evaluation 0.715111301369863, avg_loss 0.18501792629644023\n",
            "epoch 213, evaluation 0.7084760273972602, avg_loss 0.18129952375035166\n",
            "epoch 214, evaluation 0.7069777397260274, avg_loss 0.17731872918242114\n",
            "epoch 215, evaluation 0.7084760273972602, avg_loss 0.196030186924894\n",
            "epoch 216, evaluation 0.7097602739726028, avg_loss 0.18245853746341448\n",
            "epoch 217, evaluation 0.7101883561643836, avg_loss 0.17394536343418945\n",
            "epoch 218, evaluation 0.716181506849315, avg_loss 0.18302951926144503\n",
            "epoch 219, evaluation 0.7095462328767124, avg_loss 0.1810671450096672\n",
            "epoch 220, evaluation 0.7007705479452054, avg_loss 0.17858790205949443\n",
            "epoch 221, evaluation 0.7116866438356164, avg_loss 0.18325586454090426\n",
            "epoch 222, evaluation 0.7127568493150684, avg_loss 0.19503116228822934\n",
            "epoch 223, evaluation 0.7046232876712328, avg_loss 0.18091031746086428\n",
            "epoch 224, evaluation 0.714041095890411, avg_loss 0.17083627872042736\n",
            "epoch 225, evaluation 0.7080479452054794, avg_loss 0.17140760122466894\n",
            "epoch 226, evaluation 0.7206763698630136, avg_loss 0.17657877944283568\n",
            "epoch 227, evaluation 0.7076198630136986, avg_loss 0.17308601162443726\n",
            "epoch 228, evaluation 0.7157534246575342, avg_loss 0.17242513344449512\n",
            "epoch 229, evaluation 0.7080479452054794, avg_loss 0.17079064839724767\n",
            "epoch 230, evaluation 0.7168236301369864, avg_loss 0.17297349496413086\n",
            "epoch 231, evaluation 0.7136130136986302, avg_loss 0.17014571741475898\n",
            "epoch 232, evaluation 0.7097602739726028, avg_loss 0.178928521111355\n",
            "epoch 233, evaluation 0.7112585616438356, avg_loss 0.16900695620451944\n",
            "epoch 234, evaluation 0.7136130136986302, avg_loss 0.17205351656776363\n",
            "epoch 235, evaluation 0.7185359589041096, avg_loss 0.17048765460060814\n",
            "epoch 236, evaluation 0.7080479452054794, avg_loss 0.15868935163874748\n",
            "epoch 237, evaluation 0.7063356164383562, avg_loss 0.16571078487371993\n",
            "epoch 238, evaluation 0.7200342465753424, avg_loss 0.16088355525822962\n",
            "epoch 239, evaluation 0.719820205479452, avg_loss 0.16228108411118136\n",
            "epoch 240, evaluation 0.7189640410958904, avg_loss 0.16405193523456485\n",
            "epoch 241, evaluation 0.6988441780821918, avg_loss 0.1657608953951779\n",
            "epoch 242, evaluation 0.7136130136986302, avg_loss 0.17027098732226986\n",
            "epoch 243, evaluation 0.728167808219178, avg_loss 0.1738733079600132\n",
            "epoch 244, evaluation 0.7159674657534246, avg_loss 0.16406556085004645\n",
            "epoch 245, evaluation 0.7202482876712328, avg_loss 0.14792449471809097\n",
            "epoch 246, evaluation 0.7131849315068494, avg_loss 0.16085968569426212\n",
            "epoch 247, evaluation 0.7084760273972602, avg_loss 0.15914095404036974\n",
            "epoch 248, evaluation 0.7052654109589042, avg_loss 0.16398058682048725\n",
            "epoch 249, evaluation 0.7168236301369864, avg_loss 0.16673228524751582\n",
            "epoch 250, evaluation 0.6979880136986302, avg_loss 0.17137984791297023\n",
            "epoch 251, evaluation 0.715111301369863, avg_loss 0.15763005672369973\n",
            "epoch 252, evaluation 0.707833904109589, avg_loss 0.15934396886219412\n",
            "epoch 253, evaluation 0.7144691780821918, avg_loss 0.15063876027904324\n",
            "epoch 254, evaluation 0.7136130136986302, avg_loss 0.16184001236017478\n",
            "epoch 255, evaluation 0.707833904109589, avg_loss 0.15788383099992395\n",
            "epoch 256, evaluation 0.7238869863013698, avg_loss 0.15264250215584949\n",
            "epoch 257, evaluation 0.7202482876712328, avg_loss 0.16042927514951108\n",
            "epoch 258, evaluation 0.6982020547945206, avg_loss 0.14974241230826257\n",
            "epoch 259, evaluation 0.7069777397260274, avg_loss 0.16174149725396755\n",
            "epoch 260, evaluation 0.7074058219178082, avg_loss 0.1461883845281298\n",
            "epoch 261, evaluation 0.7106164383561644, avg_loss 0.15603702046861082\n",
            "epoch 262, evaluation 0.7119006849315068, avg_loss 0.14947678396257302\n",
            "epoch 263, evaluation 0.7037671232876712, avg_loss 0.14890194405185975\n",
            "epoch 264, evaluation 0.7091181506849316, avg_loss 0.16069032434310954\n",
            "epoch 265, evaluation 0.7082619863013698, avg_loss 0.14364065633360612\n",
            "epoch 266, evaluation 0.7183219178082192, avg_loss 0.1555222782416869\n",
            "epoch 267, evaluation 0.7108304794520548, avg_loss 0.14355613317782595\n",
            "epoch 268, evaluation 0.7172517123287672, avg_loss 0.15305103335718986\n",
            "epoch 269, evaluation 0.711472602739726, avg_loss 0.15035432698362966\n",
            "epoch 270, evaluation 0.7054794520547946, avg_loss 0.13935419515532962\n",
            "epoch 271, evaluation 0.6862157534246576, avg_loss 0.1534016216079057\n",
            "epoch 272, evaluation 0.6872859589041096, avg_loss 0.1537647746377072\n",
            "epoch 273, evaluation 0.7108304794520548, avg_loss 0.15048917082406707\n",
            "epoch 274, evaluation 0.711472602739726, avg_loss 0.1340753891647367\n",
            "epoch 275, evaluation 0.7116866438356164, avg_loss 0.16012916792247256\n",
            "epoch 276, evaluation 0.7050513698630136, avg_loss 0.14510893762364227\n",
            "epoch 277, evaluation 0.6997003424657534, avg_loss 0.14776292798756543\n",
            "epoch 278, evaluation 0.7065496575342466, avg_loss 0.1417548426632154\n",
            "epoch 279, evaluation 0.7029109589041096, avg_loss 0.13149259683944411\n",
            "epoch 280, evaluation 0.7142551369863014, avg_loss 0.14737117930741633\n",
            "epoch 281, evaluation 0.7146832191780822, avg_loss 0.14350187048568563\n",
            "epoch 282, evaluation 0.7084760273972602, avg_loss 0.143088488381798\n",
            "epoch 283, evaluation 0.7095462328767124, avg_loss 0.1372294373305167\n",
            "epoch 284, evaluation 0.716181506849315, avg_loss 0.1292091757879924\n",
            "epoch 285, evaluation 0.712970890410959, avg_loss 0.13704475247127526\n",
            "epoch 286, evaluation 0.6892123287671232, avg_loss 0.14392649323000747\n",
            "epoch 287, evaluation 0.6992722602739726, avg_loss 0.15816533910268443\n",
            "epoch 288, evaluation 0.7101883561643836, avg_loss 0.14931040391073389\n",
            "epoch 289, evaluation 0.7178938356164384, avg_loss 0.13846410918286292\n",
            "epoch 290, evaluation 0.6879280821917808, avg_loss 0.1423417518262641\n",
            "epoch 291, evaluation 0.7065496575342466, avg_loss 0.14084740196496753\n",
            "epoch 292, evaluation 0.7131849315068494, avg_loss 0.14635566758402324\n",
            "epoch 293, evaluation 0.7206763698630136, avg_loss 0.13203910932702534\n",
            "epoch 294, evaluation 0.7074058219178082, avg_loss 0.14116019758758908\n",
            "epoch 295, evaluation 0.703125, avg_loss 0.1359864933889801\n",
            "epoch 296, evaluation 0.7007705479452054, avg_loss 0.14056194444834175\n",
            "epoch 297, evaluation 0.7097602739726028, avg_loss 0.13116309124408132\n",
            "epoch 298, evaluation 0.702054794520548, avg_loss 0.1320064179693996\n",
            "epoch 299, evaluation 0.7009845890410958, avg_loss 0.1375492455279928\n",
            "epoch 300, evaluation 0.7009845890410958, avg_loss 0.12705864634806827\n",
            "epoch 301, evaluation 0.711472602739726, avg_loss 0.13643836084816416\n",
            "epoch 302, evaluation 0.692208904109589, avg_loss 0.1438689111779302\n",
            "epoch 303, evaluation 0.6977739726027398, avg_loss 0.13233889071744379\n",
            "epoch 304, evaluation 0.702054794520548, avg_loss 0.13563709244826588\n",
            "epoch 305, evaluation 0.7170376712328768, avg_loss 0.1272870007586681\n",
            "epoch 306, evaluation 0.7097602739726028, avg_loss 0.14052004437199084\n",
            "epoch 307, evaluation 0.7123287671232876, avg_loss 0.14106860104001173\n",
            "epoch 308, evaluation 0.7208904109589042, avg_loss 0.1267101436853409\n",
            "epoch 309, evaluation 0.7166095890410958, avg_loss 0.13001319272917206\n",
            "epoch 310, evaluation 0.7168236301369864, avg_loss 0.12129821771029699\n",
            "epoch 311, evaluation 0.7095462328767124, avg_loss 0.12721208113608723\n",
            "epoch 312, evaluation 0.7035530821917808, avg_loss 0.12441072802422411\n",
            "epoch 313, evaluation 0.7202482876712328, avg_loss 0.14158008834189278\n",
            "epoch 314, evaluation 0.7054794520547946, avg_loss 0.13880560957526755\n",
            "epoch 315, evaluation 0.712970890410959, avg_loss 0.13836453891659187\n",
            "epoch 316, evaluation 0.712542808219178, avg_loss 0.12642927317548605\n",
            "epoch 317, evaluation 0.7181078767123288, avg_loss 0.12232798459166187\n",
            "epoch 318, evaluation 0.7116866438356164, avg_loss 0.12965045264464314\n",
            "epoch 319, evaluation 0.717679794520548, avg_loss 0.12759873081566925\n",
            "epoch 320, evaluation 0.7133989726027398, avg_loss 0.12807756435062925\n",
            "epoch 321, evaluation 0.7108304794520548, avg_loss 0.12376246764877084\n",
            "epoch 322, evaluation 0.711472602739726, avg_loss 0.1290524951797926\n",
            "epoch 323, evaluation 0.7146832191780822, avg_loss 0.12594570184789472\n",
            "epoch 324, evaluation 0.7172517123287672, avg_loss 0.13524064000506522\n",
            "epoch 325, evaluation 0.7121147260273972, avg_loss 0.13027404604826942\n",
            "epoch 326, evaluation 0.7011986301369864, avg_loss 0.12775590839653703\n",
            "epoch 327, evaluation 0.7080479452054794, avg_loss 0.11866710588525413\n",
            "epoch 328, evaluation 0.7196061643835616, avg_loss 0.12827638171739497\n",
            "epoch 329, evaluation 0.7155393835616438, avg_loss 0.12926504643539252\n",
            "epoch 330, evaluation 0.724529109589041, avg_loss 0.10985015747531995\n",
            "epoch 331, evaluation 0.6800085616438356, avg_loss 0.11979788663655014\n",
            "epoch 332, evaluation 0.7037671232876712, avg_loss 0.1269823665841151\n",
            "epoch 333, evaluation 0.6977739726027398, avg_loss 0.12102087843468634\n",
            "epoch 334, evaluation 0.6992722602739726, avg_loss 0.12521616431363558\n",
            "epoch 335, evaluation 0.7071917808219178, avg_loss 0.12790209394642862\n",
            "epoch 336, evaluation 0.7035530821917808, avg_loss 0.12852425952836619\n",
            "epoch 337, evaluation 0.708904109589041, avg_loss 0.1223161939089581\n",
            "epoch 338, evaluation 0.7133989726027398, avg_loss 0.12828652313200095\n",
            "epoch 339, evaluation 0.717679794520548, avg_loss 0.12893203469782563\n",
            "epoch 340, evaluation 0.7123287671232876, avg_loss 0.11603132345540039\n",
            "epoch 341, evaluation 0.7065496575342466, avg_loss 0.11312366971525095\n",
            "epoch 342, evaluation 0.7112585616438356, avg_loss 0.12096541660948325\n",
            "epoch 343, evaluation 0.7116866438356164, avg_loss 0.12976699481576176\n",
            "epoch 344, evaluation 0.7061215753424658, avg_loss 0.1202095320037866\n",
            "epoch 345, evaluation 0.71875, avg_loss 0.11379691030893285\n",
            "epoch 346, evaluation 0.7123287671232876, avg_loss 0.11776947297029576\n",
            "epoch 347, evaluation 0.7183219178082192, avg_loss 0.12240783019086061\n",
            "epoch 348, evaluation 0.7065496575342466, avg_loss 0.11414266349274223\n",
            "epoch 349, evaluation 0.7202482876712328, avg_loss 0.11384959876916166\n",
            "epoch 350, evaluation 0.7037671232876712, avg_loss 0.11034754546580174\n",
            "epoch 351, evaluation 0.7116866438356164, avg_loss 0.13113348900766697\n",
            "epoch 352, evaluation 0.7204623287671232, avg_loss 0.11374817379450394\n",
            "epoch 353, evaluation 0.7153253424657534, avg_loss 0.11193342747766588\n",
            "epoch 354, evaluation 0.712970890410959, avg_loss 0.12080310596374132\n",
            "epoch 355, evaluation 0.7189640410958904, avg_loss 0.11803465393071963\n",
            "epoch 356, evaluation 0.717679794520548, avg_loss 0.10664673857706583\n",
            "epoch 357, evaluation 0.7116866438356164, avg_loss 0.1098940045651743\n",
            "epoch 358, evaluation 0.7037671232876712, avg_loss 0.12544090932834956\n",
            "epoch 359, evaluation 0.7146832191780822, avg_loss 0.11759481069205677\n",
            "epoch 360, evaluation 0.7009845890410958, avg_loss 0.11029831341515153\n",
            "epoch 361, evaluation 0.7166095890410958, avg_loss 0.12008388743941056\n",
            "epoch 362, evaluation 0.7155393835616438, avg_loss 0.11597129724667234\n",
            "epoch 363, evaluation 0.7217465753424658, avg_loss 0.11054017386888548\n",
            "epoch 364, evaluation 0.7163955479452054, avg_loss 0.10696543617079318\n",
            "epoch 365, evaluation 0.7097602739726028, avg_loss 0.1026764647466904\n",
            "epoch 366, evaluation 0.709332191780822, avg_loss 0.11944697651128143\n",
            "epoch 367, evaluation 0.690068493150685, avg_loss 0.1091400661174271\n",
            "epoch 368, evaluation 0.7174657534246576, avg_loss 0.10707474751871521\n",
            "epoch 369, evaluation 0.7221746575342466, avg_loss 0.10961683195526317\n",
            "epoch 370, evaluation 0.695847602739726, avg_loss 0.12460364725567022\n",
            "epoch 371, evaluation 0.7146832191780822, avg_loss 0.1085060373990465\n",
            "epoch 372, evaluation 0.7155393835616438, avg_loss 0.10728828284763178\n",
            "epoch 373, evaluation 0.7099743150684932, avg_loss 0.11732058953178132\n",
            "epoch 374, evaluation 0.706763698630137, avg_loss 0.11620212903214713\n",
            "epoch 375, evaluation 0.6930650684931506, avg_loss 0.1224238833033685\n",
            "epoch 376, evaluation 0.7146832191780822, avg_loss 0.10316468797371549\n",
            "epoch 377, evaluation 0.7208904109589042, avg_loss 0.11194317284971475\n",
            "epoch 378, evaluation 0.7110445205479452, avg_loss 0.11350878676936282\n",
            "epoch 379, evaluation 0.7127568493150684, avg_loss 0.11246670348159338\n",
            "epoch 380, evaluation 0.6866438356164384, avg_loss 0.1156641338494117\n",
            "epoch 381, evaluation 0.711472602739726, avg_loss 0.10696860245760467\n",
            "epoch 382, evaluation 0.7168236301369864, avg_loss 0.1089673127473916\n",
            "epoch 383, evaluation 0.7166095890410958, avg_loss 0.0970522782654833\n",
            "epoch 384, evaluation 0.7155393835616438, avg_loss 0.1026423888186277\n",
            "epoch 385, evaluation 0.71875, avg_loss 0.11191374029395944\n",
            "epoch 386, evaluation 0.6988441780821918, avg_loss 0.10776670114216158\n",
            "epoch 387, evaluation 0.704195205479452, avg_loss 0.10258232971053508\n",
            "epoch 388, evaluation 0.7080479452054794, avg_loss 0.10560953895229909\n",
            "epoch 389, evaluation 0.7138270547945206, avg_loss 0.10431790495828047\n",
            "epoch 390, evaluation 0.7052654109589042, avg_loss 0.10398380476034294\n",
            "epoch 391, evaluation 0.7101883561643836, avg_loss 0.11659943268523883\n",
            "epoch 392, evaluation 0.714041095890411, avg_loss 0.10370246491442292\n",
            "epoch 393, evaluation 0.719820205479452, avg_loss 0.1057336201765022\n",
            "epoch 394, evaluation 0.7065496575342466, avg_loss 0.11441290348894516\n",
            "epoch 395, evaluation 0.7163955479452054, avg_loss 0.11550059268661475\n",
            "epoch 396, evaluation 0.6999143835616438, avg_loss 0.10127478639074301\n",
            "epoch 397, evaluation 0.7127568493150684, avg_loss 0.0997746666767082\n",
            "epoch 398, evaluation 0.693707191780822, avg_loss 0.10316966504047988\n",
            "epoch 399, evaluation 0.7014126712328768, avg_loss 0.10054761597855111\n",
            "epoch 400, evaluation 0.704195205479452, avg_loss 0.11825193602402331\n",
            "epoch 401, evaluation 0.7155393835616438, avg_loss 0.12147482100369061\n",
            "epoch 402, evaluation 0.7168236301369864, avg_loss 0.10200942360590827\n",
            "epoch 403, evaluation 0.7099743150684932, avg_loss 0.09991326742505623\n",
            "epoch 404, evaluation 0.7106164383561644, avg_loss 0.109592738532919\n",
            "epoch 405, evaluation 0.706763698630137, avg_loss 0.11325125900744382\n",
            "epoch 406, evaluation 0.7063356164383562, avg_loss 0.11033078727580733\n",
            "epoch 407, evaluation 0.7086900684931506, avg_loss 0.09705580065689855\n",
            "epoch 408, evaluation 0.6988441780821918, avg_loss 0.09489721439747234\n",
            "epoch 409, evaluation 0.707833904109589, avg_loss 0.09957758339784913\n",
            "epoch 410, evaluation 0.6949914383561644, avg_loss 0.11184303481131792\n",
            "epoch 411, evaluation 0.7054794520547946, avg_loss 0.1023990380940801\n",
            "epoch 412, evaluation 0.705693493150685, avg_loss 0.09773155059349739\n",
            "epoch 413, evaluation 0.699486301369863, avg_loss 0.09783682967457226\n",
            "epoch 414, evaluation 0.6979880136986302, avg_loss 0.10185399996274609\n",
            "epoch 415, evaluation 0.716181506849315, avg_loss 0.09476373938812038\n",
            "epoch 416, evaluation 0.7071917808219178, avg_loss 0.0962496178982369\n",
            "epoch 417, evaluation 0.7095462328767124, avg_loss 0.0989104438194279\n",
            "epoch 418, evaluation 0.7026969178082192, avg_loss 0.09836627754808988\n",
            "epoch 419, evaluation 0.7074058219178082, avg_loss 0.09504058593410557\n",
            "epoch 420, evaluation 0.7046232876712328, avg_loss 0.10724400979924505\n",
            "epoch 421, evaluation 0.7101883561643836, avg_loss 0.09883519294908491\n",
            "epoch 422, evaluation 0.7202482876712328, avg_loss 0.08822943394593263\n",
            "epoch 423, evaluation 0.723458904109589, avg_loss 0.09963856230221563\n",
            "epoch 424, evaluation 0.7189640410958904, avg_loss 0.10273050869957118\n",
            "epoch 425, evaluation 0.7178938356164384, avg_loss 0.09180770696503884\n",
            "epoch 426, evaluation 0.7054794520547946, avg_loss 0.09520509039067616\n",
            "epoch 427, evaluation 0.7071917808219178, avg_loss 0.099763095779818\n",
            "epoch 428, evaluation 0.7052654109589042, avg_loss 0.10237151439601587\n",
            "epoch 429, evaluation 0.7183219178082192, avg_loss 0.09274624496445817\n",
            "epoch 430, evaluation 0.7146832191780822, avg_loss 0.08837180795716279\n",
            "epoch 431, evaluation 0.7159674657534246, avg_loss 0.10341426716265031\n",
            "epoch 432, evaluation 0.7099743150684932, avg_loss 0.10033006671247846\n",
            "epoch 433, evaluation 0.7069777397260274, avg_loss 0.09925504202433562\n",
            "epoch 434, evaluation 0.696917808219178, avg_loss 0.09348584788109539\n",
            "epoch 435, evaluation 0.6988441780821918, avg_loss 0.10417083286506645\n",
            "epoch 436, evaluation 0.7050513698630136, avg_loss 0.09619957719598028\n",
            "epoch 437, evaluation 0.7123287671232876, avg_loss 0.09629407778571723\n",
            "epoch 438, evaluation 0.716181506849315, avg_loss 0.0899283631752103\n",
            "epoch 439, evaluation 0.7108304794520548, avg_loss 0.08894616248307087\n",
            "epoch 440, evaluation 0.7148972602739726, avg_loss 0.09543807870882043\n",
            "epoch 441, evaluation 0.7095462328767124, avg_loss 0.09674404617083275\n",
            "epoch 442, evaluation 0.7144691780821918, avg_loss 0.09083796429242623\n",
            "epoch 443, evaluation 0.7116866438356164, avg_loss 0.10082704082952212\n",
            "epoch 444, evaluation 0.7026969178082192, avg_loss 0.09412079813053548\n",
            "epoch 445, evaluation 0.7136130136986302, avg_loss 0.09156132452821328\n",
            "epoch 446, evaluation 0.7116866438356164, avg_loss 0.0956358102355468\n",
            "epoch 447, evaluation 0.7024828767123288, avg_loss 0.09869151292842324\n",
            "epoch 448, evaluation 0.7127568493150684, avg_loss 0.09589658118468725\n",
            "epoch 449, evaluation 0.7065496575342466, avg_loss 0.10671323514306696\n",
            "epoch 450, evaluation 0.7155393835616438, avg_loss 0.0935714636282143\n",
            "epoch 451, evaluation 0.712970890410959, avg_loss 0.09364819497154173\n",
            "epoch 452, evaluation 0.707833904109589, avg_loss 0.09546575157476936\n",
            "epoch 453, evaluation 0.7007705479452054, avg_loss 0.08554194787106777\n",
            "epoch 454, evaluation 0.7133989726027398, avg_loss 0.0915164316199341\n",
            "epoch 455, evaluation 0.7110445205479452, avg_loss 0.09255176699893959\n",
            "epoch 456, evaluation 0.7119006849315068, avg_loss 0.09091167933291804\n",
            "epoch 457, evaluation 0.7033390410958904, avg_loss 0.09113235890675904\n",
            "epoch 458, evaluation 0.7091181506849316, avg_loss 0.09870995171999528\n",
            "epoch 459, evaluation 0.7044092465753424, avg_loss 0.09442126992142806\n",
            "epoch 460, evaluation 0.716181506849315, avg_loss 0.09147293554713666\n",
            "epoch 461, evaluation 0.7159674657534246, avg_loss 0.09410543052511194\n",
            "epoch 462, evaluation 0.7052654109589042, avg_loss 0.09066259817425477\n",
            "epoch 463, evaluation 0.7101883561643836, avg_loss 0.08967390069264476\n",
            "epoch 464, evaluation 0.7065496575342466, avg_loss 0.09315219505105989\n",
            "epoch 465, evaluation 0.7050513698630136, avg_loss 0.09727655743580248\n",
            "epoch 466, evaluation 0.7063356164383562, avg_loss 0.09361666172458712\n",
            "epoch 467, evaluation 0.7095462328767124, avg_loss 0.08791823111853357\n",
            "epoch 468, evaluation 0.7054794520547946, avg_loss 0.09873970823709742\n",
            "epoch 469, evaluation 0.707833904109589, avg_loss 0.09441032622135796\n",
            "epoch 470, evaluation 0.7091181506849316, avg_loss 0.08588256591583712\n",
            "epoch 471, evaluation 0.7091181506849316, avg_loss 0.08393181615146035\n",
            "epoch 472, evaluation 0.7065496575342466, avg_loss 0.09104945802650714\n",
            "epoch 473, evaluation 0.7052654109589042, avg_loss 0.08281318674621693\n",
            "epoch 474, evaluation 0.7084760273972602, avg_loss 0.08102523587580959\n",
            "epoch 475, evaluation 0.7095462328767124, avg_loss 0.08146123319580141\n",
            "epoch 476, evaluation 0.6870719178082192, avg_loss 0.09232658401952457\n",
            "epoch 477, evaluation 0.7024828767123288, avg_loss 0.08671232021050686\n",
            "epoch 478, evaluation 0.6979880136986302, avg_loss 0.09789808141067624\n",
            "epoch 479, evaluation 0.705693493150685, avg_loss 0.09150308668613434\n",
            "epoch 480, evaluation 0.719820205479452, avg_loss 0.092504266447435\n",
            "epoch 481, evaluation 0.7039811643835616, avg_loss 0.08742532050552762\n",
            "epoch 482, evaluation 0.7144691780821918, avg_loss 0.08802645415762218\n",
            "epoch 483, evaluation 0.679152397260274, avg_loss 0.0837987672505995\n",
            "epoch 484, evaluation 0.7065496575342466, avg_loss 0.09762643857275026\n",
            "epoch 485, evaluation 0.7061215753424658, avg_loss 0.08908766151042813\n",
            "epoch 486, evaluation 0.7119006849315068, avg_loss 0.09325490138982817\n",
            "epoch 487, evaluation 0.7110445205479452, avg_loss 0.08961245222806426\n",
            "epoch 488, evaluation 0.711472602739726, avg_loss 0.08207570146453583\n",
            "epoch 489, evaluation 0.7123287671232876, avg_loss 0.07617965408648222\n",
            "epoch 490, evaluation 0.7181078767123288, avg_loss 0.07919671775425895\n",
            "epoch 491, evaluation 0.7095462328767124, avg_loss 0.07681777890045512\n",
            "epoch 492, evaluation 0.7071917808219178, avg_loss 0.08218289418303866\n",
            "epoch 493, evaluation 0.7061215753424658, avg_loss 0.09005821318981254\n",
            "epoch 494, evaluation 0.7007705479452054, avg_loss 0.09266797851423843\n",
            "epoch 495, evaluation 0.7050513698630136, avg_loss 0.0938671915460441\n",
            "epoch 496, evaluation 0.716181506849315, avg_loss 0.08945985498001514\n",
            "epoch 497, evaluation 0.7069777397260274, avg_loss 0.08447978186847295\n",
            "epoch 498, evaluation 0.7123287671232876, avg_loss 0.08678600089624525\n",
            "epoch 499, evaluation 0.7119006849315068, avg_loss 0.08473912802982633\n",
            "epoch 500, evaluation 0.7039811643835616, avg_loss 0.08594918073454903\n",
            "epoch 501, evaluation 0.7099743150684932, avg_loss 0.08569038483803555\n",
            "epoch 502, evaluation 0.7170376712328768, avg_loss 0.0865716485843315\n",
            "epoch 503, evaluation 0.707833904109589, avg_loss 0.09064778728821014\n",
            "epoch 504, evaluation 0.6999143835616438, avg_loss 0.08830574169376139\n",
            "epoch 505, evaluation 0.712970890410959, avg_loss 0.08756230603903532\n",
            "epoch 506, evaluation 0.712542808219178, avg_loss 0.08240476334941084\n",
            "epoch 507, evaluation 0.706763698630137, avg_loss 0.08576565856208741\n",
            "epoch 508, evaluation 0.7039811643835616, avg_loss 0.08589451442772554\n",
            "epoch 509, evaluation 0.7163955479452054, avg_loss 0.08428131487946641\n",
            "epoch 510, evaluation 0.717679794520548, avg_loss 0.07437357240498571\n",
            "epoch 511, evaluation 0.7181078767123288, avg_loss 0.07707308803732364\n",
            "epoch 512, evaluation 0.7123287671232876, avg_loss 0.08842371782066963\n",
            "epoch 513, evaluation 0.7127568493150684, avg_loss 0.08123955570666466\n",
            "epoch 514, evaluation 0.7082619863013698, avg_loss 0.07698026762296588\n",
            "epoch 515, evaluation 0.7016267123287672, avg_loss 0.08526809189815895\n",
            "epoch 516, evaluation 0.7148972602739726, avg_loss 0.08409388111115007\n",
            "epoch 517, evaluation 0.7099743150684932, avg_loss 0.08801488662927838\n",
            "epoch 518, evaluation 0.7123287671232876, avg_loss 0.07994354692429809\n",
            "epoch 519, evaluation 0.7136130136986302, avg_loss 0.07911892061011266\n",
            "epoch 520, evaluation 0.705693493150685, avg_loss 0.08326564781896581\n",
            "epoch 521, evaluation 0.704195205479452, avg_loss 0.0890770908950244\n",
            "epoch 522, evaluation 0.708904109589041, avg_loss 0.08254172211291931\n",
            "epoch 523, evaluation 0.6892123287671232, avg_loss 0.07352413931956231\n",
            "epoch 524, evaluation 0.6845034246575342, avg_loss 0.07745157530199799\n",
            "epoch 525, evaluation 0.7003424657534246, avg_loss 0.09894819201869985\n",
            "epoch 526, evaluation 0.7065496575342466, avg_loss 0.08199844363982142\n",
            "epoch 527, evaluation 0.7108304794520548, avg_loss 0.08138469042477467\n",
            "epoch 528, evaluation 0.7142551369863014, avg_loss 0.0843033018332543\n",
            "epoch 529, evaluation 0.708904109589041, avg_loss 0.07533809275966201\n",
            "epoch 530, evaluation 0.6943493150684932, avg_loss 0.07472156379371882\n",
            "epoch 531, evaluation 0.7033390410958904, avg_loss 0.08213559509081356\n",
            "epoch 532, evaluation 0.7157534246575342, avg_loss 0.08408242608657328\n",
            "epoch 533, evaluation 0.7157534246575342, avg_loss 0.0786683746124223\n",
            "epoch 534, evaluation 0.7116866438356164, avg_loss 0.0744699360163472\n",
            "epoch 535, evaluation 0.6924229452054794, avg_loss 0.08104164707079782\n",
            "epoch 536, evaluation 0.6774400684931506, avg_loss 0.0786768589168787\n",
            "epoch 537, evaluation 0.7086900684931506, avg_loss 0.08713234380613696\n",
            "epoch 538, evaluation 0.7007705479452054, avg_loss 0.07748700883870913\n",
            "epoch 539, evaluation 0.7193921232876712, avg_loss 0.07711700917363672\n",
            "epoch 540, evaluation 0.6949914383561644, avg_loss 0.07683786330428921\n",
            "epoch 541, evaluation 0.7106164383561644, avg_loss 0.07748547938604981\n",
            "epoch 542, evaluation 0.7106164383561644, avg_loss 0.07725076509355489\n",
            "epoch 543, evaluation 0.7148972602739726, avg_loss 0.08240341260808251\n",
            "epoch 544, evaluation 0.693707191780822, avg_loss 0.09056725296018235\n",
            "epoch 545, evaluation 0.7037671232876712, avg_loss 0.07549659827787239\n",
            "epoch 546, evaluation 0.7071917808219178, avg_loss 0.07839496331605113\n",
            "epoch 547, evaluation 0.705693493150685, avg_loss 0.07363830230088304\n",
            "epoch 548, evaluation 0.7136130136986302, avg_loss 0.08391040407626306\n",
            "epoch 549, evaluation 0.7138270547945206, avg_loss 0.07885736252133876\n",
            "epoch 550, evaluation 0.7133989726027398, avg_loss 0.0696589944631619\n",
            "epoch 551, evaluation 0.7080479452054794, avg_loss 0.06604377777647163\n",
            "epoch 552, evaluation 0.6962756849315068, avg_loss 0.07945896313225818\n",
            "epoch 553, evaluation 0.7146832191780822, avg_loss 0.07332994680914838\n",
            "epoch 554, evaluation 0.7099743150684932, avg_loss 0.06493922769076238\n",
            "epoch 555, evaluation 0.700556506849315, avg_loss 0.08228823565009792\n",
            "epoch 556, evaluation 0.7086900684931506, avg_loss 0.07759205172532949\n",
            "epoch 557, evaluation 0.7063356164383562, avg_loss 0.0725164072572301\n",
            "epoch 558, evaluation 0.711472602739726, avg_loss 0.07397568362559807\n",
            "epoch 559, evaluation 0.7071917808219178, avg_loss 0.07466992856461113\n",
            "epoch 560, evaluation 0.7091181506849316, avg_loss 0.07028355365934766\n",
            "epoch 561, evaluation 0.7086900684931506, avg_loss 0.08327727992624297\n",
            "epoch 562, evaluation 0.712970890410959, avg_loss 0.07659709083263652\n",
            "epoch 563, evaluation 0.7178938356164384, avg_loss 0.07346162551272092\n",
            "epoch 564, evaluation 0.6960616438356164, avg_loss 0.07800354126335705\n",
            "epoch 565, evaluation 0.7084760273972602, avg_loss 0.07465970134993984\n",
            "epoch 566, evaluation 0.704195205479452, avg_loss 0.0766307909515197\n",
            "epoch 567, evaluation 0.7097602739726028, avg_loss 0.07474711400977636\n",
            "epoch 568, evaluation 0.712970890410959, avg_loss 0.07417765611908951\n",
            "epoch 569, evaluation 0.7181078767123288, avg_loss 0.07527413725758256\n",
            "epoch 570, evaluation 0.7063356164383562, avg_loss 0.07275650276944547\n",
            "epoch 571, evaluation 0.7054794520547946, avg_loss 0.0668265712021266\n",
            "epoch 572, evaluation 0.706763698630137, avg_loss 0.07366328831856786\n",
            "epoch 573, evaluation 0.7084760273972602, avg_loss 0.07791297280119132\n",
            "epoch 574, evaluation 0.7172517123287672, avg_loss 0.07412030582742418\n",
            "epoch 575, evaluation 0.7084760273972602, avg_loss 0.07623398659150984\n",
            "epoch 576, evaluation 0.7112585616438356, avg_loss 0.0769619775691338\n",
            "epoch 577, evaluation 0.7119006849315068, avg_loss 0.06802345483358634\n",
            "epoch 578, evaluation 0.7110445205479452, avg_loss 0.07644718779661393\n",
            "epoch 579, evaluation 0.715111301369863, avg_loss 0.0749242118717629\n",
            "epoch 580, evaluation 0.6982020547945206, avg_loss 0.06756794878234297\n",
            "epoch 581, evaluation 0.7082619863013698, avg_loss 0.07387067046711\n",
            "epoch 582, evaluation 0.7108304794520548, avg_loss 0.06831347232179369\n",
            "epoch 583, evaluation 0.7044092465753424, avg_loss 0.06682890023625755\n",
            "epoch 584, evaluation 0.7039811643835616, avg_loss 0.06958046618501766\n",
            "epoch 585, evaluation 0.7059075342465754, avg_loss 0.076899367007348\n",
            "epoch 586, evaluation 0.7039811643835616, avg_loss 0.0805428380320259\n",
            "epoch 587, evaluation 0.6735873287671232, avg_loss 0.07469953232113336\n",
            "epoch 588, evaluation 0.707833904109589, avg_loss 0.07210975883970573\n",
            "epoch 589, evaluation 0.7200342465753424, avg_loss 0.06765610047189866\n",
            "epoch 590, evaluation 0.7119006849315068, avg_loss 0.06730605898567049\n",
            "epoch 591, evaluation 0.7112585616438356, avg_loss 0.06437665866056488\n",
            "epoch 592, evaluation 0.711472602739726, avg_loss 0.07156322174468788\n",
            "epoch 593, evaluation 0.697345890410959, avg_loss 0.07212180294791015\n",
            "epoch 594, evaluation 0.708904109589041, avg_loss 0.07583463198773689\n",
            "epoch 595, evaluation 0.7142551369863014, avg_loss 0.07249750359047015\n",
            "epoch 596, evaluation 0.7018407534246576, avg_loss 0.06891135279783758\n",
            "epoch 597, evaluation 0.7116866438356164, avg_loss 0.067421970977369\n",
            "epoch 598, evaluation 0.7127568493150684, avg_loss 0.06932950600571298\n",
            "epoch 599, evaluation 0.709332191780822, avg_loss 0.07252599090827093\n",
            "epoch 600, evaluation 0.6720890410958904, avg_loss 0.07243686409635564\n",
            "epoch 601, evaluation 0.7048373287671232, avg_loss 0.06915595077956885\n",
            "epoch 602, evaluation 0.7084760273972602, avg_loss 0.0803019571275908\n",
            "epoch 603, evaluation 0.6975599315068494, avg_loss 0.07418153560641458\n",
            "epoch 604, evaluation 0.712542808219178, avg_loss 0.06240872247460282\n",
            "epoch 605, evaluation 0.7133989726027398, avg_loss 0.07302903708986054\n",
            "epoch 606, evaluation 0.694777397260274, avg_loss 0.0639138582162559\n",
            "epoch 607, evaluation 0.6982020547945206, avg_loss 0.06065189934540856\n",
            "epoch 608, evaluation 0.7123287671232876, avg_loss 0.06563646320600884\n",
            "epoch 609, evaluation 0.7082619863013698, avg_loss 0.06233356422192212\n",
            "epoch 610, evaluation 0.6988441780821918, avg_loss 0.06630757457134709\n",
            "epoch 611, evaluation 0.686429794520548, avg_loss 0.08180487094589083\n",
            "epoch 612, evaluation 0.700556506849315, avg_loss 0.06792933424555901\n",
            "epoch 613, evaluation 0.7022688356164384, avg_loss 0.06633542396712228\n",
            "epoch 614, evaluation 0.7044092465753424, avg_loss 0.07007745784881003\n",
            "epoch 615, evaluation 0.7054794520547946, avg_loss 0.06785783719003075\n",
            "epoch 616, evaluation 0.7065496575342466, avg_loss 0.06876911987645268\n",
            "epoch 617, evaluation 0.7082619863013698, avg_loss 0.07363576241058566\n",
            "epoch 618, evaluation 0.699486301369863, avg_loss 0.06945018409514579\n",
            "epoch 619, evaluation 0.6943493150684932, avg_loss 0.06518930347115433\n",
            "epoch 620, evaluation 0.7074058219178082, avg_loss 0.0640721851210031\n",
            "epoch 621, evaluation 0.705693493150685, avg_loss 0.06330608741017216\n",
            "epoch 622, evaluation 0.6971318493150684, avg_loss 0.06466878460151917\n",
            "epoch 623, evaluation 0.7076198630136986, avg_loss 0.0625994478857492\n",
            "epoch 624, evaluation 0.7001284246575342, avg_loss 0.07323065233432641\n",
            "epoch 625, evaluation 0.6986301369863014, avg_loss 0.0687261824322454\n",
            "epoch 626, evaluation 0.7044092465753424, avg_loss 0.06576471895184684\n",
            "epoch 627, evaluation 0.7026969178082192, avg_loss 0.07416981046277461\n",
            "epoch 628, evaluation 0.7155393835616438, avg_loss 0.06821774115501825\n",
            "epoch 629, evaluation 0.706763698630137, avg_loss 0.05497264738699768\n",
            "epoch 630, evaluation 0.7052654109589042, avg_loss 0.05976789320219245\n",
            "epoch 631, evaluation 0.7050513698630136, avg_loss 0.0732845537617045\n",
            "epoch 632, evaluation 0.7009845890410958, avg_loss 0.06333473049325206\n",
            "epoch 633, evaluation 0.7084760273972602, avg_loss 0.06683447602583821\n",
            "epoch 634, evaluation 0.7142551369863014, avg_loss 0.06555579580461322\n",
            "epoch 635, evaluation 0.7095462328767124, avg_loss 0.06517377122842027\n",
            "epoch 636, evaluation 0.7069777397260274, avg_loss 0.0728095650467706\n",
            "epoch 637, evaluation 0.7189640410958904, avg_loss 0.06507729338324171\n",
            "epoch 638, evaluation 0.7059075342465754, avg_loss 0.058959779884839844\n",
            "epoch 639, evaluation 0.7024828767123288, avg_loss 0.06875086353491928\n",
            "epoch 640, evaluation 0.7131849315068494, avg_loss 0.059581063924578286\n",
            "epoch 641, evaluation 0.7026969178082192, avg_loss 0.07077819884959924\n",
            "epoch 642, evaluation 0.7069777397260274, avg_loss 0.06331184884785848\n",
            "epoch 643, evaluation 0.7035530821917808, avg_loss 0.06724970433861018\n",
            "epoch 644, evaluation 0.7033390410958904, avg_loss 0.05761080119695704\n",
            "epoch 645, evaluation 0.704195205479452, avg_loss 0.06718801392747437\n",
            "epoch 646, evaluation 0.7084760273972602, avg_loss 0.059518892784460875\n",
            "epoch 647, evaluation 0.7069777397260274, avg_loss 0.06613694690878234\n",
            "epoch 648, evaluation 0.7074058219178082, avg_loss 0.06509429648147612\n",
            "epoch 649, evaluation 0.7138270547945206, avg_loss 0.06443603469445607\n",
            "epoch 650, evaluation 0.7061215753424658, avg_loss 0.07581576459520196\n",
            "epoch 651, evaluation 0.693279109589041, avg_loss 0.06389512380395652\n",
            "epoch 652, evaluation 0.7009845890410958, avg_loss 0.05967538579509167\n",
            "epoch 653, evaluation 0.705693493150685, avg_loss 0.06222176411804759\n",
            "epoch 654, evaluation 0.7108304794520548, avg_loss 0.05864812301045631\n",
            "epoch 655, evaluation 0.7110445205479452, avg_loss 0.06340929175711284\n",
            "epoch 656, evaluation 0.7074058219178082, avg_loss 0.06723401525168349\n",
            "epoch 657, evaluation 0.7189640410958904, avg_loss 0.05537000152566549\n",
            "epoch 658, evaluation 0.7026969178082192, avg_loss 0.06434349265675676\n",
            "epoch 659, evaluation 0.7101883561643836, avg_loss 0.06446641410653621\n",
            "epoch 660, evaluation 0.7157534246575342, avg_loss 0.07672941684407197\n",
            "epoch 661, evaluation 0.7108304794520548, avg_loss 0.06491381293162704\n",
            "epoch 662, evaluation 0.707833904109589, avg_loss 0.0587300667791801\n",
            "epoch 663, evaluation 0.7121147260273972, avg_loss 0.07381575249009213\n",
            "epoch 664, evaluation 0.712970890410959, avg_loss 0.05793164703142593\n",
            "epoch 665, evaluation 0.7097602739726028, avg_loss 0.06146664269015951\n",
            "epoch 666, evaluation 0.721318493150685, avg_loss 0.05570830302391896\n",
            "epoch 667, evaluation 0.7163955479452054, avg_loss 0.053239507794032914\n",
            "epoch 668, evaluation 0.715111301369863, avg_loss 0.057094357975648116\n",
            "epoch 669, evaluation 0.7074058219178082, avg_loss 0.06462151625398863\n",
            "epoch 670, evaluation 0.7097602739726028, avg_loss 0.061282266180772905\n",
            "epoch 671, evaluation 0.6971318493150684, avg_loss 0.0583090965194091\n",
            "epoch 672, evaluation 0.7112585616438356, avg_loss 0.0652292350125578\n",
            "epoch 673, evaluation 0.7172517123287672, avg_loss 0.056814855012758556\n",
            "epoch 674, evaluation 0.7121147260273972, avg_loss 0.05614139858033445\n",
            "epoch 675, evaluation 0.7001284246575342, avg_loss 0.0593859168898188\n",
            "epoch 676, evaluation 0.6999143835616438, avg_loss 0.0626690592427375\n",
            "epoch 677, evaluation 0.6926369863013698, avg_loss 0.060179882393202794\n",
            "epoch 678, evaluation 0.707833904109589, avg_loss 0.055446706105307754\n",
            "epoch 679, evaluation 0.7086900684931506, avg_loss 0.06258868593091177\n",
            "epoch 680, evaluation 0.7123287671232876, avg_loss 0.0659235408135011\n",
            "epoch 681, evaluation 0.7136130136986302, avg_loss 0.06186076923652347\n",
            "epoch 682, evaluation 0.7136130136986302, avg_loss 0.06766753534518057\n",
            "epoch 683, evaluation 0.7127568493150684, avg_loss 0.06269410460382321\n",
            "epoch 684, evaluation 0.6924229452054794, avg_loss 0.06581658554752752\n",
            "epoch 685, evaluation 0.7037671232876712, avg_loss 0.057974672647414065\n",
            "epoch 686, evaluation 0.7127568493150684, avg_loss 0.052917890451153964\n",
            "epoch 687, evaluation 0.7050513698630136, avg_loss 0.05748731935381005\n",
            "epoch 688, evaluation 0.6988441780821918, avg_loss 0.05992612303944968\n",
            "epoch 689, evaluation 0.714041095890411, avg_loss 0.0540073700051881\n",
            "epoch 690, evaluation 0.7071917808219178, avg_loss 0.06233053089956106\n",
            "epoch 691, evaluation 0.7046232876712328, avg_loss 0.05876852054212053\n",
            "epoch 692, evaluation 0.7076198630136986, avg_loss 0.06312461778426827\n",
            "epoch 693, evaluation 0.6979880136986302, avg_loss 0.06683270832784158\n",
            "epoch 694, evaluation 0.709332191780822, avg_loss 0.06412329813307625\n",
            "epoch 695, evaluation 0.7157534246575342, avg_loss 0.05166414023171795\n",
            "epoch 696, evaluation 0.709332191780822, avg_loss 0.05166859899892203\n",
            "epoch 697, evaluation 0.712970890410959, avg_loss 0.06208253660223494\n",
            "epoch 698, evaluation 0.7121147260273972, avg_loss 0.06451601518349627\n",
            "epoch 699, evaluation 0.7086900684931506, avg_loss 0.05221161217452421\n",
            "epoch 700, evaluation 0.7166095890410958, avg_loss 0.05539178623486373\n",
            "epoch 701, evaluation 0.7142551369863014, avg_loss 0.05893577866224667\n",
            "epoch 702, evaluation 0.710402397260274, avg_loss 0.0569039909240111\n",
            "epoch 703, evaluation 0.706763698630137, avg_loss 0.0603235723521813\n",
            "epoch 704, evaluation 0.7016267123287672, avg_loss 0.06208817672173856\n",
            "epoch 705, evaluation 0.7046232876712328, avg_loss 0.06235320690261611\n",
            "epoch 706, evaluation 0.7007705479452054, avg_loss 0.061988011690772185\n",
            "epoch 707, evaluation 0.7007705479452054, avg_loss 0.05179697777523454\n",
            "epoch 708, evaluation 0.7106164383561644, avg_loss 0.05734116622420438\n",
            "epoch 709, evaluation 0.698416095890411, avg_loss 0.05048069062137629\n",
            "epoch 710, evaluation 0.7136130136986302, avg_loss 0.05256040236454899\n",
            "epoch 711, evaluation 0.705693493150685, avg_loss 0.05088734940025892\n",
            "epoch 712, evaluation 0.7029109589041096, avg_loss 0.05617443330870089\n",
            "epoch 713, evaluation 0.702054794520548, avg_loss 0.05734665755545563\n",
            "epoch 714, evaluation 0.7048373287671232, avg_loss 0.057565491315040546\n",
            "epoch 715, evaluation 0.7061215753424658, avg_loss 0.0625758935365889\n",
            "epoch 716, evaluation 0.697345890410959, avg_loss 0.048138775590459926\n",
            "epoch 717, evaluation 0.7026969178082192, avg_loss 0.058533313576826604\n",
            "epoch 718, evaluation 0.7071917808219178, avg_loss 0.06856396252099993\n",
            "epoch 719, evaluation 0.7166095890410958, avg_loss 0.05869160431310913\n",
            "epoch 720, evaluation 0.7069777397260274, avg_loss 0.05093175828204317\n",
            "epoch 721, evaluation 0.7123287671232876, avg_loss 0.048032718813204665\n",
            "epoch 722, evaluation 0.702054794520548, avg_loss 0.055093905233415\n",
            "epoch 723, evaluation 0.7099743150684932, avg_loss 0.057749803762062124\n",
            "epoch 724, evaluation 0.7014126712328768, avg_loss 0.04477484167032575\n",
            "epoch 725, evaluation 0.7142551369863014, avg_loss 0.05995694162418782\n",
            "epoch 726, evaluation 0.7110445205479452, avg_loss 0.05555371641007773\n",
            "epoch 727, evaluation 0.7082619863013698, avg_loss 0.05693919405151727\n",
            "epoch 728, evaluation 0.6941352739726028, avg_loss 0.054976533189952624\n",
            "epoch 729, evaluation 0.6986301369863014, avg_loss 0.060142594017088415\n",
            "epoch 730, evaluation 0.7086900684931506, avg_loss 0.05205234153653984\n",
            "epoch 731, evaluation 0.7009845890410958, avg_loss 0.054237768885095496\n",
            "epoch 732, evaluation 0.7046232876712328, avg_loss 0.05898639680344169\n",
            "epoch 733, evaluation 0.7084760273972602, avg_loss 0.05993822855338202\n",
            "epoch 734, evaluation 0.702054794520548, avg_loss 0.05174671602514336\n",
            "epoch 735, evaluation 0.7110445205479452, avg_loss 0.05500141397686833\n",
            "epoch 736, evaluation 0.7136130136986302, avg_loss 0.051684158975732024\n",
            "epoch 737, evaluation 0.6988441780821918, avg_loss 0.04729342197336383\n",
            "epoch 738, evaluation 0.7044092465753424, avg_loss 0.05110217069307248\n",
            "epoch 739, evaluation 0.7148972602739726, avg_loss 0.059385858229913956\n",
            "epoch 740, evaluation 0.706763698630137, avg_loss 0.059804272260990436\n",
            "epoch 741, evaluation 0.7153253424657534, avg_loss 0.05237640249306115\n",
            "epoch 742, evaluation 0.6977739726027398, avg_loss 0.04509437970454031\n",
            "epoch 743, evaluation 0.7084760273972602, avg_loss 0.05688834150866368\n",
            "epoch 744, evaluation 0.7071917808219178, avg_loss 0.05420534484506727\n",
            "epoch 745, evaluation 0.7059075342465754, avg_loss 0.050423274057427955\n",
            "epoch 746, evaluation 0.7059075342465754, avg_loss 0.05614626189946371\n",
            "epoch 747, evaluation 0.6979880136986302, avg_loss 0.05832975027230331\n",
            "epoch 748, evaluation 0.7054794520547946, avg_loss 0.046273671220040925\n",
            "epoch 749, evaluation 0.6992722602739726, avg_loss 0.054805937598033226\n",
            "epoch 750, evaluation 0.709332191780822, avg_loss 0.05750232985148491\n",
            "epoch 751, evaluation 0.700556506849315, avg_loss 0.0478083151816864\n",
            "epoch 752, evaluation 0.691138698630137, avg_loss 0.046121650096029046\n",
            "epoch 753, evaluation 0.7121147260273972, avg_loss 0.05024830858791405\n",
            "epoch 754, evaluation 0.7121147260273972, avg_loss 0.04732538115517315\n",
            "epoch 755, evaluation 0.7014126712328768, avg_loss 0.060331344852318704\n",
            "epoch 756, evaluation 0.6999143835616438, avg_loss 0.058587034791707994\n",
            "epoch 757, evaluation 0.7033390410958904, avg_loss 0.06025825063776919\n",
            "epoch 758, evaluation 0.7112585616438356, avg_loss 0.055016860721033005\n",
            "epoch 759, evaluation 0.7080479452054794, avg_loss 0.05331656421984593\n",
            "epoch 760, evaluation 0.7011986301369864, avg_loss 0.050771454478992874\n",
            "epoch 761, evaluation 0.7033390410958904, avg_loss 0.053919680820683304\n",
            "epoch 762, evaluation 0.7148972602739726, avg_loss 0.054644899392279525\n",
            "epoch 763, evaluation 0.7080479452054794, avg_loss 0.05010022103778576\n",
            "epoch 764, evaluation 0.7112585616438356, avg_loss 0.04607928283354741\n",
            "epoch 765, evaluation 0.710402397260274, avg_loss 0.0469238188362412\n",
            "epoch 766, evaluation 0.7054794520547946, avg_loss 0.047701640832001124\n",
            "epoch 767, evaluation 0.6990582191780822, avg_loss 0.05170528746162683\n",
            "epoch 768, evaluation 0.7026969178082192, avg_loss 0.05248655889980432\n",
            "epoch 769, evaluation 0.6913527397260274, avg_loss 0.04678920092676782\n",
            "epoch 770, evaluation 0.7039811643835616, avg_loss 0.05451677920912408\n",
            "epoch 771, evaluation 0.7063356164383562, avg_loss 0.04821410636196576\n",
            "epoch 772, evaluation 0.711472602739726, avg_loss 0.041767352478483975\n",
            "epoch 773, evaluation 0.7050513698630136, avg_loss 0.050552692846789704\n",
            "epoch 774, evaluation 0.711472602739726, avg_loss 0.055877269751597514\n",
            "epoch 775, evaluation 0.7074058219178082, avg_loss 0.054190270655583274\n",
            "epoch 776, evaluation 0.6999143835616438, avg_loss 0.048442794231824196\n",
            "epoch 777, evaluation 0.7163955479452054, avg_loss 0.04679714316873164\n",
            "epoch 778, evaluation 0.7086900684931506, avg_loss 0.049045106043281446\n",
            "epoch 779, evaluation 0.6975599315068494, avg_loss 0.05129716021711228\n",
            "epoch 780, evaluation 0.6988441780821918, avg_loss 0.06076671149723737\n",
            "epoch 781, evaluation 0.7136130136986302, avg_loss 0.046534006721813674\n",
            "epoch 782, evaluation 0.710402397260274, avg_loss 0.04821479488124723\n",
            "epoch 783, evaluation 0.7076198630136986, avg_loss 0.04727965767684756\n",
            "epoch 784, evaluation 0.7052654109589042, avg_loss 0.040665210437787286\n",
            "epoch 785, evaluation 0.704195205479452, avg_loss 0.048200761664942915\n",
            "epoch 786, evaluation 0.6988441780821918, avg_loss 0.05246396588153695\n",
            "epoch 787, evaluation 0.6913527397260274, avg_loss 0.05285033988450669\n",
            "epoch 788, evaluation 0.698416095890411, avg_loss 0.06487439443136298\n",
            "epoch 789, evaluation 0.7011986301369864, avg_loss 0.05721080804369965\n",
            "epoch 790, evaluation 0.7037671232876712, avg_loss 0.04452595330411726\n",
            "epoch 791, evaluation 0.7054794520547946, avg_loss 0.04650229887989492\n",
            "epoch 792, evaluation 0.7076198630136986, avg_loss 0.053614445856646084\n",
            "epoch 793, evaluation 0.7011986301369864, avg_loss 0.04617653943132609\n",
            "epoch 794, evaluation 0.682791095890411, avg_loss 0.0438539878433665\n",
            "epoch 795, evaluation 0.7065496575342466, avg_loss 0.05087367165518009\n",
            "epoch 796, evaluation 0.7009845890410958, avg_loss 0.044637349816197054\n",
            "epoch 797, evaluation 0.7037671232876712, avg_loss 0.047184409939494555\n",
            "epoch 798, evaluation 0.7119006849315068, avg_loss 0.043018510783785734\n",
            "epoch 799, evaluation 0.6934931506849316, avg_loss 0.04287198987213295\n",
            "epoch 800, evaluation 0.6939212328767124, avg_loss 0.05658104423023129\n",
            "epoch 801, evaluation 0.6915667808219178, avg_loss 0.05715125170141711\n",
            "epoch 802, evaluation 0.6928510273972602, avg_loss 0.06538820425916653\n",
            "epoch 803, evaluation 0.7009845890410958, avg_loss 0.053354588393279806\n",
            "epoch 804, evaluation 0.7037671232876712, avg_loss 0.043659248166733375\n",
            "epoch 805, evaluation 0.6982020547945206, avg_loss 0.03850418156470661\n",
            "epoch 806, evaluation 0.695847602739726, avg_loss 0.04209198806182308\n",
            "epoch 807, evaluation 0.7035530821917808, avg_loss 0.041094206856026994\n",
            "epoch 808, evaluation 0.694777397260274, avg_loss 0.056149343631656495\n",
            "epoch 809, evaluation 0.6971318493150684, avg_loss 0.04676817518943068\n",
            "epoch 810, evaluation 0.7086900684931506, avg_loss 0.044051208689619425\n",
            "epoch 811, evaluation 0.703125, avg_loss 0.049167232808136084\n",
            "epoch 812, evaluation 0.7050513698630136, avg_loss 0.05005711445592786\n",
            "epoch 813, evaluation 0.703125, avg_loss 0.04379100400551964\n",
            "epoch 814, evaluation 0.6913527397260274, avg_loss 0.05106775044952914\n",
            "epoch 815, evaluation 0.6868578767123288, avg_loss 0.05072668523259335\n",
            "epoch 816, evaluation 0.698416095890411, avg_loss 0.04783577779820978\n",
            "epoch 817, evaluation 0.7046232876712328, avg_loss 0.041551112908854194\n",
            "epoch 818, evaluation 0.7050513698630136, avg_loss 0.04406380911704973\n",
            "epoch 819, evaluation 0.699486301369863, avg_loss 0.05172101673038708\n",
            "epoch 820, evaluation 0.698416095890411, avg_loss 0.053226433713304794\n",
            "epoch 821, evaluation 0.7016267123287672, avg_loss 0.04958959879606205\n",
            "epoch 822, evaluation 0.7026969178082192, avg_loss 0.03765810692359267\n",
            "epoch 823, evaluation 0.678082191780822, avg_loss 0.05069538744411163\n",
            "epoch 824, evaluation 0.7026969178082192, avg_loss 0.05197316403027182\n",
            "epoch 825, evaluation 0.6924229452054794, avg_loss 0.06187993536577008\n",
            "epoch 826, evaluation 0.7001284246575342, avg_loss 0.04308710840956892\n",
            "epoch 827, evaluation 0.6979880136986302, avg_loss 0.04965203873165962\n",
            "epoch 828, evaluation 0.7001284246575342, avg_loss 0.05050522886705979\n",
            "epoch 829, evaluation 0.7024828767123288, avg_loss 0.04957595000485495\n",
            "epoch 830, evaluation 0.6915667808219178, avg_loss 0.046421245500540075\n",
            "epoch 831, evaluation 0.6956335616438356, avg_loss 0.0427821187034123\n",
            "epoch 832, evaluation 0.6979880136986302, avg_loss 0.04726524575755505\n",
            "epoch 833, evaluation 0.7037671232876712, avg_loss 0.04057259707498538\n",
            "epoch 834, evaluation 0.704195205479452, avg_loss 0.04167195227178666\n",
            "epoch 835, evaluation 0.6945633561643836, avg_loss 0.04246050491139798\n",
            "epoch 836, evaluation 0.688570205479452, avg_loss 0.0409628442883239\n",
            "epoch 837, evaluation 0.6979880136986302, avg_loss 0.050147841223594496\n",
            "epoch 838, evaluation 0.6945633561643836, avg_loss 0.04643763372658799\n",
            "epoch 839, evaluation 0.6915667808219178, avg_loss 0.052181569845178875\n",
            "epoch 840, evaluation 0.7076198630136986, avg_loss 0.03923476813084809\n",
            "epoch 841, evaluation 0.7016267123287672, avg_loss 0.04748578002218599\n",
            "epoch 842, evaluation 0.6960616438356164, avg_loss 0.04229499652421386\n",
            "epoch 843, evaluation 0.7018407534246576, avg_loss 0.04425241319636294\n",
            "epoch 844, evaluation 0.6907106164383562, avg_loss 0.057782213645592585\n",
            "epoch 845, evaluation 0.7046232876712328, avg_loss 0.04661254583487003\n",
            "epoch 846, evaluation 0.6979880136986302, avg_loss 0.04957820647514536\n",
            "epoch 847, evaluation 0.7026969178082192, avg_loss 0.04640318031138661\n",
            "epoch 848, evaluation 0.7035530821917808, avg_loss 0.041502888812052115\n",
            "epoch 849, evaluation 0.7052654109589042, avg_loss 0.04820653361749788\n",
            "epoch 850, evaluation 0.6945633561643836, avg_loss 0.04286130597317686\n",
            "epoch 851, evaluation 0.699486301369863, avg_loss 0.04602226990835457\n",
            "epoch 852, evaluation 0.6919948630136986, avg_loss 0.04565962356657295\n",
            "epoch 853, evaluation 0.699486301369863, avg_loss 0.04694441177061427\n",
            "epoch 854, evaluation 0.6967037671232876, avg_loss 0.04930658235482998\n",
            "epoch 855, evaluation 0.7065496575342466, avg_loss 0.048339199865142166\n",
            "epoch 856, evaluation 0.7001284246575342, avg_loss 0.03739497335706601\n",
            "epoch 857, evaluation 0.6967037671232876, avg_loss 0.04466833322608875\n",
            "epoch 858, evaluation 0.6977739726027398, avg_loss 0.04089526667109675\n",
            "epoch 859, evaluation 0.7014126712328768, avg_loss 0.03775522254695485\n",
            "epoch 860, evaluation 0.6977739726027398, avg_loss 0.04015086659514424\n",
            "epoch 861, evaluation 0.7046232876712328, avg_loss 0.051777538355692466\n",
            "epoch 862, evaluation 0.702054794520548, avg_loss 0.048651065159652195\n",
            "epoch 863, evaluation 0.7059075342465754, avg_loss 0.04758412704755694\n",
            "epoch 864, evaluation 0.7039811643835616, avg_loss 0.04789710669399444\n",
            "epoch 865, evaluation 0.7048373287671232, avg_loss 0.036623893381822537\n",
            "epoch 866, evaluation 0.7052654109589042, avg_loss 0.037454155947760505\n",
            "epoch 867, evaluation 0.6956335616438356, avg_loss 0.044106068038697335\n",
            "epoch 868, evaluation 0.7048373287671232, avg_loss 0.0428307301714085\n",
            "epoch 869, evaluation 0.7003424657534246, avg_loss 0.053074363683808155\n",
            "epoch 870, evaluation 0.7065496575342466, avg_loss 0.0516330940727839\n",
            "epoch 871, evaluation 0.7153253424657534, avg_loss 0.04429217048652344\n",
            "epoch 872, evaluation 0.7024828767123288, avg_loss 0.04244293837997494\n",
            "epoch 873, evaluation 0.7014126712328768, avg_loss 0.03768677383392923\n",
            "epoch 874, evaluation 0.7014126712328768, avg_loss 0.03327567810390018\n",
            "epoch 875, evaluation 0.6990582191780822, avg_loss 0.041902949371753984\n",
            "epoch 876, evaluation 0.7052654109589042, avg_loss 0.04388307273956173\n",
            "epoch 877, evaluation 0.709332191780822, avg_loss 0.04723617961034337\n",
            "epoch 878, evaluation 0.7007705479452054, avg_loss 0.04546296079958773\n",
            "epoch 879, evaluation 0.706763698630137, avg_loss 0.045872297504190675\n",
            "epoch 880, evaluation 0.7009845890410958, avg_loss 0.03968913505935126\n",
            "epoch 881, evaluation 0.7018407534246576, avg_loss 0.04025584472919483\n",
            "epoch 882, evaluation 0.704195205479452, avg_loss 0.03823518055230695\n",
            "epoch 883, evaluation 0.7044092465753424, avg_loss 0.04610378865598527\n",
            "epoch 884, evaluation 0.7033390410958904, avg_loss 0.040924816305149284\n",
            "epoch 885, evaluation 0.6919948630136986, avg_loss 0.039857678109053836\n",
            "epoch 886, evaluation 0.6954195205479452, avg_loss 0.04258499583229423\n",
            "epoch 887, evaluation 0.702054794520548, avg_loss 0.050579268117119575\n",
            "epoch 888, evaluation 0.7061215753424658, avg_loss 0.04064560205502025\n",
            "epoch 889, evaluation 0.6964897260273972, avg_loss 0.04176945834049671\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# manual_write_study_params(trainer.study_name, trainer.storage)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\OneDrive\\سطح المكتب\\zeyadcode\\eeg_detection\\modules\\trainer.py:135\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mself\u001b[39m.trial = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_training(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_save\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_print\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m evaluation = evaluate_model(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.val_loader, \u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    137\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdone training\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\OneDrive\\سطح المكتب\\zeyadcode\\eeg_detection\\modules\\trainer.py:61\u001b[39m, in \u001b[36mTrainer._train_loop\u001b[39m\u001b[34m(self, n_epochs, should_save, should_print)\u001b[39m\n\u001b[32m     58\u001b[39m     avg_loss += loss.item()\n\u001b[32m     60\u001b[39m avg_loss = avg_loss / \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.train_loader)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m evaluation = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.trial.report(evaluation, epoch)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\OneDrive\\سطح المكتب\\zeyadcode\\eeg_detection\\modules\\utils.py:28\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, data_loader, device)\u001b[39m\n\u001b[32m     25\u001b[39m x = x.to(device)\n\u001b[32m     26\u001b[39m y = y.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m _, predicted = torch.max(outputs.data, \u001b[32m1\u001b[39m)\n\u001b[32m     31\u001b[39m all_preds.extend(predicted.cpu().numpy())\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mSSVEPClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m y = y.squeeze(\u001b[32m2\u001b[39m) \u001b[38;5;66;03m# B x F1 x time_sub\u001b[39;00m\n\u001b[32m     70\u001b[39m y = y.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# B x time_sub x F1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mLSTMModel.forward\u001b[39m\u001b[34m(self, x, h0, c0)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h0=\u001b[38;5;28;01mNone\u001b[39;00m, c0=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m h0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m c0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         h0 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m         c0 = torch.zeros(\u001b[38;5;28mself\u001b[39m.layer_dim, x.size(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.hidden_dim).to(x.device)\n\u001b[32m     14\u001b[39m     out, (hn, cn) = \u001b[38;5;28mself\u001b[39m.lstm(x, (h0, c0))\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# manual_write_study_params(trainer.study_name, trainer.storage)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1d8d4e73",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'test accuracy: 0.7007705479452054'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f\"test accuracy: {evaluate_model(trainer.model, trainer.val_loader, device)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c366efec",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SSVEPClassifier(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(1, 512), stride=(1, 1), padding=same, bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): DepthWiseConv2D(\n",
              "      (depthwise): Conv2d(64, 192, kernel_size=(8, 1), stride=(1, 1), groups=64, bias=False)\n",
              "    )\n",
              "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): ELU(alpha=1.0)\n",
              "    (5): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Dropout(p=0.1103041385689191, inplace=False)\n",
              "    (7): SeperableConv2D(\n",
              "      (depthwise): DepthWiseConv2D(\n",
              "        (depthwise): Conv2d(192, 192, kernel_size=(1, 16), stride=(1, 1), padding=same, groups=192, bias=False)\n",
              "      )\n",
              "      (pointwise): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "    )\n",
              "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ELU(alpha=1.0)\n",
              "    (10): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False)\n",
              "    (11): Dropout(p=0.1103041385689191, inplace=False)\n",
              "  )\n",
              "  (lstm_head): LSTMModel(\n",
              "    (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
              "    (fc): Linear(in_features=256, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c23f50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# to try:\n",
        "# - reoptimize optuna for second round\n",
        "# - extend data horizon to include validation data\n",
        "# - lower learning rate"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "icmtc_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
