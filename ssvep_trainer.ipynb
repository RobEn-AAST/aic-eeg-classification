{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a563374",
   "metadata": {
    "cellUniqueIdByVincent": "b5c77",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "2a563374",
    "outputId": "2f5357e7-a3f2-46ab-a67e-a21a666bccbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload  \n",
    "%autoreload 2  \n",
    "  \n",
    "import random  \n",
    "import numpy as np  \n",
    "import torch  \n",
    "import torch.nn as nn  \n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "# dataset related  \n",
    "from modules.competition_dataset import EEGDataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from braindecode.models import EEGSimpleConv, EEGInceptionERP\n",
    "from skorch.helper import predefined_split  \n",
    "import random\n",
    "import mne\n",
    "from skorch.helper import predefined_split  \n",
    "from skorch.callbacks import EpochScoring, Checkpoint, EarlyStopping, LRScheduler\n",
    "from optuna.integration import SkorchPruningCallback\n",
    "import optuna  \n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "device  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5bf7b0",
   "metadata": {
    "cellUniqueIdByVincent": "e6823",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8c5bf7b0",
    "outputId": "e981327e-6451-4cf6-8020-c4fb791c5b99"
   },
   "outputs": [],
   "source": [
    "data_path = './data/mtcaic3'\n",
    "model_path = './checkpoints/ssvep/models/the_honored_one.pth'\n",
    "optuna_db_path = './checkpoints/ssvep/optuna/the_honored_one.db'\n",
    "eeg_channels = [\n",
    "    \"OZ\", \n",
    "    \"PO7\",\n",
    "    \"PO8\",\n",
    "    \"PZ\",\n",
    "]\n",
    "\n",
    "# Add this at the beginning of your notebook, after imports\n",
    "def set_random_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call this function before creating datasets and models\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42089fb8",
   "metadata": {
    "cellUniqueIdByVincent": "77c22",
    "id": "42089fb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: SSVEP, split: train, domain: time, data_fraction: 1\n",
      "skipped: 114/2400\n",
      "task: SSVEP, split: validation, domain: time, data_fraction: 1\n",
      "skipped: 1/50\n"
     ]
    }
   ],
   "source": [
    "window_length = 1000 # ensure divisble by 64 the kernel size\n",
    "stride = 20\n",
    "batch_size = 64\n",
    "\n",
    "dataset_train = EEGDataset(\n",
    "    data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    data_fraction=1,\n",
    "    task=\"SSVEP\",\n",
    "    eeg_channels=eeg_channels,\n",
    "    tmin=1,\n",
    ")\n",
    "\n",
    "dataset_val = EEGDataset(\n",
    "    data_path=data_path,\n",
    "    window_length=window_length,\n",
    "    stride=stride,\n",
    "    split='validation',\n",
    "    data_fraction=1,\n",
    "    task=\"SSVEP\",\n",
    "    eeg_channels=eeg_channels,\n",
    "    tmin=1,\n",
    ")\n",
    "\n",
    "X_train = dataset_train.data\n",
    "y_train = dataset_train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0514d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-30 20:30:51,119] A new study created in memory with name: no-name-e1b2c33c-2008-4027-8da2-620a8be8cf75\n",
      "[W 2025-06-30 20:30:51,182] Trial 0 failed with parameters: {'n_filters': 6, 'drop_prob': 0.7, 'depth_multiplier': 1, 'batch_norm_alpha': 0.029182221378362842, 'n_times': 500, 'activation': 'ELU', 'lr': 2.239897647164504e-05, 'batch_size': 128, 'weight_decay': 0.005459507930918567, 'use_scheduler': True, 'scheduler_step_size': 20, 'scheduler_gamma': 0.5, 'patience': 25} because of the following error: RuntimeError('mat1 and mat2 shapes cannot be multiplied (128x124 and 60x4)').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_17096\\1558568738.py\", line 93, in objective\n",
      "    clf.fit(dataset_train)\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\braindecode\\eegneuralnet.py\", line 400, in fit\n",
      "    return super().fit(X=X, y=y, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\classifier.py\", line 165, in fit\n",
      "    return super(NeuralNetClassifier, self).fit(X, y, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1349, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\braindecode\\eegneuralnet.py\", line 350, in partial_fit\n",
      "    return super().partial_fit(X=X, y=y, classes=classes, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1308, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1220, in fit_loop\n",
      "    self.run_single_epoch(iterator_train, training=True, prefix=\"train\",\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1256, in run_single_epoch\n",
      "    step = step_fn(batch, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1135, in train_step\n",
      "    self._step_optimizer(step_fn)\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1090, in _step_optimizer\n",
      "    optimizer.step(step_fn)\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py\", line 124, in wrapper\n",
      "    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 485, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 79, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py\", line 225, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1124, in step_fn\n",
      "    step = self.train_step_single(batch, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1023, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py\", line 1551, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 240, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 240, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x124 and 60x4)\n",
      "[W 2025-06-30 20:30:51,184] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x124 and 60x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Rest of optimization code remains the same  \u001b[39;00m\n\u001b[32m     98\u001b[39m study = optuna.create_study(    \n\u001b[32m     99\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m,    \n\u001b[32m    100\u001b[39m     pruner=optuna.pruners.MedianPruner(n_startup_trials=\u001b[32m5\u001b[39m, n_warmup_steps=\u001b[32m10\u001b[39m, interval_steps=\u001b[32m1\u001b[39m)    \n\u001b[32m    101\u001b[39m )    \n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimization finished.\u001b[39m\u001b[33m\"\u001b[39m)    \n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)    \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     71\u001b[39m     callbacks.append(      \n\u001b[32m     72\u001b[39m         LRScheduler(      \n\u001b[32m     73\u001b[39m             policy=\u001b[33m'\u001b[39m\u001b[33mStepLR\u001b[39m\u001b[33m'\u001b[39m,      \n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         )      \n\u001b[32m     77\u001b[39m     )      \n\u001b[32m     79\u001b[39m clf = EEGClassifier(      \n\u001b[32m     80\u001b[39m     model,      \n\u001b[32m     81\u001b[39m     criterion=torch.nn.CrossEntropyLoss,      \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m     callbacks=callbacks,      \n\u001b[32m     91\u001b[39m )      \n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clf.history[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mvalid_accuracy\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\braindecode\\eegneuralnet.py:400\u001b[39m, in \u001b[36m_EEGNeuralNet.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_signal_args(X, y, classes=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    399\u001b[39m     \u001b[38;5;28mself\u001b[39m.signal_args_set_ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\classifier.py:165\u001b[39m, in \u001b[36mNeuralNetClassifier.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See ``NeuralNet.fit``.\u001b[39;00m\n\u001b[32m    155\u001b[39m \n\u001b[32m    156\u001b[39m \u001b[33;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m \n\u001b[32m    161\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# pylint: disable=useless-super-delegation\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# this is actually a pylint bug:\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNeuralNetClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1349\u001b[39m, in \u001b[36mNeuralNet.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initialized_:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize()\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\braindecode\\eegneuralnet.py:350\u001b[39m, in \u001b[36m_EEGNeuralNet.partial_fit\u001b[39m\u001b[34m(self, X, y, classes, **fit_params)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_signal_args(X, y, classes)\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.signal_args_set_ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1308\u001b[39m, in \u001b[36mNeuralNet.partial_fit\u001b[39m\u001b[34m(self, X, y, classes, **fit_params)\u001b[39m\n\u001b[32m   1306\u001b[39m \u001b[38;5;28mself\u001b[39m.notify(\u001b[33m'\u001b[39m\u001b[33mon_train_begin\u001b[39m\u001b[33m'\u001b[39m, X=X, y=y)\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1308\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1220\u001b[39m, in \u001b[36mNeuralNet.fit_loop\u001b[39m\u001b[34m(self, X, y, epochs, **fit_params)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m   1218\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify(\u001b[33m'\u001b[39m\u001b[33mon_epoch_begin\u001b[39m\u001b[33m'\u001b[39m, **on_epoch_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_single_epoch(iterator_valid, training=\u001b[38;5;28;01mFalse\u001b[39;00m, prefix=\u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1224\u001b[39m                           step_fn=\u001b[38;5;28mself\u001b[39m.validation_step, **fit_params)\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify(\u001b[33m\"\u001b[39m\u001b[33mon_epoch_end\u001b[39m\u001b[33m\"\u001b[39m, **on_epoch_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1256\u001b[39m, in \u001b[36mNeuralNet.run_single_epoch\u001b[39m\u001b[34m(self, iterator, training, prefix, step_fn, **fit_params)\u001b[39m\n\u001b[32m   1254\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify(\u001b[33m\"\u001b[39m\u001b[33mon_batch_begin\u001b[39m\u001b[33m\"\u001b[39m, batch=batch, training=training)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     step = \u001b[43mstep_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28mself\u001b[39m.history.record_batch(prefix + \u001b[33m\"\u001b[39m\u001b[33m_loss\u001b[39m\u001b[33m\"\u001b[39m, step[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m].item())\n\u001b[32m   1258\u001b[39m     batch_size = (get_len(batch[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[32m   1259\u001b[39m                   \u001b[38;5;28;01melse\u001b[39;00m get_len(batch))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1135\u001b[39m, in \u001b[36mNeuralNet.train_step\u001b[39m\u001b[34m(self, batch, **fit_params)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify(\n\u001b[32m   1128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mon_grad_computed\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1129\u001b[39m         named_parameters=TeeGenerator(\u001b[38;5;28mself\u001b[39m.get_all_learnable_params()),\n\u001b[32m   1130\u001b[39m         batch=batch,\n\u001b[32m   1131\u001b[39m         training=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1132\u001b[39m     )\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m step[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1135\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_accumulator.get_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1090\u001b[39m, in \u001b[36mNeuralNet._step_optimizer\u001b[39m\u001b[34m(self, step_fn)\u001b[39m\n\u001b[32m   1088\u001b[39m     optimizer.step()\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:124\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m opt = opt_ref()\n\u001b[32m    123\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:225\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    228\u001b[39m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1124\u001b[39m, in \u001b[36mNeuralNet.train_step.<locals>.step_fn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_fn\u001b[39m():\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28mself\u001b[39m._zero_grad_optimizer()\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m     step_accumulator.store_step(step)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28mself\u001b[39m.notify(\n\u001b[32m   1128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mon_grad_computed\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1129\u001b[39m         named_parameters=TeeGenerator(\u001b[38;5;28mself\u001b[39m.get_all_learnable_params()),\n\u001b[32m   1130\u001b[39m         batch=batch,\n\u001b[32m   1131\u001b[39m         training=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1132\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1023\u001b[39m, in \u001b[36mNeuralNet.train_step_single\u001b[39m\u001b[34m(self, batch, **fit_params)\u001b[39m\n\u001b[32m   1021\u001b[39m \u001b[38;5;28mself\u001b[39m._set_training(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1022\u001b[39m Xi, yi = unpack_data(batch)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.get_loss(y_pred, yi, X=Xi, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1025\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\skorch\\net.py:1551\u001b[39m, in \u001b[36mNeuralNet.infer\u001b[39m\u001b[34m(self, x, **fit_params)\u001b[39m\n\u001b[32m   1549\u001b[39m     x_dict = \u001b[38;5;28mself\u001b[39m._merge_x_and_fit_params(x, fit_params)\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module_(**x_dict)\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (128x124 and 60x4)"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):    \n",
    "    torch.manual_seed(seed)    \n",
    "    torch.cuda.manual_seed_all(seed)    \n",
    "    torch.backends.cudnn.deterministic = True    \n",
    "    torch.backends.cudnn.benchmark = False    \n",
    "    \n",
    "def objective(trial):      \n",
    "    set_seed(42 + trial.number)      \n",
    "      \n",
    "    # EEGInceptionERP hyperparameters - based on paper recommendations      \n",
    "    n_filters = trial.suggest_int(\"n_filters\", 4, 16, step=2)  # Default is 8  \n",
    "    drop_prob = trial.suggest_float(\"drop_prob\", 0.3, 0.7, step=0.1)  # Default is 0.5  \n",
    "    depth_multiplier = trial.suggest_int(\"depth_multiplier\", 1, 3)  # Default is 2  \n",
    "    batch_norm_alpha = trial.suggest_float(\"batch_norm_alpha\", 0.005, 0.05, log=True)  # Default is 0.01  \n",
    "      \n",
    "    # n_times - critical for SSVEP frequency resolution  \n",
    "    n_times = trial.suggest_categorical(\"n_times\", [500, 750, 1000, 1250])  # 2-5 seconds at 250Hz  \n",
    "          \n",
    "    # Activation function tuning      \n",
    "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"ELU\", \"LeakyReLU\"])      \n",
    "    activation_map = {      \n",
    "        \"ReLU\": torch.nn.ReLU,      \n",
    "        \"ELU\": torch.nn.ELU,       \n",
    "        \"LeakyReLU\": torch.nn.LeakyReLU,      \n",
    "    }      \n",
    "    activation = activation_map[activation_name]      \n",
    "      \n",
    "    # Training hyperparameters      \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)  \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])      \n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)      \n",
    "          \n",
    "    # Scheduler parameters      \n",
    "    use_scheduler = trial.suggest_categorical(\"use_scheduler\", [True, False])      \n",
    "    if use_scheduler:      \n",
    "        scheduler_step_size = trial.suggest_int(\"scheduler_step_size\", 10, 100, step=10)      \n",
    "        scheduler_gamma = trial.suggest_float(\"scheduler_gamma\", 0.1, 0.9, step=0.1)      \n",
    "          \n",
    "    # Early stopping patience      \n",
    "    patience = trial.suggest_int(\"patience\", 10, 50, step=5)      \n",
    "          \n",
    "    # Initialize EEGInceptionERP model with tuned parameters      \n",
    "    model = EEGInceptionERP(      \n",
    "        n_chans=4,      \n",
    "        n_outputs=4,      \n",
    "        n_times=n_times,  \n",
    "        sfreq=250,  \n",
    "        drop_prob=drop_prob,  \n",
    "        n_filters=n_filters,  \n",
    "        activation=activation,  \n",
    "        batch_norm_alpha=batch_norm_alpha,  \n",
    "        depth_multiplier=depth_multiplier,  \n",
    "    )      \n",
    "      \n",
    "    import os      \n",
    "    os.makedirs('./checkpoints/ssvep/optuna', exist_ok=True)  # Changed from mi to ssvep  \n",
    "      \n",
    "    callbacks = [      \n",
    "        \"accuracy\",      \n",
    "        SkorchPruningCallback(trial, monitor='valid_accuracy'),      \n",
    "        Checkpoint(      \n",
    "            f_params=f'./checkpoints/ssvep/optuna/best_model_trial_{trial.number}.pth',      \n",
    "            monitor='valid_accuracy',      \n",
    "            load_best=True,      \n",
    "        ),      \n",
    "        EarlyStopping(monitor='valid_accuracy', patience=patience, lower_is_better=False),      \n",
    "    ]      \n",
    "          \n",
    "    # Setup scheduler if enabled      \n",
    "    if use_scheduler:      \n",
    "        callbacks.append(      \n",
    "            LRScheduler(      \n",
    "                policy='StepLR',      \n",
    "                step_size=scheduler_step_size,      \n",
    "                gamma=scheduler_gamma      \n",
    "            )      \n",
    "        )      \n",
    "      \n",
    "    clf = EEGClassifier(      \n",
    "        model,      \n",
    "        criterion=torch.nn.CrossEntropyLoss,      \n",
    "        optimizer=torch.optim.Adam,      \n",
    "        optimizer__lr=lr,      \n",
    "        optimizer__weight_decay=weight_decay,      \n",
    "        batch_size=batch_size,      \n",
    "        max_epochs=200,      \n",
    "        train_split=predefined_split(dataset_val),      \n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",      \n",
    "        verbose=0,      \n",
    "        callbacks=callbacks,      \n",
    "    )      \n",
    "      \n",
    "    clf.fit(dataset_train)      \n",
    "    return clf.history[-1]['valid_accuracy']\n",
    "     \n",
    "  \n",
    "# Rest of optimization code remains the same  \n",
    "study = optuna.create_study(    \n",
    "    direction=\"maximize\",    \n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10, interval_steps=1)    \n",
    ")    \n",
    "    \n",
    "study.optimize(objective, n_trials=50)  \n",
    "    \n",
    "print(\"\\nOptimization finished.\")    \n",
    "print(\"Best trial:\")    \n",
    "trial = study.best_trial    \n",
    "    \n",
    "print(f\"  Value: {trial.value}\")    \n",
    "print(\"  Params: \")    \n",
    "for key, value in trial.params.items():    \n",
    "    print(f\"    {key}: {value}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642a854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no module to load\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  -------\n",
      "      1            \u001b[36m0.2690\u001b[0m        \u001b[32m1.4422\u001b[0m       \u001b[35m0.3162\u001b[0m            \u001b[31m0.3162\u001b[0m        \u001b[94m1.3425\u001b[0m  74.0789\n",
      "      2            0.2659        \u001b[32m1.4267\u001b[0m       \u001b[35m0.3235\u001b[0m            \u001b[31m0.3235\u001b[0m        1.3471  74.2054\n",
      "      3            \u001b[36m0.2777\u001b[0m        \u001b[32m1.4181\u001b[0m       0.3108            0.3108        1.3520  73.8193\n",
      "      4            \u001b[36m0.2813\u001b[0m        \u001b[32m1.4110\u001b[0m       0.2878            0.2878        1.3569  73.5667\n",
      "      5            \u001b[36m0.2862\u001b[0m        \u001b[32m1.4032\u001b[0m       0.2201            0.2201        1.3606  73.5271\n",
      "      6            \u001b[36m0.2871\u001b[0m        \u001b[32m1.3990\u001b[0m       0.2376            0.2376        1.3625  73.7118\n",
      "      7            0.2853        \u001b[32m1.3944\u001b[0m       \u001b[35m0.3525\u001b[0m            \u001b[31m0.3525\u001b[0m        1.3647  73.6788\n",
      "      8            0.2808        \u001b[32m1.3914\u001b[0m       0.3507            0.3507        1.3699  73.8711\n",
      "      9            0.2837        \u001b[32m1.3887\u001b[0m       0.2860            0.2860        1.3703  73.7590\n",
      "     10            0.2841        \u001b[32m1.3881\u001b[0m       0.3386            0.3386        1.3719  73.7177\n",
      "     11            0.2846        \u001b[32m1.3857\u001b[0m       \u001b[35m0.3779\u001b[0m            \u001b[31m0.3779\u001b[0m        1.3674  73.8305\n",
      "     12            \u001b[36m0.2881\u001b[0m        \u001b[32m1.3852\u001b[0m       \u001b[35m0.3948\u001b[0m            \u001b[31m0.3948\u001b[0m        1.3675  73.9516\n",
      "     13            \u001b[36m0.2906\u001b[0m        \u001b[32m1.3834\u001b[0m       0.2944            0.2944        1.3704  73.9724\n",
      "     14            0.2865        \u001b[32m1.3833\u001b[0m       0.2715            0.2715        1.3685  73.9403\n",
      "     15            0.2822        \u001b[32m1.3826\u001b[0m       0.2739            0.2739        1.3621  68.2964\n",
      "     16            0.2822        \u001b[32m1.3819\u001b[0m       0.2636            0.2636        1.3625  31.8879\n",
      "     17            0.2860        \u001b[32m1.3812\u001b[0m       0.2709            0.2709        1.3711  31.8670\n",
      "     18            \u001b[36m0.2909\u001b[0m        \u001b[32m1.3808\u001b[0m       0.2056            0.2056        1.3661  31.8488\n",
      "     19            \u001b[36m0.2923\u001b[0m        \u001b[32m1.3801\u001b[0m       0.2056            0.2056        1.3651  31.8164\n",
      "     20            0.2897        \u001b[32m1.3801\u001b[0m       0.2056            0.2056        1.3675  31.8979\n",
      "     21            \u001b[36m0.2933\u001b[0m        \u001b[32m1.3796\u001b[0m       0.2056            0.2056        1.3658  31.8856\n",
      "     22            0.2849        1.3798       0.2709            0.2709        1.3605  31.8031\n",
      "     23            0.2898        \u001b[32m1.3787\u001b[0m       0.2709            0.2709        1.3621  31.8531\n",
      "     24            0.2870        1.3791       0.2406            0.2406        1.3618  31.8984\n",
      "     25            0.2895        1.3789       0.2406            0.2406        1.3609  31.9345\n",
      "     26            0.2927        \u001b[32m1.3781\u001b[0m       0.2709            0.2709        1.3657  31.8483\n",
      "     27            0.2880        1.3784       0.2406            0.2406        1.3639  31.9065\n",
      "     28            0.2845        \u001b[32m1.3774\u001b[0m       0.2709            0.2709        1.3594  31.8698\n",
      "     29            0.2930        1.3777       0.2709            0.2709        1.3567  31.8645\n",
      "     30            0.2858        \u001b[32m1.3772\u001b[0m       0.2709            0.2709        1.3620  31.9803\n",
      "     31            0.2869        \u001b[32m1.3771\u001b[0m       0.2406            0.2406        1.3589  32.4048\n",
      "     32            0.2870        1.3771       0.2709            0.2709        1.3586  32.1749\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    }
   ],
   "source": [
    "n_chans = 4\n",
    "n_outputs = 4\n",
    "n_convs = 2\n",
    "kernel_size = 8\n",
    "feature_maps = 112\n",
    "resampling_freq = 100\n",
    "activation = torch.nn.ELU\n",
    "model = EEGSimpleConv(    \n",
    "        n_chans=n_chans,    \n",
    "        n_outputs=n_outputs,    \n",
    "        sfreq=250,\n",
    "        feature_maps=feature_maps,    \n",
    "        n_convs=n_convs,    \n",
    "        kernel_size=kernel_size,    \n",
    "        resampling_freq=resampling_freq, \n",
    "        activation=activation, \n",
    "    )    \n",
    "    \n",
    "clf = EEGClassifier(  \n",
    "    model,  \n",
    "    criterion=torch.nn.CrossEntropyLoss,  \n",
    "    optimizer=torch.optim.Adam,  \n",
    "    optimizer__lr=2.13537735805541e-05,  \n",
    "    optimizer__weight_decay=0.0002624302515969059,  \n",
    "    batch_size=128,  \n",
    "    max_epochs=20, # Or a reduced number for a final training run  \n",
    "    train_split=predefined_split(dataset_val), # You might want to combine train+val here for final training  \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",  \n",
    "    verbose=1,  \n",
    "    callbacks=[\"accuracy\"],  \n",
    ")  \n",
    "\n",
    "try:\n",
    "    clf.initialize()  \n",
    "    clf.module_.load_state_dict(torch.load('./checkpoints/ssvep/eegsimpleconv.pth'))\n",
    "except Exception:\n",
    "    print('no module to load')\n",
    "    \n",
    "clf.fit(dataset_train.data, dataset_train.labels)\n",
    "torch.save(clf.module_.module_.state_dict(), './checkpoints/ssvep/eegsimpleconv.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4448860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no module to load\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmad\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Convolution.cpp:1037.)\n",
      "  return F.conv2d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  -------\n",
      "      1            \u001b[36m0.2803\u001b[0m        \u001b[32m1.4425\u001b[0m       \u001b[35m0.3029\u001b[0m            \u001b[31m0.3029\u001b[0m        \u001b[94m1.4467\u001b[0m  60.5429\n",
      "      2            0.2782        \u001b[32m1.4213\u001b[0m       0.2400            0.2400        \u001b[94m1.3795\u001b[0m  74.5871\n",
      "      3            \u001b[36m0.2805\u001b[0m        \u001b[32m1.4118\u001b[0m       0.2793            0.2793        \u001b[94m1.3743\u001b[0m  74.6079\n",
      "      4            \u001b[36m0.2841\u001b[0m        \u001b[32m1.4053\u001b[0m       0.2007            0.2007        1.3788  63.2287\n",
      "      5            0.2831        \u001b[32m1.3978\u001b[0m       0.2678            0.2678        1.3805  32.9193\n",
      "      6            0.2752        \u001b[32m1.3933\u001b[0m       0.2715            0.2715        1.3780  32.0307\n",
      "      7            0.2790        \u001b[32m1.3911\u001b[0m       0.2715            0.2715        1.3756  32.7290\n",
      "      8            0.2823        \u001b[32m1.3895\u001b[0m       0.2703            0.2703        1.3778  33.1047\n",
      "      9            \u001b[36m0.2855\u001b[0m        \u001b[32m1.3882\u001b[0m       0.2727            0.2727        1.3751  34.5551\n",
      "     10            0.2821        \u001b[32m1.3872\u001b[0m       0.2733            0.2733        1.3786  33.6392\n",
      "     11            \u001b[36m0.2861\u001b[0m        \u001b[32m1.3857\u001b[0m       0.2727            0.2727        1.3753  33.4301\n",
      "     12            0.2843        \u001b[32m1.3851\u001b[0m       0.2745            0.2745        1.3744  32.9693\n",
      "     13            0.2796        \u001b[32m1.3844\u001b[0m       0.2745            0.2745        1.3755  33.1194\n",
      "     14            0.2756        \u001b[32m1.3831\u001b[0m       0.2745            0.2745        1.3753  33.3385\n",
      "     15            \u001b[36m0.2913\u001b[0m        1.3832       0.2745            0.2745        \u001b[94m1.3727\u001b[0m  33.1631\n",
      "     16            0.2847        1.3833       0.2745            0.2745        1.3746  32.7412\n",
      "     17            \u001b[36m0.2930\u001b[0m        \u001b[32m1.3822\u001b[0m       0.2745            0.2745        \u001b[94m1.3710\u001b[0m  33.1784\n",
      "     18            0.2815        \u001b[32m1.3815\u001b[0m       \u001b[35m0.3730\u001b[0m            \u001b[31m0.3730\u001b[0m        \u001b[94m1.3690\u001b[0m  33.3403\n",
      "     19            0.2830        \u001b[32m1.3813\u001b[0m       0.3271            0.3271        \u001b[94m1.3671\u001b[0m  33.2647\n",
      "     20            0.2812        \u001b[32m1.3811\u001b[0m       0.3271            0.3271        \u001b[94m1.3655\u001b[0m  32.7765\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  -------\n",
      "      1            \u001b[36m0.2848\u001b[0m        \u001b[32m1.3806\u001b[0m       \u001b[35m0.3271\u001b[0m            \u001b[31m0.3271\u001b[0m        \u001b[94m1.3633\u001b[0m  33.1311\n",
      "      2            \u001b[36m0.2852\u001b[0m        \u001b[32m1.3801\u001b[0m       \u001b[35m0.3851\u001b[0m            \u001b[31m0.3851\u001b[0m        \u001b[94m1.3630\u001b[0m  32.9089\n",
      "      3            \u001b[36m0.2980\u001b[0m        1.3807       0.3585            0.3585        \u001b[94m1.3623\u001b[0m  33.5366\n",
      "      4            0.2939        \u001b[32m1.3797\u001b[0m       0.3271            0.3271        1.3631  33.0863\n",
      "      5            0.2929        1.3799       0.3271            0.3271        \u001b[94m1.3604\u001b[0m  33.6316\n",
      "      6            0.2980        \u001b[32m1.3795\u001b[0m       0.3271            0.3271        \u001b[94m1.3584\u001b[0m  33.3698\n",
      "      7            0.2928        \u001b[32m1.3791\u001b[0m       0.3271            0.3271        1.3627  33.6136\n",
      "      8            0.2904        \u001b[32m1.3788\u001b[0m       0.3271            0.3271        \u001b[94m1.3579\u001b[0m  33.4176\n",
      "      9            0.2880        \u001b[32m1.3787\u001b[0m       0.3271            0.3271        1.3590  33.1173\n",
      "     10            \u001b[36m0.2985\u001b[0m        \u001b[32m1.3783\u001b[0m       0.3271            0.3271        \u001b[94m1.3553\u001b[0m  33.1510\n",
      "     11            0.2979        1.3784       0.3271            0.3271        1.3566  33.1951\n",
      "     12            \u001b[36m0.2998\u001b[0m        \u001b[32m1.3781\u001b[0m       0.3271            0.3271        \u001b[94m1.3541\u001b[0m  33.1573\n",
      "     13            0.2874        \u001b[32m1.3777\u001b[0m       0.2515            0.2515        1.3564  33.4166\n",
      "     14            0.2904        \u001b[32m1.3776\u001b[0m       0.3271            0.3271        1.3577  33.3853\n",
      "     15            0.2899        \u001b[32m1.3772\u001b[0m       0.2799            0.2799        \u001b[94m1.3531\u001b[0m  33.3763\n",
      "     16            0.2944        \u001b[32m1.3769\u001b[0m       0.3295            0.3295        1.3539  33.8987\n",
      "     17            0.2970        \u001b[32m1.3764\u001b[0m       0.3271            0.3271        1.3538  33.5102\n",
      "     18            0.2887        1.3770       0.2515            0.2515        \u001b[94m1.3513\u001b[0m  33.7751\n",
      "     19            0.2934        1.3767       0.3271            0.3271        \u001b[94m1.3483\u001b[0m  32.9316\n",
      "     20            \u001b[36m0.3009\u001b[0m        \u001b[32m1.3762\u001b[0m       0.3271            0.3271        1.3486  33.1558\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  -------\n",
      "      1            \u001b[36m0.3000\u001b[0m        \u001b[32m1.3759\u001b[0m       \u001b[35m0.3271\u001b[0m            \u001b[31m0.3271\u001b[0m        \u001b[94m1.3512\u001b[0m  33.1482\n",
      "      2            0.2973        1.3761       0.3005            0.3005        \u001b[94m1.3505\u001b[0m  33.4511\n",
      "      3            0.2964        \u001b[32m1.3758\u001b[0m       0.3271            0.3271        \u001b[94m1.3492\u001b[0m  33.2443\n",
      "      4            0.2906        \u001b[32m1.3757\u001b[0m       0.2721            0.2721        1.3524  33.2358\n",
      "      5            0.2931        \u001b[32m1.3754\u001b[0m       0.3271            0.3271        1.3503  33.2843\n",
      "      6            0.2910        \u001b[32m1.3747\u001b[0m       0.2515            0.2515        1.3522  32.8500\n",
      "      7            0.2975        1.3752       0.2545            0.2545        1.3518  33.1506\n",
      "      8            0.2879        1.3750       0.3271            0.3271        1.3498  33.2297\n",
      "      9            0.2974        \u001b[32m1.3744\u001b[0m       \u001b[35m0.3428\u001b[0m            \u001b[31m0.3428\u001b[0m        \u001b[94m1.3478\u001b[0m  33.2159\n",
      "     10            0.2987        \u001b[32m1.3743\u001b[0m       0.3343            0.3343        1.3510  33.2012\n",
      "     11            0.2977        1.3744       \u001b[35m0.3640\u001b[0m            \u001b[31m0.3640\u001b[0m        1.3483  33.1172\n",
      "     12            0.2980        \u001b[32m1.3742\u001b[0m       0.2551            0.2551        1.3500  33.1582\n",
      "     13            \u001b[36m0.3003\u001b[0m        \u001b[32m1.3738\u001b[0m       0.3011            0.3011        1.3496  33.2702\n",
      "     14            0.2983        \u001b[32m1.3735\u001b[0m       0.3271            0.3271        1.3505  33.1567\n",
      "     15            0.2982        \u001b[32m1.3733\u001b[0m       0.2515            0.2515        1.3502  33.0217\n",
      "     16            0.2949        \u001b[32m1.3725\u001b[0m       0.3150            0.3150        1.3514  32.8687\n",
      "     17            0.2916        1.3730       0.2969            0.2969        1.3539  32.9841\n",
      "     18            \u001b[36m0.3010\u001b[0m        1.3728       0.3005            0.3005        1.3509  33.0342\n",
      "     19            0.2904        1.3726       0.2412            0.2412        1.3505  33.0607\n",
      "     20            \u001b[36m0.3058\u001b[0m        1.3726       0.3501            0.3501        \u001b[94m1.3478\u001b[0m  33.1616\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      dur\n",
      "-------  ----------------  ------------  -----------  ----------------  ------------  -------\n",
      "      1            \u001b[36m0.2924\u001b[0m        \u001b[32m1.3725\u001b[0m       \u001b[35m0.3017\u001b[0m            \u001b[31m0.3017\u001b[0m        \u001b[94m1.3535\u001b[0m  33.2321\n",
      "      2            \u001b[36m0.3026\u001b[0m        \u001b[32m1.3724\u001b[0m       0.2823            0.2823        1.3536  33.1635\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = EEGInceptionERP(  \n",
    "    n_chans=4,  \n",
    "    n_outputs=4,  \n",
    "    n_times=1000,  # 4 seconds at 250 Hz  \n",
    "    sfreq=250,  \n",
    "    drop_prob=0.5,  # Optimal dropout rate  \n",
    "    n_filters=8,    # Default from paper  \n",
    "    activation=torch.nn.ELU,  # Your current activation is good  \n",
    ")\n",
    "    \n",
    "for i in range(100):\n",
    "    clf = EEGClassifier(  \n",
    "        model,  \n",
    "        criterion=torch.nn.CrossEntropyLoss,  \n",
    "        optimizer=torch.optim.Adam,  \n",
    "        optimizer__lr=2.13537735805541e-05,  \n",
    "        optimizer__weight_decay=0.0002624302515969059,  \n",
    "        batch_size=128,  \n",
    "        max_epochs=20, # Or a reduced number for a final training run  \n",
    "        train_split=predefined_split(dataset_val), # You might want to combine train+val here for final training  \n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",  \n",
    "        verbose=1,  \n",
    "        callbacks=[\"accuracy\"],  \n",
    "    )  \n",
    "\n",
    "    try:\n",
    "        clf.initialize()  \n",
    "        clf.module_.load_state_dict(torch.load('./checkpoints/ssvep/eeginception.pth'))\n",
    "    except Exception:\n",
    "        print('no module to load')\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "    torch.save(clf.module_.state_dict(), './checkpoints/ssvep/eeginception.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79659a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vincent": {
   "sessionId": "bb6251503a76299c2e1994d5_2025-06-21T14-24-00-772Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
